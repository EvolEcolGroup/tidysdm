[{"path":"https://evolecolgroup.github.io/tidysdm/CODE_OF_CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (https://www.contributor-covenant.org), version 1.0.0, available https://contributor-covenant.org/version/1/0/0/.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to tidysdm","title":"Contributing to tidysdm","text":"document outlines contribute development tidysdm. package maintained voluntary basis, help always appreciated.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/CONTRIBUTING.html","id":"the-basic-process-of-contributing","dir":"","previous_headings":"","what":"The basic process of contributing","title":"Contributing to tidysdm","text":"Development work tidysdm occurs dev branch. , want propose changes, work dev. Start forking project onto github repository, make changes directly fork (either dev branch, make custom branch). updating documentation checking tests pass (see ), start Pull Request. proposed changes reviewed, might asked fix/improve code. can iterative process, requiring rounds revision depending complexity code. Functions documented using roxygen. changes affects documentation , rebuild . root directory package, simply run: implemented new functionality, patched bug, consider whether add appropriate unit test. tidysdm uses testthat framework unit tests. make sure tests work : Finally, submit push request, check changes don’t break build. can check, also builds vignette runs tests.: Make sure resolved warnings notes raised devtools::check()! followed 3 steps, ready make Pull Request. changes go automatic continuous integration, check impact changes multiple platforms. everything goes well, see green tick submission.","code":"devtools::document() devtools::test() devtools::check()"},{"path":"https://evolecolgroup.github.io/tidysdm/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to tidysdm","text":"spot typos, spelling mistakes, grammatical errors documentation, fix directly file describes function. .R file R directory, .Rd file man directory. .Rd files automatically generated roxygen2 edited hand. recommend study first roxygen2 comments work.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/CONTRIBUTING.html","id":"functional-changes","dir":"","previous_headings":"","what":"Functional changes","title":"Contributing to tidysdm","text":"want make change impacts functioning tidysdm, ’s good idea first file issue explaining mind. change meant fix bug, add minimal reprex. good reprex also perfect starting point writing unit test, accompany functional change code. Unit tests also essential fixing bugs, can demonstrate fix work, prevent future changes undoing work. unit testing, use testthat; find tests tests, file dedicated function, following convention test_my_function.R naming files. creating tests, try make use built-datasets, rather adding data files package. Ideally, body Pull Request include phrase Fixes #issue-number, issue_number number Github. way, Pull Request automatically linked issue, issue closed Pull Request merged . user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html. continuous integration checks Pull Request reduce test coverage.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Functional changes","what":"Code style","title":"Contributing to tidysdm","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. Lots commenting code helps mantainability; , doubt, always add explanation new code.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to tidysdm","text":"Please note tidyverse project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Affero General Public License","title":"GNU Affero General Public License","text":"Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU Affero General Public License","text":"GNU Affero General Public License free, copyleft license software kinds works, specifically designed ensure cooperation community case network server software. licenses software practical works designed take away freedom share change works. contrast, General Public Licenses intended guarantee freedom share change versions program–make sure remains free software users. speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. Developers use General Public Licenses protect rights two steps: (1) assert copyright software, (2) offer License gives legal permission copy, distribute /modify software. secondary benefit defending users’ freedom improvements made alternate versions program, receive widespread use, become available developers incorporate. Many developers free software heartened encouraged resulting cooperation. However, case software used network servers, result may fail come . GNU General Public License permits making modified version letting public access server without ever releasing source code public. GNU Affero General Public License designed specifically ensure , cases, modified source code becomes available community. requires operator network server provide source code modified version running users server. Therefore, public use modified version, publicly accessible server, gives public access source code modified version. older license, called Affero General Public License published Affero, designed accomplish similar goals. different license, version Affero GPL, Affero released new version Affero GPL permits relicensing license. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions.","title":"GNU Affero General Public License","text":"“License” refers version 3 GNU Affero General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code.","title":"GNU Affero General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions.","title":"GNU Affero General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law.","title":"GNU Affero General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies.","title":"GNU Affero General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions.","title":"GNU Affero General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: work must carry prominent notices stating modified , giving relevant date. work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms.","title":"GNU Affero General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms.","title":"GNU Affero General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: Disclaiming warranty limiting liability differently terms sections 15 16 License; Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; Limiting use publicity purposes names licensors authors material; Declining grant rights trademark law use trade names, trademarks, service marks; Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination.","title":"GNU Affero General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies.","title":"GNU Affero General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients.","title":"GNU Affero General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents.","title":"GNU Affero General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom.","title":"GNU Affero General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_13-remote-network-interaction-use-with-the-gnu-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Remote Network Interaction; Use with the GNU General Public License.","title":"GNU Affero General Public License","text":"Notwithstanding provision License, modify Program, modified version must prominently offer users interacting remotely computer network (version supports interaction) opportunity receive Corresponding Source version providing access Corresponding Source network server charge, standard customary means facilitating copying software. Corresponding Source shall include Corresponding Source work covered version 3 GNU General Public License incorporated pursuant following paragraph. Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU General Public License single combined work, convey resulting work. terms License continue apply part covered work, work combined remain governed version 3 GNU General Public License.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License.","title":"GNU Affero General Public License","text":"Free Software Foundation may publish revised /new versions GNU Affero General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU Affero General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU Affero General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU Affero General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty.","title":"GNU Affero General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability.","title":"GNU Affero General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16.","title":"GNU Affero General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU Affero General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. software can interact users remotely computer network, also make sure provides way users get source. example, program web application, interface display “Source” link leads users archive code. many ways offer source, different solutions better different programs; see section 13 specific requirements. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU AGPL, see https://www.gnu.org/licenses/.","code":"<one line to give the program's name and a brief idea of what it does.>     Copyright (C) <year>  <name of author>      This program is free software: you can redistribute it and/or modify     it under the terms of the GNU Affero General Public License as     published by the Free Software Foundation, either version 3 of the     License, or (at your option) any later version.      This program is distributed in the hope that it will be useful,     but WITHOUT ANY WARRANTY; without even the implied warranty of     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     GNU Affero General Public License for more details.      You should have received a copy of the GNU Affero General Public License     along with this program.  If not, see <https://www.gnu.org/licenses/>."},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"sdms-with-tidymodels","dir":"Articles","previous_headings":"","what":"SDMs with tidymodels","title":"tidysdm overview","text":"Species Distribution Modelling relies several algorithms, many number hyperparameters require turning. tidymodels universe includes number packages specifically design fit, tune validate models. advantage tidymodels models syntax results returned users standardised, thus providing coherent interface modelling. Given variety models required SDM, tidymodels ideal framework. tidysdm provides number wrappers specialised functions facilitate fitting SDM tidymodels. article provides overview tidysdm facilitates fitting SDMs. articles, detailing use package palaeodata, fitting complex models troubleshoot models can found tidisdm website. tidysdm relies tidymodels, users advised familiarise introductory tutorials tidymodels website. load tidysdm, automatically loads tidymodels associated packages necessary fit models:","code":"library(tidysdm) #> Loading required package: tidymodels #> ── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ── #> ✔ broom        1.0.6      ✔ recipes      1.0.10 #> ✔ dials        1.2.1      ✔ rsample      1.2.1  #> ✔ dplyr        1.1.4      ✔ tibble       3.2.1  #> ✔ ggplot2      3.5.1      ✔ tidyr        1.3.1  #> ✔ infer        1.0.7      ✔ tune         1.2.1  #> ✔ modeldata    1.4.0      ✔ workflows    1.1.4  #> ✔ parsnip      1.2.1      ✔ workflowsets 1.1.0  #> ✔ purrr        1.0.2      ✔ yardstick    1.3.1 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Search for functions across packages at https://www.tidymodels.org/find/ #> Loading required package: spatialsample"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"accessing-the-data-for-this-vignette-how-to-use-rgbif","dir":"Articles","previous_headings":"SDMs with tidymodels","what":"Accessing the data for this vignette: how to use rgbif","title":"tidysdm overview","text":"start reading set presences species lizard inhabits Iberian peninsula, Lacerta schreiberi. data taken GBIF Occurrence Download (6 July 2023) https://doi.org/10.15468/dl.srq3b3. dataset already included tidysdm package: Alternatively, can easily access manipulate dataset using rbgif:","code":"data(lacerta)  lacerta #>              ID latitude longitude #> 1     858029749 42.57386 -7.093272 #> 2     858029738 42.57386 -7.093272 #> 3     614631090 41.36433 -7.901420 #> 4     614631085 41.33614 -7.806970 #> 5     614631083 41.33599 -7.808340 #> 6     614631080 41.38818 -7.830690 #> 7     614631072 41.37781 -7.813690 #> 8     614559731 40.34988 -7.702352 #> 9     614559728 40.38260 -7.701418 #> 10    614559657 40.35550 -7.558990 #> 11    614559646 40.29421 -7.650721 #> 12    614559638 40.31025 -7.750595 #> 13    614559626 40.30913 -7.754499 #> 14    614559614 40.30823 -7.755680 #> 15    614559580 40.36137 -7.652468 #> 16    614559536 40.32880 -7.675835 #> 17    614559494 40.32503 -7.683771 #> 18    614559485 40.32780 -7.677855 #> 19   4138510168 42.02203 -8.128677 #> 20   4138300594 41.99143 -8.268817 #> 21   4137774808 41.59728 -8.736125 #> 22   4137647526 41.28201 -8.731018 #> 23   4137647525 41.82842 -7.917928 #> 24   4134139940 40.69223 -8.162669 #> 25   4133852702 40.06937 -8.268567 #> 26   4121487317 41.68082 -7.713207 #> 27   4121307904 41.88402 -8.252778 #> 28   4121184631 40.84378 -7.726580 #> 29   4121124321 41.09547 -8.488889 #> 30   4116626006 42.04772 -8.503785 #> 31   4116285362 40.85782 -8.281194 #> 32   4116236838 40.66703 -7.900817 #> 33   4116092213 41.61971 -8.087063 #> 34   4112181181 41.70629 -8.096636 #> 35   4112023856 40.89950 -8.236026 #> 36   4111883952 41.63347 -7.574793 #> 37   4111614197 40.32288 -7.602690 #> 38   4103336935 40.08139 -8.204578 #> 39   4103238233 40.09843 -8.235022 #> 40   4102652095 41.87182 -8.208900 #> 41   4102603846 40.92872 -8.257428 #> 42   4102587117 41.86692 -8.216805 #> 43   4097012311 40.76552 -8.156668 #> 44   4096983593 40.92807 -8.258583 #> 45   4096753776 40.92810 -8.258452 #> 46   4080894369 40.37558 -8.368415 #> 47   4080783334 41.88167 -8.694533 #> 48   4076368267 40.63692 -8.439818 #> 49   4076093306 40.15494 -8.220965 #> 50   4076035760 40.76200 -8.553856 #> 51   4058226968 40.39995 -7.588924 #> 52   4058037946 40.40788 -7.562885 #> 53   4057914333 40.29825 -7.767162 #> 54   4056787598 43.40633 -5.339476 #> 55   4056745747 41.76657 -8.642622 #> 56   4056481420 40.40565 -7.891960 #> 57   4056306431 37.33848 -8.572752 #> 58   4046414634 40.84864 -8.382517 #> 59   4018170017 40.84750 -8.474317 #> 60   4015144485 41.16098 -8.482696 #> 61   4014974842 41.16116 -8.482167 #> 62   4006699733 41.80984 -8.131572 #> 63   3997288194 43.25489 -8.214883 #> 64   3997172501 43.45000 -4.980000 #> 65   3997131929 43.25482 -8.214870 #> 66   3997050991 43.45000 -4.980000 #> 68   3996318299 43.31143 -8.541861 #> 69   3996029768 43.25493 -8.215063 #> 70   3995883566 43.25478 -8.214628 #> 71   3995742860 40.24523 -5.604502 #> 72   3994052177 40.40062 -7.587015 #> 73   3966586163 37.32890 -8.583760 #> 74   3947369148 40.19553 -8.236308 #> 75   3912316870 41.09898 -8.560330 #> 76   3912179831 41.31493 -8.257731 #> 77   3907446980 42.83405 -7.066258 #> 78   3907284763 43.12507 -4.880099 #> 79   3907157374 43.22843 -6.046033 #> 80   3907117102 40.34128 -5.134518 #> 81   3906896706 43.60513 -5.888642 #> 82   3906336401 40.19971 -5.742140 #> 83   3906255544 40.33909 -5.156703 #> 84   3906081395 40.27000 -5.240000 #> 85   3906018124 41.77160 -8.188200 #> 86   3905579314 42.36183 -8.549850 #> 87   3905503521 43.31927 -8.521358 #> 88   3904992758 40.30552 -5.238461 #> 89   3904847047 42.82917 -5.776468 #> 90   3904684037 43.12507 -4.880099 #> 91   3904597803 42.90586 -8.402835 #> 92   3904516076 43.40315 -4.744980 #> 93   3904369429 43.22844 -6.046053 #> 94   3904282757 43.22840 -6.046134 #> 95   3903197158 40.08107 -8.203597 #> 96   3902606076 41.57694 -7.982271 #> 97   3902601687 41.16419 -8.482072 #> 98   3902423737 40.67403 -8.214256 #> 99   3902372729 41.25738 -7.935210 #> 100  3888804754 40.37550 -8.365012 #> 101  3873365684 40.37473 -8.365758 #> 102  3860741206 40.89950 -8.235959 #> 103  3860517442 41.28359 -7.838244 #> 104  3860325381 41.73333 -8.160636 #> 105  3860006479 39.87334 -8.852991 #> 106  3859664580 41.27729 -7.995230 #> 107  3859567137 40.37849 -8.371000 #> 108  3858854802 41.72320 -8.129511 #> 109  3858854034 40.35641 -7.558820 #> 110  3827447637 40.29260 -5.171154 #> 111  3827426685 40.30645 -5.190073 #> 114  3827170532 40.29279 -5.171848 #> 115  3827155357 43.25506 -8.213604 #> 116  3827120895 39.38935 -5.382989 #> 117  3826867567 43.29656 -8.554120 #> 118  3826866390 43.55284 -7.155949 #> 119  3826810234 43.10727 -6.259454 #> 120  3826711597 43.57161 -5.699182 #> 121  3826663742 43.49593 -5.934134 #> 123  3826194745 43.39108 -8.323385 #> 124  3826132575 40.35440 -5.112846 #> 125  3826103015 43.57041 -5.722249 #> 126  3826079371 43.55287 -7.155935 #> 127  3826034598 40.30568 -5.204373 #> 128  3825994248 43.55289 -7.156223 #> 129  3825834008 42.43518 -7.787696 #> 130  3825671626 42.29921 -8.427880 #> 131  3825286990 42.88293 -5.763342 #> 132  3825242818 43.11725 -6.284291 #> 133  3824773631 40.28159 -5.229868 #> 134  3824658562 42.75729 -8.272977 #> 135  3824458747 40.11800 -5.778082 #> 136  3824395901 43.60045 -5.920679 #> 138  3824325042 43.25876 -6.132585 #> 140  3824114271 43.55380 -7.150638 #> 141  3823724468 40.30637 -5.190222 #> 142  3823695876 42.04758 -7.833532 #> 143  3823679192 43.16100 -7.811129 #> 144  3823644579 43.22112 -8.282746 #> 145  3823558122 43.57641 -5.990616 #> 146  3823525379 42.43281 -8.395458 #> 147  3823405779 43.25557 -8.215127 #> 148  3823370594 40.30734 -5.190725 #> 149  3823352692 43.26045 -7.489225 #> 150  3823006585 43.25548 -8.215295 #> 151  3822784961 42.07946 -7.761854 #> 152  3822665898 42.90595 -5.805009 #> 153  3802554684 40.35618 -7.558206 #> 154  3802446032 41.31493 -8.257731 #> 155  3785165345 41.09905 -8.559851 #> 156  3785030262 40.44248 -7.515020 #> 157  3784779955 41.19607 -8.161694 #> 158  3773639035 42.00592 -8.166519 #> 159  3773638868 40.92789 -8.259287 #> 160  3773600023 40.32829 -7.586550 #> 161  3773579360 41.46367 -8.397842 #> 162  3773331266 41.46369 -8.397868 #> 163  3772430861 41.20036 -8.680263 #> 164  3764501931 41.28115 -8.730330 #> 165  3764237964 37.30751 -8.575900 #> 166  3760331840 40.66485 -7.906360 #> 167  3760256841 40.66477 -7.906402 #> 168  3760245302 41.80926 -8.132585 #> 169  3759917005 40.93054 -8.246717 #> 171  3759664475 41.28517 -8.340068 #> 172  3759511079 40.11704 -8.497478 #> 173  3759285347 41.31493 -8.257731 #> 174  3747177530 41.29057 -8.235037 #> 175  3747105785 41.82141 -8.295183 #> 176  3742983478 40.01487 -8.587528 #> 177  3733279230 43.54593 -8.046426 #> 178  3732795612 42.12000 -6.770000 #> 179  3732784603 43.38732 -4.323388 #> 180  3732557326 43.28899 -6.771403 #> 181  3732423322 42.43443 -7.704437 #> 182  3732260894 40.26377 -5.268384 #> 183  3731723971 43.73630 -7.703093 #> 186  3731246742 40.27246 -5.234587 #> 187  3731140171 40.27177 -5.246742 #> 188  3730867547 43.57594 -5.992134 #> 189  3730795274 43.57253 -5.994503 #> 190  3730747451 43.40228 -8.327387 #> 191  3730259342 43.30598 -8.536074 #> 192  3729886329 43.28944 -8.489956 #> 193  3729533373 43.30155 -8.503598 #> 194  3729338522 40.35877 -5.526905 #> 195  3729243652 43.59940 -5.939154 #> 196  3729232715 43.30372 -8.428818 #> 197  3729072329 40.34943 -5.295884 #> 198  3728708629 42.92490 -8.169698 #> 199  3728646722 40.27644 -5.231343 #> 200  3728584484 40.27056 -5.238148 #> 201  3728498330 43.10000 -6.270000 #> 202  3728484592 43.30411 -8.606538 #> 203  3728404536 40.21742 -7.732143 #> 204  3728122900 43.25263 -6.728374 #> 205  3728027283 40.29539 -5.173345 #> 206  3727992574 40.29203 -5.171374 #> 207  3727770407 43.04447 -6.251601 #> 208  3727675314 43.28323 -8.545810 #> 209  3727241475 40.34139 -5.186852 #> 210  3726885606 42.11812 -6.714393 #> 211  3726351000 42.03855 -6.887438 #> 212  3726280229 43.10000 -6.270000 #> 213  3726168466 43.15211 -5.600696 #> 214  3726010635 43.10000 -4.060000 #> 215  3725698669 43.13044 -4.809709 #> 216  3725422827 42.44063 -6.394794 #> 219  3725093673 40.42682 -6.149007 #> 220  3725060873 43.15699 -6.952821 #> 222  3722268573 43.55432 -6.111816 #> 224  3721736481 43.47930 -7.913536 #> 225  3721494769 43.29768 -8.570078 #> 226  3721392112 40.27242 -5.234640 #> 227  3721022126 40.51314 -6.167968 #> 228  3720913817 43.30505 -8.535715 #> 229  3720828362 43.48000 -7.050000 #> 230  3720765384 43.08638 -9.192246 #> 231  3720759073 43.44339 -5.791692 #> 232  3720503132 40.51314 -6.167968 #> 233  3720484714 43.55238 -6.014103 #> 235  3720151980 39.47717 -5.388593 #> 236  3720021627 43.13527 -4.816869 #> 237  3719941935 43.57282 -5.994296 #> 238  3719639436 43.57665 -5.990095 #> 239  3719396101 40.26263 -5.270146 #> 240  3719284704 43.28000 -5.990000 #> 241  3719224766 39.44328 -5.344995 #> 242  3719071502 40.27115 -5.235581 #> 243  3718346021 43.27167 -8.516319 #> 244  3718113343 43.10000 -6.270000 #> 245  3718042204 43.10000 -6.270000 #> 246  3717590281 43.10000 -6.270000 #> 247  3717451694 43.29755 -8.571736 #> 248  3717446813 43.29811 -8.534722 #> 249  3717173342 40.31362 -5.627051 #> 250  3716997397 42.40000 -8.490000 #> 251  3716892411 42.07009 -6.541847 #> 253  3716632135 42.07006 -6.541246 #> 254  3716268286 40.27742 -5.111580 #> 256  3715931235 43.23981 -8.901147 #> 258  3715143459 40.24000 -5.330000 #> 259  3698100690 39.31591 -7.330855 #> 260  3456534329 39.60585 -8.359525 #> 261  3415427142 41.08160 -8.471741 #> 262  3408234996 40.81437 -8.227214 #> 263  3390592007 41.81128 -8.044139 #> 264  3390592000 41.92987 -8.247404 #> 265  3390591998 41.41318 -7.846593 #> 266  3390591995 41.78309 -7.912192 #> 267  3390591971 42.00115 -8.137898 #> 268  3390591967 37.37215 -8.476313 #> 269  3390591966 41.81295 -8.272851 #> 270  3390591954 37.31002 -8.781429 #> 271  3390591946 41.65946 -8.214528 #> 272  3390591940 41.79707 -8.802613 #> 273  3390591931 41.41178 -7.715009 #> 274  3390591923 41.74814 -8.033049 #> 275  3390591909 41.29657 -7.896422 #> 276  3390591908 42.03769 -8.209891 #> 277  3390591902 41.86966 -6.525084 #> 278  3390591891 37.63410 -8.621811 #> 279  3390591866 37.35397 -8.442567 #> 280  3390591865 41.32324 -7.860128 #> 281  3390591864 41.91170 -8.223503 #> 282  3390591861 40.90137 -8.021737 #> 283  3390591856 41.93926 -8.307606 #> 284  3390591855 37.82380 -8.791282 #> 285  3390591848 41.78436 -8.056572 #> 286  3390591824 41.81945 -7.947697 #> 287  3390591815 41.85524 -7.923018 #> 288  3390591813 41.92319 -6.957134 #> 289  3390591805 41.76881 -8.429748 #> 290  3390591793 42.00124 -8.149971 #> 291  3390591787 41.76683 -8.116979 #> 292  3390591786 41.50183 -7.713232 #> 293  3390591783 41.74017 -8.165445 #> 294  3390591775 41.76674 -8.104951 #> 295  3390591774 41.88515 -8.296138 #> 296  3390591766 41.93966 -6.872173 #> 297  3390591765 41.84890 -8.260399 #> 298  3390591763 41.81022 -7.923772 #> 299  3390591761 41.90338 -8.332106 #> 300  3390591747 41.93197 -6.944793 #> 301  3390591745 41.81922 -7.923621 #> 302  3390591734 41.77685 -8.261227 #> 303  3390591720 41.81213 -8.152474 #> 304  3390591718 41.94841 -8.331636 #> 305  3390591710 41.92404 -7.005345 #> 306  3390591705 37.42686 -8.645462 #> 307  3390591681 41.76615 -8.032778 #> 308  3390591680 40.85867 -8.354572 #> 309  3390591679 41.79385 -8.116609 #> 310  3390591665 39.34446 -7.359429 #> 311  3390591654 41.77628 -8.177010 #> 312  3390591642 42.02817 -8.137533 #> 313  3390591632 41.78502 -8.140797 #> 314  3390591631 41.93447 -7.089449 #> 315  3390591625 41.80165 -7.972062 #> 316  3390591614 41.92705 -7.897690 #> 317  3390591611 41.80143 -7.947992 #> 318  3390591594 41.34952 -7.787951 #> 319  3390591593 41.87560 -8.211889 #> 320  3390591590 37.40887 -8.656846 #> 321  3390591586 40.98243 -8.020540 #> 322  3390591581 41.69460 -8.093934 #> 323  3390591574 41.75745 -8.068994 #> 324  3390591566 41.95712 -8.283281 #> 325  3390591555 41.77442 -7.948433 #> 326  3390591548 41.95775 -8.391870 #> 327  3390591547 41.36753 -7.787617 #> 328  3390591546 41.81087 -7.995991 #> 329  3390591535 41.85579 -7.983243 #> 330  3390591529 41.84646 -7.947255 #> 331  3390591521 41.84699 -8.007472 #> 332  3390591513 41.67739 -8.202297 #> 333  3390591507 41.96613 -8.283180 #> 334  3390591490 41.86642 -8.187904 #> 335  3390591486 42.02826 -8.149611 #> 336  3390591482 41.97498 -8.258943 #> 337  3390591474 41.70449 -8.213981 #> 338  3390591461 39.29008 -7.337507 #> 339  3390591422 41.74918 -8.165329 #> 340  3390591411 41.88026 -7.079013 #> 341  3390591410 41.71192 -8.009551 #> 342  3390591403 41.79265 -7.972206 #> 343  3390591393 41.89377 -8.235774 #> 344  3390591374 42.01016 -8.137777 #> 345  3390591372 39.29926 -7.348887 #> 346  3390591366 41.72250 -8.213761 #> 347  3390591343 42.00167 -8.210336 #> 348  3390591323 41.88886 -7.054650 #> 349  3390591318 41.91036 -6.752582 #> 350  3390591315 41.75725 -8.044940 #> 351  3390591312 41.75782 -8.117103 #> 352  3390591306 41.93986 -8.416165 #> 353  3390591295 41.26835 -7.777515 #> 354  3390591291 41.82919 -8.031831 #> 355  3390591283 41.92297 -6.945082 #> 356  3390591267 41.92929 -6.800157 #> 357  3390591254 40.80315 -8.129872 #> 358  3390591245 41.34077 -7.812017 #> 359  3390591231 41.77453 -7.960463 #> 360  3390591221 41.92995 -8.259464 #> 361  3390591219 41.79243 -7.948139 #> 362  3390591208 41.77602 -8.140917 #> 363  3390591205 41.83745 -7.947403 #> 364  3390591203 41.91805 -7.897845 #> 365  3390591199 41.88477 -8.235881 #> 366  3390591191 37.34516 -8.487790 #> 367  3390591167 41.93986 -8.416165 #> 368  3390591125 41.37654 -7.787450 #> 369  3390591119 41.97440 -8.174466 #> 370  3390591118 37.31886 -8.702402 #> 371  3390591116 41.81911 -7.911583 #> 372  3390591111 41.32382 -7.919859 #> 373  3390591108 41.79394 -8.128643 #> 374  3390591086 41.91793 -7.885789 #> 375  3390591085 41.85590 -7.995288 #> 376  3390591082 41.87347 -7.946812 #> 377  3390591080 41.75673 -7.984807 #> 378  3390591079 41.94204 -7.004784 #> 379  3390591076 41.83767 -7.971486 #> 380  3390591065 40.94556 -7.926045 #> 381  3390591060 41.72171 -8.105575 #> 382  3390591051 42.07328 -8.149011 #> 383  3390591043 41.81044 -7.947845 #> 384  3390591036 41.86480 -7.983100 #> 385  3390591035 41.72180 -8.117596 #> 386  3390591031 41.67801 -8.298398 #> 387  3390591011 41.91816 -7.909902 #> 388  3390591002 41.82122 -8.164394 #> 389  3390590999 41.67763 -8.238334 #> 390  3390590995 41.94747 -8.186877 #> 391  3390590988 41.69548 -8.214090 #> 392  3390590986 41.94367 -7.101237 #> 393  3390590978 41.97506 -8.271011 #> 394  3390590977 41.75801 -8.141157 #> 395  3390590976 42.01934 -8.161808 #> 396  3390590973 41.93261 -6.980956 #> 397  3390590959 41.83974 -8.236417 #> 398  3390590956 41.91545 -7.029728 #> 399  3390590947 41.78586 -8.261123 #> 400  3390590946 41.82823 -7.923470 #> 401  3390590942 41.87358 -7.958860 #> 402  3390590935 41.80237 -8.056308 #> 403  3390590924 37.34531 -8.521658 #> 404  3390590910 40.95510 -7.985299 #> 405  3390590909 41.89826 -7.078474 #> 406  3390590903 37.42683 -8.634159 #> 407  3390590890 41.83799 -8.007611 #> 408  3390590889 41.90004 -7.898155 #> 409  3390590874 41.88529 -8.320242 #> 410  3390590870 41.73151 -8.213652 #> 411  3390590867 42.07337 -8.161097 #> 412  3390590864 41.75773 -8.105076 #> 413  3390590860 41.48300 -7.641741 #> 414  3390590859 41.68648 -8.214200 #> 415  3390590840 41.92159 -8.368094 #> 416  3390590838 41.96635 -8.319381 #> 417  3390590831 41.74909 -8.153303 #> 418  3390590821 41.83065 -8.224482 #> 419  3390590819 41.82196 -8.272749 #> 420  3390590815 41.83949 -8.200288 #> 421  3390590813 41.89430 -8.320146 #> 422  3390590787 41.90331 -8.320051 #> 423  3390590784 41.89992 -7.886102 #> 424  3390590783 41.70415 -8.165911 #> 425  3390590778 42.04627 -8.149371 #> 426  3390590762 41.83689 -7.887196 #> 427  3390590760 41.98341 -8.174350 #> 428  3390590759 41.85547 -7.947107 #> 429  3390590756 41.73099 -8.141517 #> 430  3390590755 41.34965 -7.799902 #> 431  3390590749 42.04644 -8.173534 #> 432  3390590743 41.41243 -7.774819 #> 433  3390590737 41.49296 -7.725386 #> 434  3390590736 41.93880 -8.235237 #> 435  3390590732 39.30875 -7.383459 #> 436  3390590722 41.71212 -8.033588 #> 437  3390590697 41.83756 -7.959444 #> 438  3390590696 37.50798 -8.645079 #> 439  3390590688 41.81204 -8.140437 #> 440  3390590680 41.76541 -7.948580 #> 441  3390590679 41.78331 -7.936255 #> 442  3390590664 41.70312 -8.033723 #> 443  3390590662 41.94788 -8.247192 #> 444  3390590653 41.95720 -8.295346 #> 445  3390590652 41.75852 -8.213322 #> 446  3390590647 41.42024 -7.666979 #> 447  3390590623 41.84833 -8.176088 #> 448  3390590621 41.24362 -8.016652 #> 449  3390590617 41.98391 -8.246768 #> 450  3390590613 41.94854 -8.355764 #> 451  3390590611 41.67786 -8.274373 #> 452  3390590583 41.24219 -7.861537 #> 453  3390590580 41.99233 -8.162162 #> 454  3390590576 41.74026 -8.177469 #> 455  3390590573 41.88452 -8.199727 #> 456  3390590562 41.99267 -8.210448 #> 457  3390590559 41.87302 -7.898619 #> 458  3390590556 41.95734 -8.319477 #> 459  3390590552 37.33655 -8.589445 #> 460  3390590546 41.51149 -7.772954 #> 461  3390590545 41.98383 -8.234699 #> 462  3390590531 37.82378 -8.779920 #> 463  3390590528 37.66117 -8.633012 #> 464  3390590520 39.30859 -7.371864 #> 465  3390590515 41.82173 -8.236631 #> 466  3390590509 37.35461 -8.600638 #> 467  3390590498 41.98329 -6.798299 #> 468  3390590487 37.67916 -8.621582 #> 469  3390590486 42.01051 -8.186075 #> 470  3390590480 41.35914 -7.847547 #> 471  3390590476 39.31776 -7.383252 #> 472  3390590473 41.93469 -6.619057 #> 473  3390590470 41.70351 -8.081790 #> 474  3390590468 37.48996 -8.645164 #> 475  3390590462 41.70370 -8.105825 #> 476  3390590451 37.34491 -8.431345 #> 477  3390590437 41.94140 -6.968616 #> 478  3390590419 39.29909 -7.337294 #> 479  3390590417 41.80999 -7.899699 #> 480  3390590416 41.91162 -8.211447 #> 481  3390590414 41.79709 -8.814649 #> 482  3390590381 41.76645 -8.068864 #> 483  3390590371 42.07292 -8.100665 #> 484  3390590370 39.38081 -7.381799 #> 485  3390590368 41.68569 -8.106074 #> 486  3390590363 41.70332 -8.057757 #> 487  3390590351 41.81231 -8.176549 #> 488  3390590335 41.73125 -8.177584 #> 489  3390590333 41.93903 -8.271421 #> 490  3390590320 37.44508 -8.713205 #> 491  3390590317 41.76692 -8.129008 #> 492  3390590311 41.81288 -8.260813 #> 493  3390590307 41.82218 -8.308869 #> 494  3390590304 41.80266 -8.092414 #> 495  3390590303 41.85557 -7.959153 #> 496  3390590300 41.88498 -6.837833 #> 497  3390590297 41.71270 -8.105700 #> 498  3390590275 39.38900 -7.323552 #> 499  3390590263 41.80098 -7.899853 #> 500  3390590258 41.70424 -8.177928 #> 501  3390590256 41.78511 -8.152830 #> 502  3390590251 41.90904 -7.898000 #> 503  3390590228 37.31860 -8.612115 #> 504  3390590223 41.35853 -7.787784 #> 505  3390590218 41.70361 -8.093807 #> 506  3390590217 41.92694 -7.885632 #> 507  3390590208 41.51957 -7.688912 #> 508  3390590207 41.76800 -8.285389 #> 509  3390590203 40.88240 -7.915197 #> 510  3390590191 41.81118 -8.032102 #> 511  3390590186 41.68605 -8.154129 #> 512  3390590185 41.82845 -7.947550 #> 513  3390590169 41.95019 -6.956272 #> 514  3390590159 41.72198 -8.141637 #> 515  3390590151 41.49242 -7.677481 #> 516  3390590137 41.83966 -8.224374 #> 517  3390590110 41.85798 -6.838743 #> 518  3390590109 41.79438 -8.188814 #> 519  3390590102 41.94161 -6.980672 #> 520  3390590099 41.80247 -8.068343 #> 521  3390590092 41.74844 -8.069124 #> 522  3390590090 41.82019 -8.031967 #> 523  3390590082 41.77593 -8.128887 #> 524  3390590080 41.90113 -6.740851 #> 525  3390590076 41.80976 -7.875627 #> 526  3390590068 41.72993 -8.009275 #> 527  3390590064 41.98323 -8.150210 #> 528  3390590062 41.36766 -7.799571 #> 529  3390590061 41.91231 -8.319955 #> 530  3390590057 41.95040 -6.968330 #> 531  3390590042 41.00003 -7.972721 #> 532  3390590033 39.28074 -7.314540 #> 533  3390590026 41.71333 -8.189833 #> 534  3390590009 41.76664 -8.092922 #> 535  3390590008 41.89765 -7.042329 #> 536  3390590005 37.52598 -8.633677 #> 537  3390589999 41.82008 -8.019928 #> 538  3390589987 41.24320 -7.968923 #> 539  3390589980 39.27240 -7.361110 #> 540  3390589974 41.42038 -7.678942 #> 541  3390589971 41.35037 -7.871608 #> 542  3390589968 41.82140 -8.188473 #> 543  3390589950 41.25071 -7.813649 #> 544  3390589924 39.27071 -7.245228 #> 545  3390589922 41.92079 -8.235452 #> 546  3390589910 42.01034 -8.161926 #> 547  3390589905 41.80075 -7.875784 #> 548  3390589901 41.77464 -7.972493 #> 549  3390589894 41.68614 -8.166143 #> 550  3390589875 41.87551 -8.199840 #> 551  3390589860 41.73013 -8.033318 #> 552  3390589851 39.39800 -7.323337 #> 553  3390589843 41.80304 -8.140557 #> 554  3390589842 41.81899 -7.899545 #> 555  3390589838 41.92361 -6.981239 #> 556  3390589813 37.35472 -8.634512 #> 557  3390589812 41.74943 -8.201406 #> 558  3390589806 41.91194 -8.259672 #> 559  3390589803 41.71261 -8.093681 #> 560  3390589800 41.71252 -8.081662 #> 561  3390589794 41.84589 -7.887040 #> 563  3390589783 39.20817 -7.281532 #> 564  3390589780 41.81077 -7.983955 #> 565  3390589774 37.36377 -8.645760 #> 566  3390589772 41.70432 -8.189946 #> 567  3390589771 41.72982 -7.997253 #> 568  3390589754 42.05545 -8.173418 #> 569  3390589750 42.03735 -8.161571 #> 570  3390589741 41.96548 -8.186648 #> 571  3390589740 41.99299 -8.258734 #> 572  3390589717 41.99307 -8.270806 #> 573  3390589699 41.33164 -7.800233 #> 574  3390589684 41.88500 -8.272035 #> 575  3390589683 41.87293 -6.681597 #> 576  3390589680 41.47508 -7.737712 #> 577  3390589674 41.79221 -7.924073 #> 578  3390589651 41.81977 -7.983812 #> 579  3390589649 41.77574 -8.104826 #> 580  3390589648 41.84689 -7.995429 #> 581  3390589647 42.03726 -8.149491 #> 582  3390589630 41.71232 -8.057625 #> 583  3390589622 41.85806 -8.284388 #> 584  3390589617 41.82029 -8.044005 #> 585  3390589616 41.71222 -8.045607 #> 586  3390589609 42.01916 -8.137655 #> 587  3390589604 37.40006 -8.735973 #> 588  3390589599 41.37667 -7.799406 #> 589  3390589595 41.50129 -7.665320 #> 590  3390589585 41.76552 -7.960608 #> 591  3390589579 41.75818 -8.165212 #> 592  3390589569 37.39998 -8.702079 #> 593  3390589564 41.93073 -8.392126 #> 594  3390589558 41.79412 -8.152711 #> 595  3390589553 41.77611 -8.152948 #> 596  3390589541 41.70406 -8.153893 #> 597  3390589530 41.92717 -7.909749 #> 598  3390589529 41.78554 -8.212992 #> 599  3390589524 39.33561 -7.371238 #> 600  3390589523 41.87534 -8.175741 #> 601  3390589522 37.31822 -8.510545 #> 602  3390589494 41.39575 -7.906708 #> 603  3390589484 42.05562 -8.197584 #> 604  3390589483 41.73117 -8.165562 #> 605  3390589475 41.48435 -7.761488 #> 606  3390589468 37.32757 -8.600782 #> 607  3390589457 37.49879 -8.588557 #> 608  3390589454 41.49283 -7.713410 #> 609  3390589453 39.19830 -7.223871 #> 610  3390589450 41.71349 -8.213871 #> 611  3390589448 37.34544 -8.555527 #> 612  3390589441 41.76625 -8.044807 #> 613  3390589433 41.73963 -8.093302 #> 614  3390589432 41.86413 -7.910820 #> 615  3390589410 41.81499 -8.754400 #> 616  3390589408 41.95287 -7.113029 #> 617  3390589400 41.76753 -8.213212 #> 618  3390589399 41.89416 -8.296039 #> 619  3390589378 41.33225 -7.859971 #> 620  3390589375 41.76701 -8.141037 #> 621  3390589368 41.80312 -8.152593 #> 622  3390589364 41.75792 -8.129130 #> 623  3390589361 41.80257 -8.080379 #> 624  3390589360 41.74783 -7.996973 #> 625  3390589356 41.73872 -7.985090 #> 626  3390589352 41.69540 -8.202074 #> 627  3390589343 41.88191 -7.886415 #> 628  3390589331 41.88203 -7.898465 #> 629  3390589324 41.81318 -8.308965 #> 630  3390589323 41.76736 -8.189154 #> 631  3390589285 41.38567 -7.799240 #> 632  3390589279 41.73159 -8.225674 #> 633  3390589270 41.82188 -8.260710 #> 634  3390589269 39.32578 -7.313460 #> 635  3390589266 41.43958 -7.786278 #> 636  3390589262 41.34977 -7.811853 #> 637  3390589233 41.35995 -7.931217 #> 638  3390589194 41.81097 -8.008028 #> 639  3390589191 41.73090 -8.129495 #> 640  3390589189 41.32393 -7.931806 #> 641  3390589188 41.74009 -8.153421 #> 642  3390589185 41.73108 -8.153539 #> 643  3390589178 41.82899 -8.007750 #> 644  3390589175 41.97127 -7.136623 #> 645  3390589171 41.91238 -8.332012 #> 646  3390589154 41.81280 -8.248775 #> 647  3390589148 42.05527 -8.149251 #> 648  3390589144 41.69514 -8.166027 #> 649  3390589141 41.85767 -8.224156 #> 650  3390589139 41.97474 -8.222738 #> 651  3390589132 41.97563 -8.367561 #> 652  3390589125 37.76955 -8.711957 #> 653  3390589120 40.83035 -8.153236 #> 654  3390589119 41.81108 -8.020065 #> 655  3390589118 37.32793 -8.724940 #> 656  3390589117 37.34521 -8.499080 #> 657  3390589109 41.38542 -7.775326 #> 658  3390589099 42.06446 -8.173301 #> 659  3390589094 39.32709 -7.406241 #> 660  3390589087 41.35878 -7.811689 #> 661  3390589078 41.69506 -8.154011 #> 662  3390589075 42.04662 -8.197698 #> 663  3390589072 41.84905 -8.284488 #> 664  3390589068 40.99992 -7.960833 #> 665  3390589051 41.95697 -8.259151 #> 666  3390589050 41.86458 -7.959006 #> 667  3390589048 39.25321 -7.280432 #> 668  3390589042 40.89141 -7.915050 #> 669  3390589040 41.81247 -8.200624 #> 670  3390589039 41.80087 -7.887819 #> 671  3390589032 41.75810 -8.153185 #> 672  3390589030 41.72233 -8.189720 #> 673  3390589026 41.79403 -8.140677 #> 674  3390589014 41.80295 -8.128521 #> 675  3390588999 41.71324 -8.177814 #> 676  3390588996 41.92417 -6.547096 #> 677  3390588968 41.85798 -8.272342 #> 678  3390588966 41.76728 -8.177125 #> 679  3390588937 37.32745 -8.566920 #> 680  3390588932 41.89126 -7.922413 #> 681  3390588929 41.85783 -8.248249 #> 682  3390588908 41.76710 -8.153066 #> 683  3390588902 37.39989 -8.668185 #> 684  3390588896 41.78475 -8.104700 #> 685  3390588893 41.81010 -7.911735 #> 686  3390588883 41.82929 -8.043871 #> 687  3390588878 41.76777 -8.249301 #> 688  3390588876 41.85733 -8.175972 #> 689  3390588874 41.92029 -6.800467 #> 690  3390588869 41.97482 -8.234806 #> 691  3390588864 41.83111 -8.296730 #> 692  3390588860 37.31856 -8.600830 #> 693  3390588855 41.40405 -7.834791 #> 694  3390588844 41.73134 -8.189607 #> 695  3390588825 41.78343 -7.948286 #> 696  3390588823 41.93918 -8.295545 #> 697  3390588813 37.48981 -8.599918 #> 698  3390588812 42.07310 -8.124838 #> 699  3390588807 42.02861 -8.197924 #> 700  3390588802 39.32628 -7.348252 #> 701  3390588801 41.77485 -7.996553 #> 702  3390588796 42.01068 -8.210225 #> 703  3390588782 41.34101 -7.835915 #> 704  3390588776 41.92340 -6.969187 #> 705  3390588775 42.01977 -8.222190 #> 706  3390588773 41.83734 -7.935361 #> 707  3390588771 41.78520 -8.164862 #> 708  3390588763 37.30019 -8.510662 #> 709  3390588762 41.69488 -8.129980 #> 710  3390588755 41.85568 -7.971198 #> 711  3390588753 41.70388 -8.129859 #> 712  3390588750 41.99291 -8.246662 #> 713  3390588743 41.70473 -8.250034 #> 714  3390588728 41.88484 -8.247932 #> 715  3390588726 41.90769 -6.620061 #> 716  3390588723 41.26032 -7.873159 #> 717  3390588722 37.64311 -8.621765 #> 718  3390588716 41.81888 -7.887507 #> 719  3390588708 41.72207 -8.153657 #> 720  3390588690 41.93911 -8.283483 #> 721  3390588687 42.03744 -8.173651 #> 722  3390588686 41.74034 -8.189493 #> 723  3390588666 42.03680 -8.089093 #> 724  3390588662 41.80987 -7.887663 #> 725  3390588637 41.40278 -7.715187 #> 726  3390588622 41.77715 -8.309352 #> 727  3390588620 41.27856 -7.896726 #> 728  3390588611 41.86447 -7.946960 #> 729  3390588608 41.81966 -7.971774 #> 730  3390588597 41.71298 -8.141757 #> 731  3390588595 41.80276 -8.104450 #> 732  3390588589 41.94318 -6.594616 #> 733  3390588565 37.81470 -8.745863 #> 734  3390588563 37.53495 -8.622314 #> 735  3390588557 41.75715 -8.032914 #> 736  3390588534 40.66855 -8.202608 #> 737  3390588529 41.83712 -7.911278 #> 738  3390588524 41.83723 -7.923320 #> 739  3390588520 41.96483 -6.774802 #> 740  3390588517 41.78406 -8.020476 #> 741  3390588515 41.38604 -7.835113 #> 742  3390588514 37.76953 -8.700602 #> 743  3390588510 41.88006 -7.066968 #> 744  3390588503 41.72282 -8.261846 #> 745  3390588502 37.45390 -8.645334 #> 746  3390588498 41.92547 -7.089717 #> 747  3390588493 41.82180 -8.248670 #> 748  3390588487 41.80285 -8.116485 #> 749  3390588486 41.96552 -6.810978 #> 750  3390588483 41.69588 -8.274170 #> 751  3390588481 42.03761 -8.197811 #> 752  3390588479 41.99224 -8.150091 #> 753  3390588471 37.31877 -8.668544 #> 754  3390588468 41.87369 -7.970909 #> 755  3390588458 42.00133 -8.162044 #> 756  3390588451 42.02789 -8.101300 #> 757  3390588440 40.41881 -8.759554 #> 758  3390588438 41.71307 -8.153776 #> 759  3390588433 41.85775 -8.236203 #> 761  3390588430 37.27376 -8.691301 #> 762  3390588423 39.38998 -7.393199 #> 763  3390588419 41.74824 -8.045074 #> 764  3390588417 41.94874 -8.391955 #> 765  3390588416 41.81055 -7.959881 #> 766  3390588408 37.30955 -8.600878 #> 767  3390588402 41.82867 -7.971630 #> 768  3390588399 41.98332 -8.162280 #> 769  3390588396 41.72216 -8.165678 #> 770  3390588376 37.31852 -8.589544 #> 771  3390588355 37.34560 -8.600686 #> 772  3390588347 41.78465 -8.092668 #> 773  3390588343 41.38555 -7.787283 #> 774  3390588342 41.72113 -8.033453 #> 775  3390588340 41.26944 -7.884941 #> 776  3390588324 41.71316 -8.165795 #> 777  3390588321 41.88845 -7.030557 #> 778  3390588320 41.43945 -7.774311 #> 779  3390588319 41.42194 -7.822505 #> 780  3390588318 41.85490 -7.886883 #> 781  3390588315 42.00192 -8.246556 #> 782  3390588310 39.34510 -7.405832 #> 783  3384283434 41.31493 -8.257731 #> 784  3384032591 40.89956 -8.236110 #> 785  3383987429 41.31493 -8.257731 #> 786  3355554412 41.16513 -8.024694 #> 787  3355482953 39.76608 -8.722703 #> 788  3355429222 41.88071 -8.268456 #> 789  3355111133 41.31493 -8.257731 #> 790  3344133771 39.49644 -9.053765 #> 791  3344058720 40.32542 -7.679457 #> 792  3343928818 41.71674 -8.306244 #> 793  3338021879 40.67482 -8.147830 #> 794  3337969276 41.85034 -8.708264 #> 795  3337814053 40.45982 -8.699257 #> 796  3337522793 41.90394 -8.313740 #> 797  3333553516 41.19651 -8.286673 #> 798  3333322755 41.19677 -8.286339 #> 799  3333087262 41.30550 -8.013683 #> 800  3330449692 41.33464 -7.803995 #> 801  3329293007 40.79827 -7.930406 #> 802  3329292643 40.62208 -8.360187 #> 804  3329292489 41.47911 -8.463901 #> 805  3329292218 41.43395 -8.163685 #> 806  3329291164 41.30013 -8.450164 #> 807  3329290889 40.23326 -7.252807 #> 808  3329290293 41.59946 -6.982664 #> 809  3329289525 40.30044 -7.037613 #> 810  3329289416 41.75964 -8.508078 #> 811  3329289405 40.43793 -7.682814 #> 812  3329289345 40.18139 -7.937795 #> 813  3329289078 40.81415 -7.741975 #> 814  3329288931 41.53684 -8.136827 #> 815  3329288878 39.32383 -7.409703 #> 816  3329288495 40.29993 -6.966788 #> 817  3329287995 40.80019 -7.865224 #> 818  3329287794 38.28950 -8.062067 #> 819  3329287308 40.54416 -8.191079 #> 820  3329286256 39.32098 -7.318697 #> 821  3329286062 41.20594 -7.337775 #> 822  3329285642 41.77013 -8.427015 #> 823  3329285403 41.80095 -8.808292 #> 824  3329284841 41.92975 -6.851007 #> 825  3329284082 41.88093 -8.546836 #> 826  3329283912 40.90548 -7.653728 #> 828  3329283703 40.17311 -7.932686 #> 829  3329283485 40.45912 -7.139199 #> 830  3329283360 41.35113 -8.275640 #> 831  3329283212 40.92608 -7.189482 #> 832  3329283194 40.68944 -8.006563 #> 833  3329283189 39.32081 -7.318469 #> 834  3329283151 40.98469 -8.016802 #> 835  3329282785 41.21284 -7.325327 #> 836  3329282652 40.62333 -8.363190 #> 837  3329282362 40.06480 -8.003267 #> 838  3329281650 40.98857 -8.369366 #> 839  3329281274 41.52545 -7.980517 #> 840  3329281147 40.11942 -8.550599 #> 841  3329280937 40.05002 -8.005264 #> 842  3329280489 40.80193 -8.123279 #> 843  3329280435 41.88278 -8.556417 #> 844  3329280402 39.30109 -7.349788 #> 845  3329280401 39.87476 -7.978568 #> 846  3329280295 40.75816 -8.595894 #> 847  3329280068 40.32426 -8.304472 #> 848  3329280050 40.79479 -7.935689 #> 849  3329279931 40.81977 -7.944811 #> 850  3329278939 41.00465 -7.040703 #> 851  3329278834 41.00228 -7.965674 #> 852  3329278573 40.17715 -8.426964 #> 853  3329277877 41.46072 -8.279451 #> 855  3329277708 40.33114 -8.307957 #> 856  3329277146 40.30354 -8.688116 #> 857  3329276875 40.17129 -8.397171 #> 858  3329276267 40.76254 -7.909937 #> 859  3329274822 41.21222 -7.328873 #> 860  3329274757 38.14434 -8.050680 #> 861  3329274207 41.46729 -6.881023 #> 862  3329274076 40.09568 -8.537851 #> 863  3329273492 40.84940 -7.512144 #> 864  3329273030 41.89388 -8.555810 #> 865  3329272959 40.04882 -8.002632 #> 866  3329272801 40.34474 -8.713571 #> 867  3329272798 40.64800 -7.833212 #> 868  3329272294 40.74691 -8.199169 #> 869  3329272221 39.47958 -8.232746 #> 870  3329272189 40.55999 -8.025016 #> 871  3329272143 41.89364 -8.555848 #> 872  3329272091 41.71129 -8.665769 #> 873  3329271080 40.84172 -8.597462 #> 874  3329270605 40.35631 -8.049071 #> 875  3329270302 41.44566 -8.753304 #> 876  3329270193 41.40641 -8.204237 #> 877  3329270137 40.59277 -7.629190 #> 878  3329269664 40.55015 -8.236240 #> 879  3329268998 40.22743 -7.832010 #> 880  3329268625 39.40389 -9.074734 #> 881  3329268601 40.06502 -8.102769 #> 882  3329268500 40.31277 -7.003000 #> 883  3329268250 41.50371 -8.242478 #> 884  3329268211 40.42511 -8.754608 #> 885  3329268192 41.71394 -8.497860 #> 886  3329268059 40.99385 -7.342022 #> 887  3329267484 40.78550 -8.350278 #> 888  3329267045 39.97873 -8.576666 #> 889  3329266549 39.99827 -8.744826 #> 890  3329265979 41.01000 -7.020330 #> 891  3329265750 40.61655 -8.608747 #> 892  3329265659 40.17984 -8.423253 #> 893  3329265562 41.16039 -7.338355 #> 894  3329265299 41.22146 -8.027222 #> 895  3329265296 41.31373 -8.554012 #> 896  3329265256 40.07628 -7.725050 #> 897  3329265135 41.21260 -7.320383 #> 898  3329265058 40.88559 -7.909962 #> 899  3329264463 39.13289 -8.677529 #> 900  3329264362 40.73346 -8.403175 #> 901  3329264206 40.32114 -7.594683 #> 902  3329264135 40.88774 -7.913250 #> 903  3329263242 39.79958 -8.770150 #> 904  3329263172 41.32134 -8.729542 #> 905  3329263117 39.77781 -8.035293 #> 906  3329262770 41.27965 -8.731684 #> 907  3329262614 39.72223 -8.510228 #> 908  3329262405 41.40157 -8.075658 #> 909  3329262357 40.86014 -7.912871 #> 910  3329262321 41.89324 -8.520350 #> 911  3329261495 40.76322 -8.170042 #> 912  3329261363 40.85510 -8.351734 #> 913  3329261335 41.66934 -8.665709 #> 914  3329261289 40.83051 -8.158470 #> 915  3329261080 41.75645 -8.459314 #> 916  3329260948 41.50521 -7.251483 #> 917  3329260253 39.81100 -8.253795 #> 918  3329260124 39.44980 -9.091008 #> 919  3329260102 40.09333 -7.911411 #> 920  3329259974 41.66926 -8.687368 #> 921  3329257778 40.80162 -8.627247 #> 922  3329257684 39.81313 -8.222964 #> 923  3329257621 40.00383 -8.116083 #> 924  3329257280 41.78401 -7.176541 #> 925  3329256605 39.30069 -7.350284 #> 926  3329256397 40.60728 -8.369051 #> 927  3329256396 41.39631 -8.095865 #> 928  3329256334 39.74861 -8.713499 #> 929  3329255580 39.70804 -8.604141 #> 930  3329255378 41.76828 -8.425119 #> 931  3329255186 40.81262 -7.751607 #> 932  3329255118 40.97396 -8.641266 #> 933  3329254004 41.87921 -8.706823 #> 934  3329253774 41.03691 -8.306363 #> 935  3329253656 40.23111 -7.830172 #> 936  3329253374 40.17881 -8.423344 #> 937  3329252898 41.52615 -8.563536 #> 938  3329252897 40.04944 -7.999505 #> 939  3329252876 40.89557 -7.975600 #> 940  3329252841 40.68882 -8.005756 #> 941  3329252719 39.27257 -7.366605 #> 942  3329252391 40.33365 -6.789421 #> 943  3329252258 41.41467 -8.188689 #> 944  3329252102 41.29927 -8.450195 #> 945  3329252028 41.47109 -8.578420 #> 946  3329251770 39.87230 -8.345529 #> 947  3329251524 41.44397 -7.673695 #> 948  3329251287 40.86075 -7.913882 #> 949  3329249557 39.20990 -7.293589 #> 950  3329249265 40.33659 -7.160206 #> 951  3329248958 40.14062 -8.560379 #> 952  3329248791 39.82900 -8.190883 #> 953  3329248578 40.08343 -8.606221 #> 954  3329248004 39.97836 -8.780291 #> 955  3329247839 39.71666 -8.466260 #> 956  3329247089 39.21079 -7.294957 #> 957  3329246963 41.58490 -8.214442 #> 958  3329246632 41.52477 -7.980096 #> 959  3329246610 41.22547 -8.459798 #> 960  3329246597 40.07315 -8.655712 #> 961  3329246330 41.52504 -7.980344 #> 962  3329246328 39.69417 -8.270858 #> 963  3329246131 40.67806 -8.130962 #> 964  3329245783 40.28729 -6.887813 #> 965  3329245454 40.83295 -8.156684 #> 966  3329244895 41.16444 -7.348718 #> 967  3329243367 39.39241 -7.327004 #> 968  3329243036 41.37094 -7.999303 #> 969  3329242919 40.42877 -8.588924 #> 970  3329242503 40.94492 -7.934767 #> 971  3329242288 41.20867 -7.476886 #> 972  3329242226 41.43275 -8.163485 #> 973  3329242179 41.56036 -8.101197 #> 974  3329241813 40.88610 -7.910346 #> 975  3329241617 39.27173 -7.365801 #> 976  3329241457 40.22941 -7.826170 #> 977  3329241354 39.33458 -7.373922 #> 978  3329241325 41.00033 -7.959415 #> 979  3329241313 40.38082 -7.213919 #> 980  3329241311 40.10979 -8.135222 #> 981  3329240837 40.31269 -7.004638 #> 983  3329240656 39.75410 -8.273740 #> 984  3329240509 39.97857 -8.781403 #> 985  3329240503 41.10537 -8.291460 #> 986  3329240371 41.69253 -8.484147 #> 987  3329240302 41.89251 -8.553469 #> 988  3329240177 40.57657 -7.468421 #> 989  3329239499 40.83094 -8.040520 #> 990  3329238885 40.57944 -7.986429 #> 991  3329238706 41.52142 -7.232687 #> 992  3329237581 40.06742 -8.731552 #> 993  3329237541 41.71913 -6.857837 #> 994  3329237325 40.31319 -6.988419 #> 995  3329237307 41.03934 -7.510086 #> 996  3329236759 40.00478 -8.109193 #> 997  3329236660 41.13220 -8.230756 #> 998  3329236628 40.81532 -7.742118 #> 999  3329236543 40.18139 -7.937795 #> 1000 3329235840 41.16039 -7.338355 #> 1001 3329235624 40.17879 -8.423344 #> 1002 3329235538 40.07722 -7.489520 #> 1003 3329235027 40.74497 -8.200614 #> 1004 3329234984 40.90220 -8.017028 #> 1005 3329234891 41.32958 -8.215425 #> 1006 3329234444 40.10671 -8.635760 #> 1007 3329233983 40.44031 -7.598147 #> 1008 3329233821 41.01649 -7.596240 #> 1009 3329233670 39.60229 -8.290854 #> 1010 3329232644 40.77072 -7.328269 #> 1011 3329232265 40.23934 -7.262936 #> 1012 3329232002 40.95994 -7.041040 #> 1013 3329231973 41.47861 -8.458120 #> 1014 3329231279 39.25212 -7.307283 #> 1015 3329231248 40.00846 -8.365027 #> 1016 3329230868 39.13883 -8.739823 #> 1017 3329230750 40.97773 -8.319131 #> 1018 3329230582 40.36025 -8.305775 #> 1019 3329230479 39.74024 -8.711655 #> 1020 3329229853 40.33376 -6.788417 #> 1021 3329229550 41.31180 -8.693084 #> 1022 3329228895 41.97741 -6.796891 #> 1023 3329228573 41.89390 -8.555810 #> 1024 3329228057 40.56777 -8.138864 #> 1025 3329228049 41.32126 -8.223232 #> 1026 3329227210 39.39429 -7.324660 #> 1027 3329226869 40.26048 -7.096915 #> 1028 3329226802 40.92141 -8.382510 #> 1029 3329226486 40.50972 -7.361537 #> 1030 3329226099 39.73339 -8.059449 #> 1031 3329225343 41.93110 -8.532780 #> 1032 3329225143 39.47505 -8.240865 #> 1033 3329224148 40.12043 -7.588182 #> 1034 3329224014 40.35682 -8.271140 #> 1035 3329223556 40.30103 -7.081481 #> 1036 3329223477 41.80021 -8.818573 #> 1037 3329223385 40.77072 -7.328269 #> 1038 3329222702 40.08596 -7.865174 #> 1039 3329222597 40.59228 -7.628751 #> 1040 3329222569 40.99286 -7.343711 #> 1041 3329222385 41.39733 -8.077034 #> 1042 3329221987 41.33928 -8.233368 #> 1043 3329221800 41.16039 -7.338355 #> 1044 3329221795 40.08337 -8.685898 #> 1045 3329221718 41.89706 -8.558343 #> 1046 3329221669 41.87868 -8.542236 #> 1047 3329221429 39.74366 -8.712551 #> 1048 3329221247 41.03665 -8.306188 #> 1049 3329219888 41.47375 -8.267618 #> 1050 3329219813 41.65409 -7.175876 #> 1051 3329219266 41.89469 -8.557419 #> 1053 3329218886 40.29223 -6.881883 #> 1054 3329218486 40.93601 -8.339373 #> 1055 3329218222 39.19787 -7.235137 #> 1056 3329217651 39.72253 -8.509911 #> 1057 3329217509 39.79390 -7.464717 #> 1058 3329217484 40.71705 -7.353480 #> 1059 3329217480 39.78458 -8.628064 #> 1060 3329217268 41.89030 -8.530534 #> 1061 3329217196 40.65565 -7.845097 #> 1062 3329217076 40.92800 -7.059585 #> 1063 3329216967 40.97365 -8.324902 #> 1064 3329216709 40.37176 -7.129368 #> 1065 3329216595 40.36019 -8.286850 #> 1066 3329216398 40.79310 -7.861979 #> 1067 3329215990 39.33543 -7.374622 #> 1068 3329215429 40.94187 -7.929803 #> 1069 3329214435 41.96264 -7.106005 #> 1070 3329214108 41.60223 -8.602288 #> 1071 3329213871 40.74497 -8.200614 #> 1072 3329213226 39.26543 -7.297875 #> 1073 3329213107 40.03876 -8.193203 #> 1075 3329213063 40.07160 -8.338665 #> 1076 3329212780 41.90885 -8.550545 #> 1077 3329212737 40.69075 -8.004307 #> 1078 3329212394 41.52819 -8.189849 #> 1079 3329212361 41.89684 -8.497238 #> 1080 3329212162 40.67135 -8.200169 #> 1081 3329211674 41.81407 -7.919885 #> 1082 3329211435 41.44390 -8.750497 #> 1083 3329211410 39.31996 -7.999063 #> 1084 3327946053 40.32822 -7.586815 #> 1085 3327764029 40.65869 -8.149047 #> 1086 3325960476 40.72036 -8.538632 #> 1087 3325781989 40.32800 -7.587047 #> 1088 3321164780 41.83132 -7.942085 #> 1089 3307291805 41.49652 -8.243656 #> 1090 3307271877 40.32037 -8.566488 #> 1091 3307182822 41.78869 -8.591312 #> 1092 3307161738 41.49651 -8.243683 #> 1093 3302539581 41.50533 -8.161291 #> 1094 3302233883 41.31493 -8.257731 #> 1095 3301982830 41.38788 -8.265899 #> 1096 3301826816 41.33477 -7.803562 #> 1097 3124825852 42.03279 -8.163900 #> 1098 3124779788 42.05133 -8.198026 #> 1099 3124754035 41.22317 -8.561836 #> 1100 3124604519 39.44886 -9.133770 #> 1101 3124488826 41.87712 -8.256831 #> 1102 3118420503 41.15581 -8.408566 #> 1103 3118391236 39.43973 -9.143595 #> 1104 3117881727 41.09676 -8.484416 #> 1105 3113529636 41.52669 -8.630224 #> 1106 3112407006 41.23718 -8.709723 #> 1107 3109252420 41.80656 -8.858371 #> 1108 3109183625 41.40160 -8.217143 #> 1109 3097274255 40.63201 -8.109882 #> 1110 3097106695 41.80341 -8.128234 #> 1111 3090715918 41.71145 -8.817823 #> 1112 3079864025 41.21568 -8.251008 #> 1113 3079862130 41.13973 -8.269148 #> 1114 3079584556 40.11270 -8.514184 #> 1115 3070748544 41.12344 -8.437679 #> 1116 3067983204 40.84669 -8.474585 #> 1119 2992761833 40.99824 -8.621130 #> 1120 2898528714 40.24081 -7.591225 #> 1121 2874008486 37.32097 -8.548824 #> 1122 2845585733 41.75710 -8.149500 #> 1123 2845542923 43.28971 -8.491898 #> 1124 2845516149 43.30898 -8.477036 #> 1125 2845349904 43.31149 -8.507613 #> 1126 2845334615 43.32957 -8.497227 #> 1127 2845293407 43.28848 -6.434583 #> 1128 2845132346 43.28847 -8.581265 #> 1129 2844723836 43.33248 -4.874070 #> 1130 2844618545 40.25159 -5.652627 #> 1131 2844378197 43.04453 -6.254827 #> 1132 2844154155 43.29387 -8.555052 #> 1133 2843867361 40.15298 -8.236695 #> 1134 2843562133 40.19912 -5.746585 #> 1135 2843403263 43.31809 -8.456978 #> 1136 2843301951 40.27000 -5.240000 #> 1137 2843093231 41.96983 -8.373956 #> 1138 2843004378 40.21925 -5.696695 #> 1139 2842929767 42.43000 -8.570000 #> 1140 2842870786 41.75000 -8.150000 #> 1141 2842651692 40.24732 -5.630430 #> 1142 2842642419 42.12057 -6.698999 #> 1143 2842549036 41.07755 -3.466298 #> 1144 2842359348 43.34130 -8.477517 #> 1145 2842334059 40.05896 -8.257942 #> 1146 2842244019 43.29786 -8.424032 #> 1147 2842197327 40.18448 -5.826730 #> 1148 2842193042 39.48495 -5.394226 #> 1149 2841838788 43.33014 -4.880765 #> 1150 2841364775 40.21499 -5.751517 #> 1151 2841355546 40.31148 -5.286775 #> 1152 2841329348 40.33873 -5.130339 #> 1153 2841316543 40.24914 -5.268631 #> 1154 2841129966 43.03182 -6.866026 #> 1155 2841066720 40.11677 -5.777357 #> 1156 2840890164 40.33886 -5.130678 #> 1157 2840889720 40.27000 -5.240000 #> 1158 2840870836 41.73683 -8.199550 #> 1159 2840830738 40.27000 -5.240000 #> 1160 2840594896 40.27000 -5.240000 #> 1161 2840525475 40.06247 -5.856696 #> 1162 2840523224 43.11069 -5.977732 #> 1163 2840312658 40.27000 -5.240000 #> 1164 2840306004 41.08000 -3.450000 #> 1165 2840303817 42.89175 -6.795387 #> 1166 2840292852 41.97207 -8.374844 #> 1167 2840244813 40.27000 -5.240000 #> 1168 2840185623 40.27000 -5.240000 #> 1169 2840162233 40.17000 -5.650300 #> 1170 2840145487 40.75691 -3.904489 #> 1171 2840142275 39.49710 -8.231414 #> 1172 2840134830 41.73141 -8.208388 #> 1173 2840129413 41.76089 -8.123175 #> 1175 2840082387 41.18265 -8.231380 #> 1176 2840042179 41.66050 -8.173500 #> 1177 2840017878 40.24707 -5.095995 #> 1178 2839982802 40.99556 -7.930924 #> 1179 2839974632 40.18682 -7.878788 #> 1180 2839972578 40.86818 -8.287479 #> 1181 2839953172 41.67700 -7.714682 #> 1182 2839928622 40.27000 -5.240000 #> 1183 2839907297 40.88343 -3.735452 #> 1184 2839825133 40.21632 -7.919050 #> 1185 2839809180 42.63835 -7.143388 #> 1186 2839682166 43.32225 -8.451903 #> 1187 2839599293 41.77272 -8.173882 #> 1188 2839543911 39.41503 -7.450761 #> 1189 2839434925 43.15211 -5.600696 #> 1190 2839407375 42.11769 -6.715107 #> 1191 2839356244 40.25123 -5.287739 #> 1192 2839310357 40.36715 -7.727330 #> 1193 2839295245 40.31514 -5.770912 #> 1194 2839207300 42.13130 -6.730400 #> 1195 2839166262 42.12280 -6.746807 #> 1196 2839034042 40.93071 -8.234135 #> 1197 2839030636 40.47957 -6.117539 #> 1198 2838823725 40.27557 -5.231838 #> 1199 2838791606 40.27000 -5.240000 #> 1200 2838778687 40.27000 -5.240000 #> 1201 2838752247 41.76209 -8.209383 #> 1202 2838735563 40.50179 -6.220665 #> 1203 2838676866 42.80724 -6.760454 #> 1204 2838669598 40.48351 -6.109772 #> 1205 2838634562 40.49187 -6.195946 #> 1206 2838608913 40.20221 -5.756310 #> 1207 2838532905 40.72205 -8.114642 #> 1208 2838456682 41.76000 -8.150000 #> 1209 2838369301 42.56408 -7.223082 #> 1210 2838341128 39.45570 -8.341199 #> 1211 2838329042 40.76740 -3.991851 #> 1212 2837969737 43.30493 -8.459416 #> 1213 2837841247 40.30061 -5.056714 #> 1214 2837792104 40.46311 -6.149812 #> 1215 2837519261 43.32665 -8.409953 #> 1216 2837330897 40.13678 -5.667590 #> 1217 2837286494 40.11677 -5.777357 #> 1218 2837266233 43.09498 -7.046270 #> 1219 2836762392 41.75760 -8.193000 #> 1220 2836746137 41.08000 -3.460000 #> 1221 2836705492 40.18757 -5.149333 #> 1222 2836683836 40.31000 -5.200000 #> 1223 2836414814 40.27000 -5.240000 #> 1224 2836355137 37.63849 -8.620611 #> 1225 2836335900 41.03139 -8.046394 #> 1226 2836330062 41.07724 -3.464272 #> 1227 2836300204 41.36648 -7.790171 #> 1228 2836285101 40.19830 -5.300700 #> 1229 2836186446 43.33715 -8.358267 #> 1230 2835994932 40.37393 -7.517241 #> 1231 2835979614 40.07515 -8.229189 #> 1232 2835972608 40.27000 -5.240000 #> 1233 2835956692 40.27000 -5.240000 #> 1234 2835836112 40.23184 -5.270004 #> 1235 2835730323 42.58437 -7.109184 #> 1236 2835676384 40.36617 -5.730057 #> 1237 2835671705 40.31000 -5.200000 #> 1238 2835605846 41.73529 -8.204877 #> 1239 2835556300 40.34079 -5.166244 #> 1240 2835520583 40.27000 -5.240000 #> 1241 2835315306 43.30859 -8.539913 #> 1242 2835249043 40.38443 -7.704903 #> 1243 2835228741 41.90432 -6.380310 #> 1244 2835218217 40.24479 -7.950605 #> 1245 2835202861 40.13605 -5.667826 #> 1246 2834983824 43.09492 -7.045802 #> 1247 2834817357 40.06247 -5.856696 #> 1248 2834361613 41.74000 -8.170000 #> 1249 2834285996 40.38311 -7.545340 #> 1250 2834282668 40.38310 -7.545344 #> 1251 2834210083 43.33548 -8.479428 #> 1252 2834106611 40.25759 -5.655870 #> 1253 2833589613 40.06848 -5.711360 #> 1254 2833491788 42.08339 -8.678606 #> 1255 2833359648 42.33765 -8.444519 #> 1256 2833144483 40.15827 -5.656801 #> 1257 2833038154 40.11728 -5.777557 #> 1258 2833006260 40.32750 -5.130056 #> 1259 2832994801 41.97207 -8.374844 #> 1260 2832994638 41.03139 -8.046394 #> 1261 2832984061 40.22100 -5.141100 #> 1262 2832932466 41.57798 -8.230280 #> 1263 2832927493 40.16900 -5.650477 #> 1264 2832919246 40.27000 -5.240000 #> 1265 2832795760 41.13430 -8.664462 #> 1266 2832761972 40.22096 -5.749867 #> 1267 2832685385 42.10434 -6.770668 #> 1268 2832674015 40.36523 -5.045183 #> 1269 2832601288 40.36002 -5.762501 #> 1270 2832583582 41.78928 -8.153801 #> 1271 2832582758 40.27219 -5.234327 #> 1272 2832519117 40.12036 -5.776772 #> 1273 2832517081 40.45940 -6.144000 #> 1274 2832480115 39.36000 -7.390000 #> 1275 2832400951 43.02674 -6.863365 #> 1276 2832295330 42.63374 -7.133861 #> 1277 2832293209 42.58584 -7.055038 #> 1278 2832292205 41.80000 -8.140000 #> 1279 2832282319 40.24507 -7.950164 #> 1280 2832271078 40.36735 -7.727165 #> 1281 2832258818 42.75239 -8.173088 #> 1282 2832246684 40.18118 -7.861965 #> 1283 2832217151 40.21632 -7.918836 #> 1285 2626338858 41.82110 -8.297938 #> 1286 2626301455 38.79306 -9.422247 #> 1287 2521406365 41.26347 -7.442267 #> 1288 2521405897 41.40247 -7.465518 #> 1289 2521405667 41.35846 -7.403038 #> 1290 2464748146 41.72318 -8.129463 #> 1291 2442913242 37.28064 -8.555517 #> 1292 2442871645 41.75148 -8.201509 #> 1293 1945419094 41.24880 -7.811231 #> 1294 1890067578 39.30139 -9.217903 #> 1295 1580129201 38.08485 -6.422796 #> 1296 1562900214 39.30134 -9.218001 #> 1297 1338880563 37.34785 -8.816786 # download presences library(rgbif) occ_download_get(key = \"0068808-230530130749713\", path = tempdir()) # read file library(readr) distrib <- read_delim(file.path(tempdir(), \"0068808-230530130749713.zip\")) # keep the necessary columns and rename them lacerta <- distrib %>% select(gbifID, decimalLatitude, decimalLongitude) %>%   rename(ID = gbifID, latitude = decimalLatitude, longitude = decimalLongitude)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"preparing-your-data","dir":"Articles","previous_headings":"","what":"Preparing your data","title":"tidysdm overview","text":"First, let us visualise presences plotting map. tidysdm works sf objects represent locations, cast coordinates sf object, set projections standard ‘lonlat’ (crs = 4326). usually advisable plot locations directly raster used extract climatic variables, see locations fall within discrete space raster. vignette, use WorldClim source climatic information. access WorldClim data via library pastclim; even though library, name suggests, mostly designed handle palaeoclimatic reconstructions, also provides convenient functions access present day reconstructions future projections. pastclim handy function get land mask available datasets, can use background locations. cut raster Iberian peninsula, lizard lives. simply illustration, bother project raster, equal area projection desirable… plotting, take advantage tidyterra, makes handling terra rasters ggplot breeze.","code":"library(sf) #> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE lacerta <- st_as_sf(lacerta, coords = c(\"longitude\", \"latitude\")) st_crs(lacerta) <- 4326 library(pastclim) download_dataset(dataset = \"WorldClim_2.1_10m\") land_mask <-   get_land_mask(time_ce = 1985, dataset = \"WorldClim_2.1_10m\")  # Iberia peninsula extension iberia_poly <-   terra::vect(     \"POLYGON((-9.8 43.3,-7.8 44.1,-2.0 43.7,3.6 42.5,3.8 41.5,1.3 40.8,0.3 39.5,      0.9 38.6,-0.4 37.5,-1.6 36.7,-2.3 36.3,-4.1 36.4,-4.5 36.4,-5.0 36.1,     -5.6 36.0,-6.3 36.0,-7.1 36.9,-9.5 36.6,-9.4 38.0,-10.6 38.9,-9.5 40.8,     -9.8 43.3))\"   )  crs(iberia_poly) <- \"lonlat\" # crop the extent land_mask <- crop(land_mask, iberia_poly) # and mask to the polygon land_mask <- mask(land_mask, iberia_poly) #> Loading required package: terra #> terra 1.7.78 #>  #> Attaching package: 'terra' #> The following object is masked from 'package:tidyr': #>  #>     extract #> The following object is masked from 'package:scales': #>  #>     rescale library(tidyterra) #>  #> Attaching package: 'tidyterra' #> The following object is masked from 'package:stats': #>  #>     filter library(ggplot2) ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_1985)) +   geom_sf(data = lacerta) + scale_fill_gradient(na.value = \"transparent\")"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"thinning-step","dir":"Articles","previous_headings":"","what":"Thinning step","title":"tidysdm overview","text":"Now, thin observations one per cell raster (better equal area projection…):  Now, thin remove points closer 20km. However, note standard map units ‘lonlat’ projection meters. tidysdm provides convening conversion function, km2m(), avoid write lots zeroes): Let’s see left points:  now need select points represent potential available area species. two approaches, can either sample background sample_background(), can generate pseudo-absences sample_pseudoabs(). example, sample background; specifically, attempt account potential sampling biases using target group approach, presences species within taxonomic group used condition sampling background, providing information differential sampling different areas within region interest. start downloading records 8 genera Lacertidae, covering geographic region Iberian peninsula GBIF https://doi.org/10.15468/dl.53js5z: need convert observations raster whose values number records (later used determine likely cell used background point):  can see sampling far random, certain locations large number records. can now sample background, using ‘bias’ method represent heterogeneity sampling effort: Let’s see presences background:  Generally, can use pastclim check variables available WorldClim dataset: first download dataset right resolution (10 arc-minutes): create terra SpatRaster object. dataset covers period 1970-2000, pastclim dates 1985 (midpoint). can directly crop Iberian peninsula: Next, extract climate presences background points: Based paper (https://doi.org/10.1007/s10531-010-9865-2), interested variables: “bio06”, “bio05”, “bio13”, “bio14”, “bio15”. can visualise differences presences background using violin plots: can see variables interest seem different distribution presences background. can formally quantify mismatch two computing overlap: , can see variables interest seem good candidates clear signal. Let us focus variables: Environmental variables often highly correlated, collinearity issue several types models. can inspect correlation among variables :  can see variables rather high correlation (e.g. bio05 vs bio14). can subset variables certain threshold correlation (e.g. 0.7) : , removing bio14 leaves us set uncorrelated variables. Note filter_collinear methods based variable inflation also worth exploring. example, remove bio14 work remaining variables.","code":"set.seed(1234567) lacerta <- thin_by_cell(lacerta, raster = land_mask) nrow(lacerta) #> [1] 226 ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_1985)) +   geom_sf(data = lacerta) + scale_fill_gradient(na.value = \"transparent\") set.seed(1234567) lacerta_thin <- thin_by_dist(lacerta, dist_min = km2m(20)) nrow(lacerta_thin) #> [1] 111 ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_1985)) +   geom_sf(data = lacerta_thin) + scale_fill_gradient(na.value = \"transparent\") # download presences library(rgbif) # download file occ_download_get(key = \"0121761-240321170329656\", path = tempdir()) # read file library(readr) backg_distrib <- readr::read_delim(file.path(tempdir(), \"0121761-240321170329656.zip\"))  # keep the necessary columns lacertidae_background <- backg_distrib %>% select(gbifID, decimalLatitude, decimalLongitude) %>%   rename(ID = gbifID, latitude = decimalLatitude, longitude = decimalLongitude)  lacertidae_background <- st_as_sf(lacertidae_background, coords = c(\"longitude\", \"latitude\")) st_crs(lacertidae_background) <- 4326 lacertidae_background_raster <- rasterize(lacertidae_background, land_mask, fun = \"count\")  plot(lacertidae_background_raster) set.seed(1234567) lacerta_thin <- sample_background(data = lacerta_thin, raster = lacertidae_background_raster,                   n = 3 * nrow(lacerta_thin),                   method = \"bias\",                   class_label = \"background\",                   return_pres = TRUE) ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_1985)) +   geom_sf(data = lacerta_thin, aes(col = class)) + scale_fill_gradient(na.value = \"transparent\") climate_vars <- get_vars_for_dataset(\"WorldClim_2.1_10m\") download_dataset(\"WorldClim_2.1_10m\") climate_present <- pastclim::region_slice(   time_ce = 1985,   bio_variables = climate_vars,   data = \"WorldClim_2.1_10m\",   crop = iberia_poly ) lacerta_thin <- lacerta_thin %>%   bind_cols(terra::extract(climate_present, lacerta_thin, ID = FALSE)) lacerta_thin %>% plot_pres_vs_bg(class) lacerta_thin %>% dist_pres_vs_bg(class) #>      bio09      bio12      bio16      bio13      bio05      bio10      bio19  #> 0.44341125 0.43673315 0.42163656 0.41676947 0.41107299 0.40554870 0.40009102  #>      bio02      bio07      bio04      bio08      bio17      bio18      bio14  #> 0.36398134 0.34354633 0.31492272 0.30408833 0.30393285 0.27604384 0.26619609  #>      bio01      bio15      bio03      bio11   altitude      bio06  #> 0.26516698 0.24779818 0.15863624 0.10530412 0.09195507 0.04780224 suggested_vars <- c(\"bio06\", \"bio05\", \"bio13\", \"bio14\", \"bio15\") pairs(climate_present[[suggested_vars]]) climate_present <- climate_present[[suggested_vars]]  vars_uncor <- filter_collinear(climate_present, cutoff = 0.7, method = \"cor_caret\") vars_uncor #> [1] \"bio15\" \"bio05\" \"bio13\" \"bio06\" #> attr(,\"to_remove\") #> [1] \"bio14\" lacerta_thin <- lacerta_thin %>% select(all_of(c(vars_uncor, \"class\"))) climate_present <- climate_present[[vars_uncor]] names(climate_present) # added to highlight which variables are retained in the end #> [1] \"bio15\" \"bio05\" \"bio13\" \"bio06\""},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"fit-the-model-by-cross-validation","dir":"Articles","previous_headings":"","what":"Fit the model by cross-validation","title":"tidysdm overview","text":"Next, need set recipe define handle dataset. don’t want anything data terms transformations, just need define formula (class outcome, variables predictors; note , sf objects, geometry automatically replaced X Y columns assigned role coords, thus used predictors): classification models tidymodels, assumption level interest response (case, presences) reference level. can confirm data correctly formatted : now build workflow_set different models, defining hyperparameters want tune. use glm, random forest, boosted_trees maxent models (details use workflow_sets, see tutorial). latter three models tunable hyperparameters. commonly used models, tidysdm automatically chooses important parameters, possible fully customise model specifications (e.g. see help sdm_spec_rf). now want set spatial block cross-validation scheme tune assess models. split data creating 3 folds. use spatial_block_cv function package spatialsample. spatialsample offers number sampling approaches spatial data; also possible convert objects created blockCV (offers features spatial sampling, stratified sampling) rsample object suitable tisysdm function blockcv2rsample.  can now use block CV folds tune assess models (keep computations fast, explore 3 combination hyperparameters per model; far little real life!): Note workflow_set correctly detects tuning parameters glm. can look performance models :  Now let’s create ensemble, selecting best set parameters model (really relevant random forest, hype-parameters tune glm gam). use Boyce continuous index metric choose best random forest boosted tree. adding members ensemble, automatically fitted full training dataset, ready make predictions. visualise  tabular form model metrics can obtained :","code":"lacerta_rec <- recipe(lacerta_thin, formula = class ~ .) lacerta_rec #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:   1 #> predictor: 4 #> coords:    2 lacerta_thin %>% check_sdm_presence(class) #> [1] TRUE lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(default = lacerta_rec),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # rf specs with tuning       rf = sdm_spec_rf(),       # boosted tree model (gbm) specs with tuning       gbm = sdm_spec_boost_tree(),       # maxent specs with tuning       maxent = sdm_spec_maxent()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid()) library(tidysdm) set.seed(100) #lacerta_cv <- spatial_block_cv(lacerta_thin, v = 5)  lacerta_cv <- spatial_block_cv(data = lacerta_thin, v = 3, n = 5) autoplot(lacerta_cv) set.seed(1234567) lacerta_models <-   lacerta_models %>%   workflow_map(\"tune_grid\",     resamples = lacerta_cv, grid = 3,     metrics = sdm_metric_set(), verbose = TRUE   ) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 4 resampling: default_glm #> ✔ 1 of 4 resampling: default_glm (212ms) #> i 2 of 4 tuning:     default_rf #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 2 of 4 tuning:     default_rf (825ms) #> i 3 of 4 tuning:     default_gbm #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 3 of 4 tuning:     default_gbm (4s) #> i 4 of 4 tuning:     default_maxent #> ✔ 4 of 4 tuning:     default_maxent (1.2s) autoplot(lacerta_models) lacerta_ensemble <- simple_ensemble() %>%   add_member(lacerta_models, metric = \"boyce_cont\") lacerta_ensemble #> A simple_ensemble of models #>  #> Members: #> • default_glm #> • default_rf #> • default_gbm #> • default_maxent #>  #> Available metrics: #> • boyce_cont #> • roc_auc #> • tss_max #>  #> Metric used to tune workflows: #> • boyce_cont autoplot(lacerta_ensemble) lacerta_ensemble %>% collect_metrics() #> # A tibble: 12 × 5 #>    wflow_id       .metric     mean std_err     n #>    <chr>          <chr>      <dbl>   <dbl> <int> #>  1 default_glm    boyce_cont 0.547 0.127       3 #>  2 default_glm    roc_auc    0.773 0.0349      3 #>  3 default_glm    tss_max    0.507 0.0430      3 #>  4 default_rf     boyce_cont 0.722 0.0989      3 #>  5 default_rf     roc_auc    0.771 0.00988     3 #>  6 default_rf     tss_max    0.472 0.0467      3 #>  7 default_gbm    boyce_cont 0.661 0.129       3 #>  8 default_gbm    roc_auc    0.788 0.00224     3 #>  9 default_gbm    tss_max    0.514 0.0135      3 #> 10 default_maxent boyce_cont 0.751 0.101       3 #> 11 default_maxent roc_auc    0.798 0.0198      3 #> 12 default_maxent tss_max    0.554 0.0186      3"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"projecting-to-the-present","dir":"Articles","previous_headings":"","what":"Projecting to the present","title":"tidysdm overview","text":"can now make predictions ensemble (using default option taking mean predictions model).  can subset ensemble use best models, based Boyce continuous index, setting minimum threshold 0.7 metric. also take median available model predictions (instead mean, default). plot change much (models quite consistent).  Sometimes, desirable binary predictions (presence vs absence), rather probability occurrence. , first need calibrate threshold used convert probabilities classes (case, optimise TSS): now can predict whole continent:","code":"prediction_present <- predict_raster(lacerta_ensemble, climate_present) ggplot() +   geom_spatraster(data = prediction_present, aes(fill = mean)) +   scale_fill_terrain_c() +   # plot presences used in the model   geom_sf(data = lacerta_thin %>% filter(class == \"presence\")) prediction_present_boyce <- predict_raster(lacerta_ensemble, climate_present,   metric_thresh = c(\"boyce_cont\", 0.7),   fun = \"median\" ) ggplot() +   geom_spatraster(data = prediction_present_boyce, aes(fill = median)) +   scale_fill_terrain_c() +   geom_sf(data = lacerta_thin %>% filter(class == \"presence\")) lacerta_ensemble <- calib_class_thresh(lacerta_ensemble,   class_thresh = \"tss_max\",    metric_thresh = c(\"boyce_cont\", 0.7) ) prediction_present_binary <- predict_raster(lacerta_ensemble,   climate_present,   type = \"class\",   class_thresh = c(\"tss_max\"),    metric_thresh = c(\"boyce_cont\", 0.7) ) ggplot() +   geom_spatraster(data = prediction_present_binary, aes(fill = binary_mean)) +   geom_sf(data = lacerta_thin %>% filter(class == \"presence\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"projecting-to-the-future","dir":"Articles","previous_headings":"","what":"Projecting to the future","title":"tidysdm overview","text":"WorldClim wide selection projections future based different models Shared Socio-economic Pathways (SSP). Type help(\"WorldClim_2.1\") full list. use predictions based “HadGEM3-GC31-LL” model SSP 245 (intermediate green house gas emissions) resolution present day data (10 arc-minutes). first download data: Let’s see times available: predict 2090, prediction future available. Let’s now check available variables: Note future predictions include altitude (change time), needed , copy present. However, set uncorrelated variables used earlier, don’t need worry . predict using ensemble:","code":"download_dataset(\"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\") get_time_ce_steps(\"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\") #> [1] 2030 2050 2070 2090 get_vars_for_dataset(\"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\") #>  [1] \"bio01\" \"bio02\" \"bio03\" \"bio04\" \"bio05\" \"bio06\" \"bio07\" \"bio08\" \"bio09\" #> [10] \"bio10\" \"bio11\" \"bio12\" \"bio13\" \"bio14\" \"bio15\" \"bio16\" \"bio17\" \"bio18\" #> [19] \"bio19\" climate_future <- pastclim::region_slice(   time_ce = 2090,   bio_variables = vars_uncor,   data = \"WorldClim_2.1_HadGEM3-GC31-LL_ssp245_10m\",   crop = iberia_poly ) prediction_future <- predict_raster(lacerta_ensemble, climate_future)  ggplot() +   geom_spatraster(data = prediction_future, aes(fill = mean)) +   scale_fill_terrain_c()"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"dealing-with-extrapolation","dir":"Articles","previous_headings":"","what":"Dealing with extrapolation","title":"tidysdm overview","text":"total area projection model may include environmental conditions lie outside range conditions covered calibration dataset. phenomenon can lead misinterpretation SDM outcomes due spatial extrapolation. tidysdm offers couple approaches deal problem. simplest one can clamp environmental variables stay within limits observed calibration set:  predictions seem changed little. alternative allow values exceed ranges calibration set, compute Multivariate environmental similarity surfaces (MESS) (Elith et al. 2010) highlight areas extrapolation occurs thus visualise prediction’s uncertainty. estimate MESS future time slice used :  Extrapolation occurs areas MESS values negative, magnitude negative values indicating extreme interpolation. plot, can see area extrapolation model already predicted suitability zero. explains clamping little predictions. can now overlay MESS values current prediction visualize areas characterized spatial extrapolation.  Note clamping MESS useful making predictions future, also past present (latter case, allows us make sure background/pseudoabsences cover full range predictor variables area interest). tidymodels universe also includes functions estimate area applicability package waywiser, can used tidysdm.","code":"climate_future_clamped <- clamp_predictors(climate_future,                                             training = lacerta_thin,                                            .col= class) prediction_future_clamped <- predict_raster(lacerta_ensemble,                                              raster = climate_future_clamped)  ggplot() +   geom_spatraster(data = prediction_future_clamped, aes(fill = mean)) +   scale_fill_terrain_c() lacerta_mess_future <- extrapol_mess(x = climate_future,                                        training = lacerta_thin,                                        .col = \"class\")  ggplot() + geom_spatraster(data = lacerta_mess_future) +    scale_fill_viridis_b(na.value = \"transparent\") # subset mess  lacerta_mess_future_subset <- lacerta_mess_future lacerta_mess_future_subset[lacerta_mess_future_subset >= 0] <- NA lacerta_mess_future_subset[lacerta_mess_future_subset < 0] <- 1  # convert into polygon lacerta_mess_future_subset <- as.polygons(lacerta_mess_future_subset)  # plot as a mask  ggplot() + geom_spatraster(data = prediction_future) +    scale_fill_viridis_b(na.value = \"transparent\") + geom_sf(data = lacerta_mess_future_subset, fill= \"lightgray\", alpha = 0.5, linewidth = 0.5)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"visualising-the-contribution-of-individual-variables","dir":"Articles","previous_headings":"","what":"Visualising the contribution of individual variables","title":"tidysdm overview","text":"sometimes interest understand relative contribution individual variables prediction. complex task, especially interactions among variables. simpler linear models, possible obtain marginal response curves (show effect variable whilst keeping variables mean) using step_profile() recipes package. use step_profile() define new recipe can bake generate appropriate dataset make marginal prediction. can plot predictions values variable interest. example, investigate contribution bio05, :  also possible use DALEX,explore tidysdm models; see details tidymodels additions article.","code":"bio05_prof <- lacerta_rec %>%   step_profile(-bio05, profile = vars(bio05)) %>%   prep(training = lacerta_thin)  bio05_data <- bake(bio05_prof, new_data = NULL)  bio05_data <- bio05_data %>%   mutate(     pred = predict(lacerta_ensemble, bio05_data)$mean   )  ggplot(bio05_data, aes(x = bio05, y = pred)) +   geom_point(alpha = .5, cex = 1)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html","id":"repeated-ensembles","dir":"Articles","previous_headings":"","what":"Repeated ensembles","title":"tidysdm overview","text":"steps thinning sampling pseudo-absences can bit impact performance SDMs. steps stochastic, good practice explore effect repeating , creating ensembles models repeats. tidysdm, possible create repeat_ensembles. start creating list simple_ensembles, looping SDM pipeline. just use two fast models speed process. Now can create repeat_ensemble list: can summarise goodness fit models repeat collect_metrics(), autoplot() function repeated_ensemble objects. can predict usual way (take mean median models):","code":"# empty object to store the simple ensembles that we will create ensemble_list <- list() set.seed(123) # make sure you set the seed OUTSIDE the loop for (i_repeat in 1:3) {   # thin the data   lacerta_thin_rep <- thin_by_cell(lacerta, raster = climate_present)   lacerta_thin_rep <- thin_by_dist(lacerta_thin_rep, dist_min = 20000)   # sample pseudo-absences   lacerta_thin_rep <- sample_pseudoabs(lacerta_thin_rep,     n = 3 * nrow(lacerta_thin_rep),     raster = climate_present,     method = c(\"dist_min\", 50000)   )   # get climate   lacerta_thin_rep <- lacerta_thin_rep %>%     bind_cols(terra::extract(climate_present, lacerta_thin_rep, ID = FALSE))   # create folds   lacerta_thin_rep_cv <- spatial_block_cv(lacerta_thin_rep, v = 5)   # create a recipe   lacerta_thin_rep_rec <- recipe(lacerta_thin_rep, formula = class ~ .)   # create a workflow_set   lacerta_thin_rep_models <-     # create the workflow_set     workflow_set(       preproc = list(default = lacerta_thin_rep_rec),       models = list(         # the standard glm specs         glm = sdm_spec_glm(),         # maxent specs with tuning         maxent = sdm_spec_maxent()       ),       # make all combinations of preproc and models,       cross = TRUE     ) %>%     # tweak controls to store information needed later to create the ensemble     option_add(control = control_ensemble_grid())    # train the model   lacerta_thin_rep_models <-     lacerta_thin_rep_models %>%     workflow_map(\"tune_grid\",       resamples = lacerta_thin_rep_cv, grid = 10,       metrics = sdm_metric_set(), verbose = TRUE     )   # make an simple ensemble and add it to the list   ensemble_list[[i_repeat]] <- simple_ensemble() %>%     add_member(lacerta_thin_rep_models, metric = \"boyce_cont\") } #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (222ms) #> i 2 of 2 tuning:     default_maxent #> ✔ 2 of 2 tuning:     default_maxent (6.8s) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (226ms) #> i 2 of 2 tuning:     default_maxent #> ✔ 2 of 2 tuning:     default_maxent (7.1s) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (229ms) #> i 2 of 2 tuning:     default_maxent #> ✔ 2 of 2 tuning:     default_maxent (7.3s) lacerta_rep_ens <- repeat_ensemble() %>% add_repeat(ensemble_list) lacerta_rep_ens #> A repeat_ensemble of models #>  #> Number of repeats: #> • 3 #>  #> Members: #> • default_glm #> • default_maxent #>  #> Available metrics: #> • boyce_cont #> • roc_auc #> • tss_max #>  #> Metric used to tune workflows: #> • boyce_cont lacerta_rep_ens <- predict_raster(lacerta_rep_ens, climate_present,   fun = c(\"mean\", \"median\") ) ggplot() +   geom_spatraster(data = lacerta_rep_ens, aes(fill = median)) +   scale_fill_terrain_c()"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a1_palaeodata_application.html","id":"sdms-with-tidymodels-for-palaeo-data","dir":"Articles","previous_headings":"","what":"SDMs with tidymodels for palaeo data","title":"Application with palaeodata","text":"article, show Species Distribution Model can fitted tidysdm time-scattered (.e.palaeontological, archaeozoological, archaeological) data, samples covering different time periods. recommend users first read “tidysdm overview” article, introduces number functions concepts used present article. first load tidysdm:","code":"library(tidysdm) #> Loading required package: tidymodels #> ── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ── #> ✔ broom        1.0.6      ✔ recipes      1.0.10 #> ✔ dials        1.2.1      ✔ rsample      1.2.1  #> ✔ dplyr        1.1.4      ✔ tibble       3.2.1  #> ✔ ggplot2      3.5.1      ✔ tidyr        1.3.1  #> ✔ infer        1.0.7      ✔ tune         1.2.1  #> ✔ modeldata    1.4.0      ✔ workflows    1.1.4  #> ✔ parsnip      1.2.1      ✔ workflowsets 1.1.0  #> ✔ purrr        1.0.2      ✔ yardstick    1.3.1 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Search for functions across packages at https://www.tidymodels.org/find/ #> Loading required package: spatialsample"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a1_palaeodata_application.html","id":"preparing-your-data","dir":"Articles","previous_headings":"","what":"Preparing your data","title":"Application with palaeodata","text":"start loading set radiocarbon dates (calibrated) horses, covering 22k years ago 8k years ago. convert dataset sf data.frame can easily plot (tidyterra shines): background presences, use land mask present, taken pastclim, cut cover Europe: use tidyterra plot:  now thin presences, locations 100km 2000 years apart. see left:  now need time series palaeoclimate reconstructions. vignette, use example dataset pastclim. dataset reconstructions every 5k years past 20k years 1 degree resolution, 3 bioclimatic variables. suffice illustrative purposes, recommend download higher quality datasets pastclim real analysis. land mask, cut reconstructions cover Europe : Now thin observations keep one per cell raster (better equal area projection…), remove locations outside desired area (): Let’s see left points:  Now sample pseudo-absences (constraint least 70km away presences), selecting three times number presences Let’s see presences absences:  Now let’s get climate location. pastclim requires data frame two columns coordinates column time years present (negative values represent time past). manipulate sf object accordingly:","code":"data(horses) horses #> # A tibble: 788 × 3 #>    latitude longitude time_bp #>       <dbl>     <dbl>   <int> #>  1     43.2     -2.04  -14000 #>  2     43.2     -2.04  -14000 #>  3     43.2     -2.04  -14000 #>  4     43.2     -2.04  -14000 #>  5     43.2     -2.04  -16000 #>  6     43.3     -1.89  -16000 #>  7     43.2     -2.2   -14000 #>  8     43.2     -2.2   -19000 #>  9     43.2     -2.2   -20000 #> 10     43.2     -2.2   -21000 #> # ℹ 778 more rows library(sf) #> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE horses <- st_as_sf(horses, coords = c(\"longitude\", \"latitude\")) st_crs(horses) <- 4326 #> Loading required package: terra #> terra 1.7.78 #>  #> Attaching package: 'terra' #> The following object is masked from 'package:tidyr': #>  #>     extract #> The following object is masked from 'package:scales': #>  #>     rescale library(pastclim) land_mask <- pastclim::get_land_mask(time_bp = 0, dataset = \"Example\") europe_poly <- vect(region_outline$Europe) crs(europe_poly) <- \"lonlat\" land_mask <- crop(land_mask, europe_poly) land_mask <- mask(land_mask, europe_poly) library(tidyterra) #>  #> Attaching package: 'tidyterra' #> The following object is masked from 'package:stats': #>  #>     filter ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_0)) +   geom_sf(data = horses, aes(col = time_bp)) set.seed(123) horses <- thin_by_dist_time(horses,   dist_min = km2m(100),   interval_min = y2d(2000),   time_col = \"time_bp\",   lubridate_fun = pastclim::ybp2date ) nrow(horses) #> [1] 185 ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_0)) +   geom_sf(data = horses, aes(col = time_bp)) library(pastclim) climate_vars <- c(\"bio01\", \"bio10\", \"bio12\") climate_full <- pastclim::region_series(   bio_variables = climate_vars,   data = \"Example\",   crop = region_outline$Europe ) set.seed(123) horses <- thin_by_cell_time(horses,   raster = climate_full,   time_col = \"time_bp\",   lubridate_fun = pastclim::ybp2date ) nrow(horses) #> [1] 138 ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_0)) +   geom_sf(data = horses, aes(col = time_bp)) set.seed(123) horses <- sample_pseudoabs_time(horses,   n_per_presence = 3,   raster = climate_full,   time_col = \"time_bp\",   lubridate_fun = pastclim::ybp2date,   method = c(\"dist_min\", km2m(70)) ) ggplot() +   geom_spatraster(data = land_mask, aes(fill = land_mask_0)) +   geom_sf(data = horses, aes(col = class)) horses_df <- horses %>%   dplyr::bind_cols(sf::st_coordinates(horses)) %>%   mutate(time_bp = date2ybp(time_step)) %>%   as.data.frame() %>%   select(-geometry) # get climate horses_df <- location_slice_from_region_series(horses_df,   region_series = climate_full )  # add the climate reconstructions to the sf object, and remove the time_step # as we don't need it for modelling horses <- horses %>%   bind_cols(horses_df[, climate_vars]) %>%   select(-time_step)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a1_palaeodata_application.html","id":"fit-the-model-by-crossvalidation","dir":"Articles","previous_headings":"","what":"Fit the model by crossvalidation","title":"Application with palaeodata","text":"Next, need set recipe define handle dataset. don’t want transform data, just need define formula (class outcome, variables predictors; note , sf objects, geometry automatically ignored predictor): can quickly check variables want : now build workflow_set different models, defining hyperparameters want tune. use glm, gam, random forest boosted trees models, random forest boosted trees tunable hyperparameters. commonly used models, tidysdm automatically chooses important parameters, possible fully customise model specifications. Note gams unusual, need specify formula define variables fit smooths. default, gam_formula() fits smooth every continuous predictor, custom formula can provided instead. now want set spatial block cross-validation scheme tune assess models:  can now use block CV folds tune assess models: Note workflow_set correctly detects tuning parameters glm gam. can look performance models :  Now let’s create ensemble, selecting best set parameters model (really relevant random forest, hype-parameters tune glm gam). use Boyce continuous index metric choose best random forest boosted tree. adding members ensemble, automatically fitted full training dataset, ready make predictions. visualise ","code":"horses_rec <- recipe(horses, formula = class ~ .) horses_rec #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:   1 #> predictor: 3 #> coords:    2 horses_rec$var_info #> # A tibble: 6 × 4 #>   variable type      role      source   #>   <chr>    <list>    <chr>     <chr>    #> 1 bio01    <chr [2]> predictor original #> 2 bio10    <chr [2]> predictor original #> 3 bio12    <chr [2]> predictor original #> 4 X        <chr [2]> coords    original #> 5 Y        <chr [2]> coords    original #> 6 class    <chr [3]> outcome   original horses_models <-   # create the workflow_set   workflow_set(     preproc = list(default = horses_rec),     models = list(       # the standard glm specs  (no params to tune)       glm = sdm_spec_glm(),       # the standard sdm specs (no params to tune)       gam = sdm_spec_gam(),       # rf specs with tuning       rf = sdm_spec_rf(),       # boosted tree model (gbm) specs with tuning       gbm = sdm_spec_boost_tree()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # set formula for gams   update_workflow_model(\"default_gam\",     spec = sdm_spec_gam(),     formula = gam_formula(horses_rec)   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid()) library(tidysdm) set.seed(1005) horses_cv <- spatial_block_cv(horses, v = 5) autoplot(horses_cv) set.seed(123) horses_models <-   horses_models %>%   workflow_map(\"tune_grid\",     resamples = horses_cv, grid = 5,     metrics = sdm_metric_set(), verbose = TRUE   ) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 4 resampling: default_glm #> ✔ 1 of 4 resampling: default_glm (249ms) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 2 of 4 resampling: default_gam #> ✔ 2 of 4 resampling: default_gam (651ms) #> i 3 of 4 tuning:     default_rf #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 3 of 4 tuning:     default_rf (2.4s) #> i 4 of 4 tuning:     default_gbm #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 4 of 4 tuning:     default_gbm (15.1s) autoplot(horses_models) horses_ensemble <- simple_ensemble() %>%   add_member(horses_models, metric = \"boyce_cont\") autoplot(horses_ensemble)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a1_palaeodata_application.html","id":"projecting-to-other-times","dir":"Articles","previous_headings":"","what":"Projecting to other times","title":"Application with palaeodata","text":"can now make predictions ensemble (using default option taking mean predictions model) Last Glacial Maximum (LGM, 21,000 years ago). predict using ensemble:","code":"climate_lgm <- pastclim::region_slice(   time_bp = -20000,   bio_variables = climate_vars,   data = \"Example\",   crop = region_outline$Europe ) prediction_lgm <- predict_raster(horses_ensemble, climate_lgm) ggplot() +   geom_spatraster(data = prediction_lgm, aes(fill = mean)) +   scale_fill_terrain_c()"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a2_tidymodels_additions.html","id":"additional-features-of-tidymodels","dir":"Articles","previous_headings":"","what":"Additional features of tidymodels","title":"Examples of additional tidymodels features","text":"vignette, illustrate number features tidymodels can used enhance conventional SDM pipeline. recommend users first become familiar tidymodels; number excellent tutorials (introductory advanced) dedicated website reuse example Iberian lizard used tidysdm overview article.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a2_tidymodels_additions.html","id":"exploring-models-with-dalex","dir":"Articles","previous_headings":"","what":"Exploring models with DALEX","title":"Examples of additional tidymodels features","text":"issue machine learning algorithms easy understand role different variables giving final prediction. number packages created explore explain behaviour ML algorithms, used tidysdm. tidysdm overview article, illustrated use recipes create profiles. demonstrate use DALEX, excellent package methods deal tidymodels. tidysdm contains additional functions allow use use DALEX functions directly tidysdm ensembles. use simple ensemble built first step DALEX create explainer object, can queried different functions package, turn explainer explanation (following DALEX lingo). first step, use custom function explain_tidysdm generate explainer: Now explainer, can explore variable importance ensemble:  generate partial dependency plots given variable (e.g. bio05):  many functions DALEX can applied explainer explore behaviour model; see several tutorial https://modeloriented.github.io/DALEX/ also possible explore individual models make ensemble: resulting list can used build lists explanations, can plotted.","code":"library(tidysdm) #> Loading required package: tidymodels #> ── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ── #> ✔ broom        1.0.6      ✔ recipes      1.0.10 #> ✔ dials        1.2.1      ✔ rsample      1.2.1  #> ✔ dplyr        1.1.4      ✔ tibble       3.2.1  #> ✔ ggplot2      3.5.1      ✔ tidyr        1.3.1  #> ✔ infer        1.0.7      ✔ tune         1.2.1  #> ✔ modeldata    1.4.0      ✔ workflows    1.1.4  #> ✔ parsnip      1.2.1      ✔ workflowsets 1.1.0  #> ✔ purrr        1.0.2      ✔ yardstick    1.3.1 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Search for functions across packages at https://www.tidymodels.org/find/ #> Loading required package: spatialsample lacerta_ensemble #> A simple_ensemble of models #>  #> Members: #> • default_glm #> • default_rf #> • default_gbm #> • default_maxent #>  #> Available metrics: #> • boyce_cont #> • roc_auc #> • tss_max #>  #> Metric used to tune workflows: #> • boyce_cont explainer_lacerta_ens <- explain_tidysdm(lacerta_ensemble) #> Preparation of a new explainer is initiated #>   -> model label       :  data.frame  (  default  ) #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  predict_function  #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidysdm , ver. 0.9.5 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.06237169 , mean =  0.2679146 , max =  0.8991577   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.571285 , mean =  -0.01791465 , max =  0.7453912   #>   A new explainer has been created! library(DALEX) #> Welcome to DALEX (version: 2.4.3). #> Find examples and detailed introduction at: http://ema.drwhy.ai/ #> Additional features will be available after installation of: ggpubr. #> Use 'install_dependencies()' to get all suggested dependencies #>  #> Attaching package: 'DALEX' #> The following object is masked from 'package:dplyr': #>  #>     explain vip_ensemble <- model_parts(explainer = explainer_lacerta_ens) plot(vip_ensemble) pdp_bio05 <- model_profile(explainer_lacerta_ens, N = 500, variables = \"bio05\") plot(pdp_bio05) explainer_list <- explain_tidysdm(tidysdm::lacerta_ensemble, by_workflow = TRUE) #> Preparation of a new explainer is initiated #>   -> model label       :  default_glm  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.005923341 , mean =  0.75 , max =  0.9993179   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.9513805 , mean =  3.739236e-15 , max =  0.9628654   #>   A new explainer has been created!   #> Preparation of a new explainer is initiated #>   -> model label       :  default_rf  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0 , mean =  0.7514254 , max =  1   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.5907905 , mean =  -0.001425431 , max =  0.523096   #>   A new explainer has been created!   #> Preparation of a new explainer is initiated #>   -> model label       :  default_gbm  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.255644 , mean =  0.6272817 , max =  0.7515493   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.7198042 , mean =  0.1227183 , max =  0.582286   #>   A new explainer has been created!   #> Preparation of a new explainer is initiated #>   -> model label       :  default_maxent  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.09570046 , mean =  0.7996343 , max =  0.9997615   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.8792301 , mean =  -0.04963429 , max =  0.6252604   #>   A new explainer has been created! profile_list <- lapply(explainer_list, model_profile,   N = 500,   variables = \"bio05\" ) plot(profile_list)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a2_tidymodels_additions.html","id":"the-initial-split","dir":"Articles","previous_headings":"","what":"The initial split","title":"Examples of additional tidymodels features","text":"standard approach tidymodels make initial split data test training set. use retain 20% data (1/5) testing set, use rest training. start loading set presences absences associated climate, analogous one generated tidysdm overview article: use spatial_initial_split split, using spatial_block_cv scheme partition data:  check balance presences vs pseudoabsences: can now extract training set lacerta_initial split, sample folds set cross validation (note set cellsize offset based full dataset, lacerta_thin; allows us use grid used initial_split).  check balance dataset:","code":"library(tidysdm) library(sf) #> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE lacerta_thin <- readRDS(system.file(\"extdata/lacerta_climate_sf.RDS\",   package = \"tidysdm\" )) set.seed(1005) lacerta_initial <- spatial_initial_split(lacerta_thin,   prop = 1 / 5, spatial_block_cv ) autoplot(lacerta_initial) check_splits_balance(lacerta_initial, class) #> # A tibble: 1 × 4 #>   presence_test pseudoabs_test presence_train pseudoabs_train #>           <int>          <int>          <int>           <int> #> 1            88            267             25              72 set.seed(1005) lacerta_training <- training(lacerta_initial) lacerta_cv <- spatial_block_cv(lacerta_training,   v = 5,   cellsize = grid_cellsize(lacerta_thin),   offset = grid_offset(lacerta_thin) ) autoplot(lacerta_cv) check_splits_balance(lacerta_cv, class) #> # A tibble: 5 × 4 #>   presence_assessment pseudoabs_assessment presence_analysis pseudoabs_analysis #>                 <int>                <int>             <int>              <int> #> 1                  74                  197                14                 70 #> 2                  59                  225                29                 42 #> 3                  73                  220                15                 47 #> 4                  76                  209                12                 58 #> 5                  70                  218                18                 49"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a2_tidymodels_additions.html","id":"different-recipes-for-certain-models","dir":"Articles","previous_headings":"","what":"Different recipes for certain models","title":"Examples of additional tidymodels features","text":"certain type models (e.g. glm, svm) struggle correlated variables; algorithms, random forests, can handle correlated variables. , create two recipes, one variables, one variables uncorrelated: now use two recipes workflowset (keep small computational time), selecting appropriate recipe model. include model (polynomial support vector machines, SVM) wrapper tidysdm creating model specification. However, can use standard model spec yardstick: can now use block CV folds tune assess models. Note multiple tuning approaches, besides standard grid method. use tune_bayes tune package (see help page see Gaussian Process model used choose parameter combinations). tuning method (opposed use standard grid) allow hyper-parameters unknown limits, mtry random forest undefined upper range depends number variables dataset. , tuning, need finalise mtry informing set dials actual data: now can tune models: can look performance models :","code":"lacerta_rec_all <- recipe(lacerta_thin, formula = class ~ .) lacerta_rec_uncor <- lacerta_rec_all %>%   step_rm(all_of(c(     \"bio01\", \"bio02\", \"bio03\", \"bio04\", \"bio07\", \"bio08\",     \"bio09\", \"bio10\", \"bio11\", \"bio12\", \"bio14\", \"bio16\",     \"bio17\", \"bio18\", \"bio19\", \"altitude\"   )))  lacerta_rec_uncor #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:    1 #> predictor: 20 #> coords:     2 #>  #> ── Operations #> • Variables removed: all_of(c(\"bio01\", \"bio02\", \"bio03\", \"bio04\", \"bio07\", #>   \"bio08\", \"bio09\", \"bio10\", \"bio11\", \"bio12\", \"bio14\", \"bio16\", \"bio17\", #>   \"bio18\", \"bio19\", \"altitude\")) lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(       uncor = lacerta_rec_uncor, # recipe for the glm       all = lacerta_rec_all, # recipe for the random forest       all = lacerta_rec_uncor # recipe for svm     ),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # rf specs with tuning       rf = sdm_spec_rf(),       # svm specs with tuning       svm = parsnip::svm_poly(         cost = tune(),         degree = tune()       ) %>%         parsnip::set_engine(\"kernlab\") %>%         parsnip::set_mode(\"classification\")     ),     # make all combinations of preproc and models,     cross = FALSE   ) %>%   # tweak controls to store information needed later to create the ensemble   # note that we use the bayes version as we will use a Bayes search (see later)   option_add(control = stacks::control_stack_bayes()) rf_param <- lacerta_models %>%   # extract the rf workflow   extract_workflow(\"all_rf\") %>%   # extract its parameters dials (used to tune)   extract_parameter_set_dials() %>%   # give it the predictors to finalize mtry   finalize(x = st_drop_geometry(lacerta_thin) %>% select(-class))  # now update the workflowset with the new parameter info lacerta_models <- lacerta_models %>%   option_add(param_info = rf_param, id = \"all_rf\") set.seed(1234567) lacerta_models <-   lacerta_models %>%   workflow_map(\"tune_bayes\",     resamples = lacerta_cv, initial = 8,     metrics = sdm_metric_set(), verbose = TRUE   ) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 3 resampling: uncor_glm #> ✔ 1 of 3 resampling: uncor_glm (306ms) #> i 2 of 3 tuning:     all_rf #> ! No improvement for 10 iterations; returning current results. #> ✔ 2 of 3 tuning:     all_rf (16.1s) #> i 3 of 3 tuning:     all_svm #> ✔ 3 of 3 tuning:     all_svm (21.1s) autoplot(lacerta_models)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a2_tidymodels_additions.html","id":"stack-ensembles","dir":"Articles","previous_headings":"","what":"Stack ensembles","title":"Examples of additional tidymodels features","text":"Instead building simple ensemble best version model type, can build stack ensemble, implemented package stacks. Stacking uses meta-learning algorithm learn best combine multiple models, including multiple versions algorithm different hyper-parameters.  can see three versions SVM one random forests selected; stacking coefficients give indication weight model carries within ensemble. can now use ensemble make predictions testing data: look goodness fit using commonly used sdm metrics. Note sdm_metric_set first invoked generate function (empty ()) used data. can now make predictions stacked ensemble. start extracting climate variables interest","code":"library(stacks) set.seed(1005) lacerta_stack <-   # initialize the stack   stacks() %>%   # add candidate members   add_candidates(lacerta_models) %>%   # determine how to combine their predictions   blend_predictions() %>%   # fit the candidates with non-zero weights (i.e.non-zero stacking coefficients)   fit_members()  autoplot(lacerta_stack, type = \"weights\") lacerta_testing <- testing(lacerta_initial)  lacerta_test_pred <-   lacerta_testing %>%   bind_cols(predict(lacerta_stack, ., type = \"prob\")) sdm_metric_set()(data = lacerta_test_pred, truth = class, .pred_presence) #> # A tibble: 3 × 3 #>   .metric    .estimator .estimate #>   <chr>      <chr>          <dbl> #> 1 boyce_cont binary         0.853 #> 2 roc_auc    binary         0.986 #> 3 tss_max    binary         0.92 download_dataset(\"WorldClim_2.1_10m\") climate_vars <- lacerta_rec_all$var_info %>%   filter(role == \"predictor\") %>%   pull(variable)  climate_present <- pastclim::region_slice(   time_ce = 1985,   bio_variables = climate_vars,   data = \"WorldClim_2.1_10m\",   crop = iberia_poly ) prediction_present <- predict_raster(lacerta_stack, climate_present,   type = \"prob\" ) library(tidyterra) #>  #> Attaching package: 'tidyterra' #> The following object is masked from 'package:stats': #>  #>     filter ggplot() +   geom_spatraster(data = prediction_present, aes(fill = .pred_presence)) +   scale_fill_terrain_c() +   # plot presences used in the model   geom_sf(data = lacerta_thin %>% filter(class == \"presence\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a3_troubleshooting.html","id":"nas-in-the-data","dir":"Articles","previous_headings":"","what":"NAs in the data","title":"Troubleshooting models that fail","text":"algorithms allow NAs. can generate problematic dataset loading Lacerta dataset, manually add NA: Let us set recipe fit workflow_set can see error self-explanatory. Also, note error impacts data splits (technically, rset objects): error repeated 15 times (5 splits 3 hyperparameter values). Prepping recipe (trains dataset) can help diagnosing problems: Note , training information, warned 1 row incomplete. use step_naomit deal programmatically, ascertain generating missing data (prefer latter, good SDM pipeline generate observations, presences pseudoabsences, missing data).","code":"library(tidysdm) #> Loading required package: tidymodels #> ── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ── #> ✔ broom        1.0.6      ✔ recipes      1.0.10 #> ✔ dials        1.2.1      ✔ rsample      1.2.1  #> ✔ dplyr        1.1.4      ✔ tibble       3.2.1  #> ✔ ggplot2      3.5.1      ✔ tidyr        1.3.1  #> ✔ infer        1.0.7      ✔ tune         1.2.1  #> ✔ modeldata    1.4.0      ✔ workflows    1.1.4  #> ✔ parsnip      1.2.1      ✔ workflowsets 1.1.0  #> ✔ purrr        1.0.2      ✔ yardstick    1.3.1 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Search for functions across packages at https://www.tidymodels.org/find/ #> Loading required package: spatialsample lacerta_thin <- readRDS(system.file(\"extdata/lacerta_climate_sf.RDS\",   package = \"tidysdm\" )) lacerta_thin$bio05[37] <- NA lacerta_rec <- recipe(lacerta_thin, formula = class ~ .) %>%   step_rm(all_of(c(     \"bio01\", \"bio02\", \"bio03\", \"bio04\", \"bio07\", \"bio08\",     \"bio09\", \"bio10\", \"bio11\", \"bio12\", \"bio14\", \"bio16\",     \"bio17\", \"bio18\", \"bio19\", \"altitude\"   )))  lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(default = lacerta_rec),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # rf specs with tuning       rf = sdm_spec_rf()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid()) set.seed(100) lacerta_cv <- spatial_block_cv(lacerta_thin, v = 5) lacerta_models <-   lacerta_models %>%   workflow_map(\"tune_grid\",     resamples = lacerta_cv, grid = 3,     metrics = sdm_metric_set(), verbose = TRUE   ) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (363ms) #> i 2 of 2 tuning:     default_rf #> i Creating pre-processing data to finalize unknown parameter: mtry #> → A | error:   Missing data in columns: bio05. #> There were issues with some computations   A: x1 #> There were issues with some computations   A: x15 #>  #> Warning: All models failed. Run `show_notes(.Last.tune.result)` for more #> information. #> Warning: Unknown or uninitialised column: `.notes`. #> ✖ 2 of 2 tuning:     default_rf failed with lacerta_prep <- lacerta_rec %>% prep(lacerta_thin) lacerta_prep #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:    1 #> predictor: 20 #> coords:     2 #>  #> ── Training information #> Training data contained 452 data points and 1 incomplete row. #>  #> ── Operations #> • Variables removed: bio01, bio02, bio03, bio04, bio07, bio08, ... | Trained"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a3_troubleshooting.html","id":"recipes-and-the-response-variable","dir":"Articles","previous_headings":"","what":"Recipes and the response variable","title":"Troubleshooting models that fail","text":"response variable treated special way recipes, can lead problems. best manipulate (e.g. transform character factor) response variable recipe, since response variable available train test models, make projections. hard-coded step recipe includes response variable, model fit, fail start making predictions. Another potential mistake remove response variable selecting variables interest. can happen use step_select choose variables interest, error less clear: Let’s load data create recipe step_select: Now create workflow set fit : errors intuitive. However, models failed algorithms, suggests problem lies data preparation side (either data , recipe). Ideally, already look data (summary glimpse). , case, know data fine. Whilst prepping (sometimes baking) recipe generally informative predictor variables, hard diagnose problems outcome variable recipe. Prepping show anything obvious: case, process exclusion. Everything seems fine, models don’t work. ask outcome variable might problematic. general rule, found easier rely step_rm remove variables (e.g. correlated variables).","code":"lacerta_thin <- readRDS(system.file(\"extdata/lacerta_climate_sf.RDS\",   package = \"tidysdm\" )) suggested_vars <- c(\"bio05\", \"bio06\", \"bio13\", \"bio14\", \"bio15\") lacerta_rec_sel <- recipe(lacerta_thin, formula = class ~ .) %>%   step_select(all_of(suggested_vars)) lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(default = lacerta_rec_sel),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # rf specs with tuning       rf = sdm_spec_rf()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid())  set.seed(100) lacerta_cv <- spatial_block_cv(lacerta_thin, v = 5) lacerta_models <-   lacerta_models %>%   workflow_map(\"tune_grid\",     resamples = lacerta_cv, grid = 3,     metrics = sdm_metric_set(), verbose = TRUE   ) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> → A | error:   ! `logistic_reg()` was unable to find an outcome. #>                ℹ Ensure that you have specified an outcome column and that it hasn't been #>                  removed in pre-processing. #> Warning: All models failed. Run `show_notes(.Last.tune.result)` for more #> information. #> Warning: Unknown or uninitialised column: `.notes`. #> ✖ 1 of 2 resampling: default_glm failed with  #> i 2 of 2 tuning:     default_rf #> i Creating pre-processing data to finalize unknown parameter: mtry #> → A | error:   ! `rand_forest()` was unable to find an outcome. #>                ℹ Ensure that you have specified an outcome column and that it hasn't been #>                  removed in pre-processing. #> There were issues with some computations   A: x1 #> There were issues with some computations   A: x15 #>  #> Warning: All models failed. Run `show_notes(.Last.tune.result)` for more information. #> Unknown or uninitialised column: `.notes`. #> ✖ 2 of 2 tuning:     default_rf failed with lacerta_prep_sel <- lacerta_rec_sel %>% prep(lacerta_thin) lacerta_prep_sel #>  #> ── Recipe ────────────────────────────────────────────────────────────────────── #>  #> ── Inputs #> Number of variables by role #> outcome:    1 #> predictor: 20 #> coords:     2 #>  #> ── Training information #> Training data contained 452 data points and no incomplete rows. #>  #> ── Operations #> • Variables selected: bio05, bio06, bio13, bio14, bio15 | Trained"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a3_troubleshooting.html","id":"using-the-desired-formula-with-gam","dir":"Articles","previous_headings":"","what":"Using the desired formula with GAM","title":"Troubleshooting models that fail","text":"General Additive Models unusual syntax, user define variables fitted splines. tidysdm functions simplify process, assuming user just wants fit standard smooth every continuous predictor. Note step defining formula incompatible using step_cor recipe. step_cor removes correlated variables recipes, using similar algorithm filter_collinear using method cor_caret. However, algorithm fitted data split cross-validating. means different variables eventually presented model fitted split, leading error mismatch formula available variables. known issue GAMs implemented tidymodels.","code":"lacerta_thin <- readRDS(system.file(\"extdata/lacerta_climate_sf.RDS\",   package = \"tidysdm\" ))  lacerta_rec <- recipe(lacerta_thin, formula = class ~ .) %>%   step_rm(all_of(c(     \"bio01\", \"bio02\", \"bio03\", \"bio04\", \"bio07\", \"bio08\",     \"bio09\", \"bio10\", \"bio11\", \"bio12\", \"bio14\", \"bio16\",     \"bio17\", \"bio18\", \"bio19\", \"altitude\"   )))  lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(default = lacerta_rec),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # the standard gam specs       gam = sdm_spec_gam()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # set formula for gams   update_workflow_model(\"default_gam\",     spec = sdm_spec_gam(),     formula = gam_formula(lacerta_rec)   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid()) set.seed(100) lacerta_cv <- spatial_block_cv(lacerta_thin, v = 5) lacerta_models <-   lacerta_models %>%   workflow_map(\"tune_grid\",     resamples = lacerta_cv, grid = 3,     metrics = sdm_metric_set(), verbose = TRUE   ) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 2 resampling: default_glm #> ✔ 1 of 2 resampling: default_glm (268ms) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 2 of 2 resampling: default_gam #> ✔ 2 of 2 resampling: default_gam (1.4s)"},{"path":"https://evolecolgroup.github.io/tidysdm/articles/a3_troubleshooting.html","id":"when-only-some-splits-fail","dir":"Articles","previous_headings":"","what":"When only some splits fail","title":"Troubleshooting models that fail","text":"examples , splits used cross-validation given algorithms failed. However, also possible failures occur splits certain algorithms (technically, specific rsplit within certain workflows). type problem occurs, best extract problematic workflow, potentially investigate fitting specific rsplit. generate problematic dataset subsampling lacerta dataset: create 3 folds attempt fit models: see one folds gives us error using GAMs. error (“Fitting terminated step failure - check results carefully”) comes gam function package mgcv. quick google StackOverflow[https://stats.stackexchange.com/questions/576273/gam-model-warning-message-step-failure--theta-estimation] gives us idea error comes . start extracting results gam fits: see , .notes column, second item empty (zero rows). can check indeed contains error wanted: can now get problematic data split, extract training data: case, nothing obvious leads error (important check make sure enough presences split; presences generally lead errors). can now extract workflow refit split confirm isolated problem: next step dig deeper data, trying understand whether outliers problematic. specific steps depend algorithm giving problems.","code":"lacerta_thin <- readRDS(system.file(\"extdata/lacerta_climate_sf.RDS\",   package = \"tidysdm\" )) set.seed(123) lacerta_thin <- lacerta_thin[sample(   1:nrow(lacerta_thin),   nrow(lacerta_thin) / 5 ), ]  lacerta_rec <- recipe(lacerta_thin, formula = class ~ .) %>%   step_rm(all_of(c(     \"bio01\", \"bio02\", \"bio03\", \"bio04\", \"bio07\", \"bio08\",     \"bio09\", \"bio10\", \"bio11\", \"bio12\", \"bio14\", \"bio16\",     \"bio17\", \"bio18\", \"bio19\", \"altitude\"   )))  lacerta_models <-   # create the workflow_set   workflow_set(     preproc = list(default = lacerta_rec),     models = list(       # the standard glm specs       glm = sdm_spec_glm(),       # the standard gam specs       gam = sdm_spec_gam(),       # rf specs with tuning       rf = sdm_spec_rf()     ),     # make all combinations of preproc and models,     cross = TRUE   ) %>%   # set formula for gams   update_workflow_model(\"default_gam\",     spec = sdm_spec_gam(),     formula = gam_formula(lacerta_rec)   ) %>%   # tweak controls to store information needed later to create the ensemble   option_add(control = control_ensemble_grid()) set.seed(100) lacerta_cv <- spatial_block_cv(lacerta_thin, v = 3) lacerta_models <-   lacerta_models %>%   workflow_map(\"tune_grid\",     resamples = lacerta_cv, grid = 3,     metrics = sdm_metric_set(), verbose = TRUE   ) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 1 of 3 resampling: default_glm #> ✔ 1 of 3 resampling: default_glm (188ms) #> i  No tuning parameters. `fit_resamples()` will be attempted #> i 2 of 3 resampling: default_gam #> → A | warning: Fitting terminated with step failure - check results carefully #> There were issues with some computations   A: x1 #> There were issues with some computations   A: x1 #>  #> ✔ 2 of 3 resampling: default_gam (1.6s) #> i 3 of 3 tuning:     default_rf #> i Creating pre-processing data to finalize unknown parameter: mtry #> ✔ 3 of 3 tuning:     default_rf (465ms) gam_results <- extract_workflow_set_result(lacerta_models, id = \"default_gam\") gam_results #> # Resampling results #> # 3-fold spatial block cross-validation  #> # A tibble: 3 × 5 #>   splits          id    .metrics         .notes           .predictions      #>   <list>          <chr> <list>           <list>           <list>            #> 1 <split [54/36]> Fold1 <tibble [3 × 4]> <tibble [0 × 3]> <tibble [36 × 5]> #> 2 <split [63/27]> Fold2 <tibble [3 × 4]> <tibble [1 × 3]> <tibble [27 × 5]> #> 3 <split [64/26]> Fold3 <tibble [3 × 4]> <tibble [0 × 3]> <tibble [26 × 5]> #>  #> There were issues with some computations: #>  #>   - Warning(s) x1: Fitting terminated with step failure - check results carefully #>  #> Run `show_notes(.Last.tune.result)` for more information. gam_results$.notes[2] #> [[1]] #> # A tibble: 1 × 3 #>   location                    type    note                                       #>   <chr>                       <chr>   <chr>                                      #> 1 preprocessor 1/1, model 1/1 warning Fitting terminated with step failure - ch… problem_split <- gam_results$splits[2][[1]] summary(training(problem_split)) #>        class             geometry      bio01           bio02        #>  presence :18   POINT        :63   Min.   : 4.74   Min.   : 6.737   #>  pseudoabs:45   epsg:4326    : 0   1st Qu.:11.81   1st Qu.: 9.336   #>                 +proj=long...: 0   Median :13.09   Median :10.937   #>                                    Mean   :12.88   Mean   :11.052   #>                                    3rd Qu.:14.82   3rd Qu.:12.649   #>                                    Max.   :17.87   Max.   :14.037   #>      bio03           bio04           bio05           bio06         #>  Min.   :34.30   Min.   :341.2   Min.   :19.90   Min.   :-6.2732   #>  1st Qu.:39.30   1st Qu.:500.8   1st Qu.:24.91   1st Qu.:-0.6787   #>  Median :40.55   Median :610.8   Median :28.59   Median : 1.1918   #>  Mean   :40.54   Mean   :584.6   Mean   :28.57   Mean   : 1.2175   #>  3rd Qu.:42.19   3rd Qu.:656.1   3rd Qu.:32.31   3rd Qu.: 3.5664   #>  Max.   :46.98   Max.   :756.7   Max.   :35.31   Max.   : 8.2344   #>      bio07           bio08            bio09            bio10       #>  Min.   :16.40   Min.   : 1.922   Min.   : 1.588   Min.   :12.86   #>  1st Qu.:23.32   1st Qu.: 7.716   1st Qu.:16.995   1st Qu.:18.53   #>  Median :27.88   Median : 9.668   Median :19.828   Median :20.51   #>  Mean   :27.35   Mean   : 9.450   Mean   :18.938   Mean   :20.48   #>  3rd Qu.:31.49   3rd Qu.:11.341   3rd Qu.:22.607   3rd Qu.:23.08   #>  Max.   :35.27   Max.   :16.882   Max.   :25.470   Max.   :25.71   #>      bio11            bio12            bio13           bio14       #>  Min.   :-2.060   Min.   : 249.0   Min.   : 36.0   Min.   : 2.00   #>  1st Qu.: 4.968   1st Qu.: 452.0   1st Qu.: 59.0   1st Qu.: 8.00   #>  Median : 6.236   Median : 628.0   Median : 91.0   Median :17.00   #>  Mean   : 6.268   Mean   : 757.8   Mean   :101.5   Mean   :21.97   #>  3rd Qu.: 8.455   3rd Qu.:1016.5   3rd Qu.:119.0   3rd Qu.:30.50   #>  Max.   :11.795   Max.   :1622.0   Max.   :248.0   Max.   :74.00   #>      bio15           bio16           bio17            bio18       #>  Min.   :13.44   Min.   : 96.0   Min.   : 17.00   Min.   : 22.0   #>  1st Qu.:30.07   1st Qu.:157.0   1st Qu.: 43.00   1st Qu.: 47.0   #>  Median :38.97   Median :249.0   Median : 71.00   Median : 78.0   #>  Mean   :41.58   Mean   :280.3   Mean   : 88.08   Mean   : 96.0   #>  3rd Qu.:54.30   3rd Qu.:334.0   3rd Qu.:109.50   3rd Qu.:117.5   #>  Max.   :71.59   Max.   :714.0   Max.   :253.00   Max.   :253.0   #>      bio19          altitude      #>  Min.   : 68.0   Min.   :  38.0   #>  1st Qu.:128.5   1st Qu.: 319.5   #>  Median :225.0   Median : 689.0   #>  Mean   :252.5   Mean   : 685.5   #>  3rd Qu.:319.5   3rd Qu.: 855.0   #>  Max.   :714.0   Max.   :1926.0 gam_workflow <- extract_workflow(lacerta_models, id = \"default_gam\") faulty_gam <- fit(gam_workflow, training(problem_split)) #> Warning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, #> : Fitting terminated with step failure - check results carefully"},{"path":"https://evolecolgroup.github.io/tidysdm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michela Leonardi. Author. Margherita Colucci. Author. Andrea Pozzi. Author. Andrea Manica. Author, maintainer.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Leonardi M, Colucci M, Pozzi , Manica (2024). tidysdm: Species Distribution Models Tidymodels. R package version 0.9.5,  https://evolecolgroup.github.io/tidysdm/, https://github.com/EvolEcolGroup/tidysdm.","code":"@Manual{,   title = {tidysdm: Species Distribution Models with Tidymodels},   author = {Michela Leonardi and Margherita Colucci and Andrea Pozzi and Andrea Manica},   year = {2024},   note = {R package version 0.9.5,  https://evolecolgroup.github.io/tidysdm/},   url = {https://github.com/EvolEcolGroup/tidysdm}, }"},{"path":"https://evolecolgroup.github.io/tidysdm/index.html","id":"tidysdm-","dir":"","previous_headings":"","what":"Species Distribution Models with Tidymodels","title":"Species Distribution Models with Tidymodels","text":"goal tidysdm implement Species Distribution Models using tidymodels framework. advantage tidymodels model syntax results returned user standardised, thus providing coherent interface modelling. Given variety models required SDM, tidymodels ideal framework. tidysdm provides number wrappers specialised functions facilitate fitting SDM tidymodels. Besides modelling contemporary species, tidysdm number functions specifically designed work palaeontological data. Whilst users free use environmental data, articles showcase potential integration pastclim, helps downloading manipulating present day data, future predictions, palaeoclimate reconstructions. overview capabilities tidysdm given Leonardi et al. (2023).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Species Distribution Models with Tidymodels","text":"tidysdm CRAN, easiest way install : version CRAN recommended every day use. New features bug fixes appear first dev branch GitHub, make way CRAN. need early access new features, can install tidysdm directly GitHub. install GitHub, need use devtools; haven’t done already, get CRAN install.packages(\"devtools\"). can install latest dev version tidysdm GitHub :","code":"install.packages(\"tidysdm\") # install.packages(\"devtools\") devtools::install_github(\"EvolEcolGroup/tidysdm\", ref = \"dev\")"},{"path":"https://evolecolgroup.github.io/tidysdm/index.html","id":"overview-of-functionality","dir":"","previous_headings":"","what":"Overview of functionality","title":"Species Distribution Models with Tidymodels","text":"dedicated website, can find Articles giving step--step overview fitting SDMs contemporary species, well equivalent tutorial using palaeontological data. Furthermore, Article examples leverage various features tidymodels commonly adopted SDM pipelines also dev version site updated dev branch tidysdm (top left dev website, version number red format x.x.x.9xxx, indicating development version). want contribute, make sure read contributing guide.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/index.html","id":"when-something-does-not-work","dir":"","previous_headings":"","what":"When something does not work","title":"Species Distribution Models with Tidymodels","text":"get error trying fit model? tidysdm relatively new package, might well , get error, might encountered bug. However, also possible misspecified model (error comes tidymodels, model valid). prepared Article diagnose failing models. fully comprehensive list everything go wrong, hopefully give ideas dig deeper wrong. also check issues GitHub see whether problem already reported. convinced problem bug tidysdm, feel free create new issue. Please make sure updated latest version tidysdm, well updating packages system, provide reproducible example developers investigate problem. think can help fixing bug, read contributing guide.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_member.html","id":null,"dir":"Reference","previous_headings":"","what":"Add best member of workflow to a simple ensemble — add_member","title":"Add best member of workflow to a simple ensemble — add_member","text":"function adds member(s) simple_ensemble() object, taking best member workflow provided. possible pass individual tune_results objects tuned workflow, workflowsets::workflow_set().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_member.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add best member of workflow to a simple ensemble — add_member","text":"","code":"add_member(x, member, ...)  # S3 method for default add_member(x, member, ...)  # S3 method for tune_results add_member(x, member, metric = NULL, id = NULL, ...)  # S3 method for workflow_set add_member(x, member, metric = NULL, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_member.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add best member of workflow to a simple ensemble — add_member","text":"x simple_ensemble member(s) added member  tune_results, workflowsets::workflow_set ... used moment. metric character string (NULL) metric optimize. NULL, first metric used. id name given workflow wflow_id column.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_member.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add best member of workflow to a simple ensemble — add_member","text":"simple_ensemble additional member(s)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_repeat.html","id":null,"dir":"Reference","previous_headings":"","what":"Add repeat(s) to a repeated ensemble — add_repeat","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"function adds repeat(s) repeat_ensemble object, repeat simple_ensemble. repeats must contain members, selected using metric.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_repeat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"","code":"add_repeat(x, rep, ...)  # S3 method for default add_repeat(x, rep, ...)  # S3 method for simple_ensemble add_repeat(x, rep, ...)  # S3 method for list add_repeat(x, rep, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_repeat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"x repeat_ensemble repeat(s) added rep repeat, single simple_ensemble, list simple_ensemble objects ... used moment.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/add_repeat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add repeat(s) to a repeated ensemble — add_repeat","text":"repeat_ensemble additional repeat(s)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.simple_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the results of a simple ensemble — autoplot.simple_ensemble","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"autoplot() method plots performance metrics ranked using metric.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.simple_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"","code":"# S3 method for simple_ensemble autoplot(   object,   rank_metric = NULL,   metric = NULL,   std_errs = stats::qnorm(0.95),   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.simple_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"object simple_ensemble whose elements results. rank_metric character string metric used rank results. none given, first metric metric set used (filtering metric option). metric character vector metrics (apart rank_metric) included visualization. NULL (default), available metrics plotted std_errs number standard errors plot (standard error exists). ... options pass autoplot(). Currently unused.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.simple_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"ggplot object.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.simple_ensemble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"function intended produce default plot visualize helpful information across possible applications simple_ensemble. sophisticated plots can produced using standard ggplot2 code plotting. x-axis workflow rank set (value one best) versus performance metric(s) y-axis. multiple metrics, facets metric, rank_metric first (provided; otherwise metric used create simple_ensemble used). multiple resamples used, confidence bounds shown result (95% confidence, default).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.simple_ensemble.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the results of a simple ensemble — autoplot.simple_ensemble","text":"","code":"# \\donttest{ #' # we use the two_class_example from `workflowsets` two_class_ens <- simple_ensemble() %>%   add_member(two_class_res, metric = \"roc_auc\") #>  #> Attaching package: ‘plotrix’ #> The following object is masked from ‘package:scales’: #>  #>     rescale autoplot(two_class_ens)  # }"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.spatial_initial_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"method provides good visualization method spatial initial rsplit.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.spatial_initial_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"","code":"# S3 method for spatial_initial_split autoplot(object, ..., alpha = 0.6)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.spatial_initial_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"object spatial_initial_rsplit object. Note resamples made sf objects  create spatial_initial_rsplit objects; function work resamples made non-spatial tibbles data.frames. ... Options passed ggplot2::geom_sf(). alpha Opacity, passed ggplot2::geom_sf(). Values alpha range 0 1, lower values corresponding transparent colors.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.spatial_initial_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"ggplot object fold assigned color, made using ggplot2::geom_sf().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.spatial_initial_split.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"plot method wrapper around standard spatial_rsplit method, re-labels folds Testing Training following convention standard initial_split object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/autoplot.spatial_initial_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a ggplot for a spatial initial rsplit. — autoplot.spatial_initial_split","text":"","code":"set.seed(123) block_initial <- spatial_initial_split(boston_canopy,   prop = 1 / 5, spatial_block_cv ) autoplot(block_initial)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/blockcv2rsample.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an object created with blockCV to an rsample object — blockcv2rsample","title":"Convert an object created with blockCV to an rsample object — blockcv2rsample","text":"function creates objects created blockCV rsample objects can used tidysdm. BlockCV provides sophisticated sampling options spatialsample library. example, possible stratify sampling ensure presences absences evenly distributed among folds (see example ).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/blockcv2rsample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an object created with blockCV to an rsample object — blockcv2rsample","text":"","code":"blockcv2rsample(x, data)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/blockcv2rsample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an object created with blockCV to an rsample object — blockcv2rsample","text":"x object created blockCV function data sf object used create x","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/blockcv2rsample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an object created with blockCV to an rsample object — blockcv2rsample","text":"rsample object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/blockcv2rsample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert an object created with blockCV to an rsample object — blockcv2rsample","text":"Note currently objects type cv_spatial cv_cluster supported.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/blockcv2rsample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an object created with blockCV to an rsample object — blockcv2rsample","text":"","code":"# \\donttest{ library(blockCV) #> blockCV 3.1.4 points <- read.csv(system.file(\"extdata/\", \"species.csv\", package = \"blockCV\")) pa_data <- sf::st_as_sf(points, coords = c(\"x\", \"y\"), crs = 7845) sb1 <- cv_spatial(   x = pa_data,   column = \"occ\", # the response column to balance the folds   k = 5, # number of folds   size = 350000, # size of the blocks in metres   selection = \"random\", # random blocks-to-fold   iteration = 10 ) # find evenly dispersed folds #>    |                                                                               |                                                                      |   0%   |                                                                               |=======                                                               |  10%   |                                                                               |==============                                                        |  20%   |                                                                               |=====================                                                 |  30%   |                                                                               |============================                                          |  40%   |                                                                               |===================================                                   |  50%   |                                                                               |==========================================                            |  60%   |                                                                               |=================================================                     |  70%   |                                                                               |========================================================              |  80%   |                                                                               |===============================================================       |  90%   |                                                                               |======================================================================| 100% #>   train_0 train_1 test_0 test_1 #> 1     172     207     85     36 #> 2     218     202     39     41 #> 3     218     192     39     51 #> 4     217     171     40     72 #> 5     203     200     54     43  sb1_rsample <- blockcv2rsample(sb1, pa_data) class(sb1_rsample) #> [1] \"spatial_rset\" \"rset\"         \"tbl_df\"       \"tbl\"          \"data.frame\"   autoplot(sb1_rsample)  # }"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/boyce_cont.html","id":null,"dir":"Reference","previous_headings":"","what":"Boyce continuous index (BCI) — boyce_cont","title":"Boyce continuous index (BCI) — boyce_cont","text":"function Boyce Continuous Index, measure model accuracy appropriate Species Distribution Models presence data (.e. using pseudoabsences background). algorithm used comes package enmSdm, uses multiple overlapping windows.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/boyce_cont.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Boyce continuous index (BCI) — boyce_cont","text":"","code":"boyce_cont(data, ...)  # S3 method for data.frame boyce_cont(   data,   truth,   ...,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL )  # S3 method for sf boyce_cont(data, ...)  boyce_cont_vec(   truth,   estimate,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/boyce_cont.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Boyce continuous index (BCI) — boyce_cont","text":"data Either data.frame containing columns specified truth estimate arguments, table/matrix true class results columns table. ... set unquoted column names one dplyr selector functions choose variables contain class probabilities. truth binary, 1 column selected, correspond value event_level. Otherwise, many columns factor levels truth ordering columns factor levels truth. truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). _vec() functions, factor vector. estimator One \"binary\", \"hand_till\", \"macro\", \"macro_weighted\" specify type averaging done. \"binary\" relevant two class case. others general methods calculating multiclass metrics. default automatically choose \"binary\" truth binary, \"hand_till\" truth >2 levels case_weights specified, \"macro\" truth >2 levels case_weights specified (case \"hand_till\" well-defined). na_rm logical value indicating whether NA values stripped computation proceeds. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default uses internal helper generally defaults \"first\" case_weights optional column identifier case weights. unquoted column name evaluates numeric column data. _vec() functions, numeric vector. estimate truth binary, numeric vector class probabilities corresponding \"relevant\" class. Otherwise, matrix many columns factor levels truth. assumed order levels truth.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/boyce_cont.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Boyce continuous index (BCI) — boyce_cont","text":"tibble columns .metric, .estimator, .estimate 1 row values. grouped data frames, number rows returned number groups.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/boyce_cont.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Boyce continuous index (BCI) — boyce_cont","text":"multiclass version function, operates binary predictions (e.g. presences absences SDMs).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/boyce_cont.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Boyce continuous index (BCI) — boyce_cont","text":"Boyce, M.S., P.R. Vernier, S.E. Nielsen F.K.. Schmiegelow. 2002. Evaluating resource selection functions. Ecol. Model., 157, 281-300. Hirzel, .H., G. Le Lay, V. Helfer, C. Randin . Guisan. 2006. Evaluating ability habitat suitability models predict species presences. Ecol. Model., 199, 142-152.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/boyce_cont.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Boyce continuous index (BCI) — boyce_cont","text":"","code":"boyce_cont(two_class_example, truth, Class1) #> # A tibble: 1 × 3 #>   .metric    .estimator .estimate #>   <chr>      <chr>          <dbl> #> 1 boyce_cont binary         0.805"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/calib_class_thresh.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibrate class thresholds — calib_class_thresh","title":"Calibrate class thresholds — calib_class_thresh","text":"Predict new dataset using simple ensemble. Predictions individual models combined according fun","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/calib_class_thresh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibrate class thresholds — calib_class_thresh","text":"","code":"calib_class_thresh(object, class_thresh, metric_thresh = NULL)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/calib_class_thresh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibrate class thresholds — calib_class_thresh","text":"object simple_ensemble object class_thresh probability threshold used convert probabilities classes. can number (0 1), character metric (currently \"tss_max\", \"kap_max\" \"sensitivity\"). sensitivity, additional target value passed along second element vector, e.g. c(\"sensitivity\",0.8). metric_thresh vector length 2 giving metric threshold, used prune models ensemble used prediction. 'metrics' need computed workflow tuned. metric's threshold needs match value used prediction. Examples c(\"accuracy\",0.8) c(\"boyce_cont\",0.7).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/calib_class_thresh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibrate class thresholds — calib_class_thresh","text":"simple_ensemble object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/calib_class_thresh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibrate class thresholds — calib_class_thresh","text":"","code":"test_ens <- simple_ensemble() %>%   add_member(two_class_res[1:3, ], metric = \"roc_auc\") test_ens <- calib_class_thresh(test_ens, class_thresh = \"tss_max\") test_ens <- calib_class_thresh(test_ens, class_thresh = \"kap_max\") test_ens <- calib_class_thresh(test_ens, class_thresh = c(\"sens\", 0.9))"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_coords_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that we have a valid pair of coordinate names — check_coords_names","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"internal function checks coords (passed functions) valid set names, , NULL, standard variable names data","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_coords_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"","code":"check_coords_names(data, coords)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_coords_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"data data.frame containing locations. coords vector length two giving names \"x\" \"y\" coordinates, points data.frame use standard names.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_coords_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that we have a valid pair of coordinate names — check_coords_names","text":"vector length 2 valid names, correct order","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_sdm_presence.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that the column with presences is correctly formatted — check_sdm_presence","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":"tidysdm, string defining presences first level response factor. function checks column correctly formatted.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_sdm_presence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":"","code":"check_sdm_presence(.data, .col, presence_level = \"presence\")"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_sdm_presence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":".data data.frame tibble, derived object sf data.frame .col column containing presences presence_level string used define presence level .col","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_sdm_presence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that the column with presences is correctly formatted — check_sdm_presence","text":"TRUE correctly formatted","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_splits_balance.html","id":null,"dir":"Reference","previous_headings":"","what":"Check the balance of presences vs pseudoabsences among splits — check_splits_balance","title":"Check the balance of presences vs pseudoabsences among splits — check_splits_balance","text":"Check balance presences vs pseudoabsences among splits","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_splits_balance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check the balance of presences vs pseudoabsences among splits — check_splits_balance","text":"","code":"check_splits_balance(splits, .col)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_splits_balance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check the balance of presences vs pseudoabsences among splits — check_splits_balance","text":"splits data splits (rset split object), generated function spatialsample::spatial_block_cv() .col column containing presences","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_splits_balance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check the balance of presences vs pseudoabsences among splits — check_splits_balance","text":"tibble number presences pseudoabsences assessment analysis set split (training testing initial split)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/check_splits_balance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check the balance of presences vs pseudoabsences among splits — check_splits_balance","text":"","code":"lacerta_thin <- readRDS(system.file(\"extdata/lacerta_climate_sf.RDS\",   package = \"tidysdm\" )) lacerta_cv <- spatial_block_cv(lacerta_thin, v = 5) check_splits_balance(lacerta_cv, class) #> # A tibble: 5 × 4 #>   presence_assessment pseudoabs_assessment presence_analysis pseudoabs_analysis #>                 <int>                <int>             <int>              <int> #> 1                  80                  273                33                 66 #> 2                  80                  283                33                 56 #> 3                  97                  272                16                 67 #> 4                  94                  262                19                 77 #> 5                 101                  267                12                 72"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/clamp_predictors.html","id":null,"dir":"Reference","previous_headings":"","what":"Clamp the predictors to match values in training set — clamp_predictors","title":"Clamp the predictors to match values in training set — clamp_predictors","text":"function clamps environmental variables terra::SpatRaster terra::SpatRasterDataset minimum maximum values exceed range training dataset.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/clamp_predictors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clamp the predictors to match values in training set — clamp_predictors","text":"","code":"clamp_predictors(x, training, .col, use_na)  # S3 method for default clamp_predictors(x, training, .col, use_na)  # S3 method for SpatRaster clamp_predictors(x, training, .col, use_na = FALSE)  # S3 method for SpatRasterDataset clamp_predictors(x, training, .col, use_na = FALSE)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/clamp_predictors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clamp the predictors to match values in training set — clamp_predictors","text":"x terra::SpatRaster terra::SpatRasterDataset clamp. training training dataset (data.frame sf::sf object. .col column containing presences (optional). specified, excluded clamping. use_na boolean determining whether values outside range training dataset removed (set NA). FALSE (default), values outside training range replaced extremes training range.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/clamp_predictors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clamp the predictors to match values in training set — clamp_predictors","text":"terra::SpatRaster terra::SpatRasterDataset clamped ranges training","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/collect_metrics.simple_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain and format results produced by tuning functions for ensemble objects — collect_metrics.simple_ensemble","title":"Obtain and format results produced by tuning functions for ensemble objects — collect_metrics.simple_ensemble","text":"Return tibble performance metrics models.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/collect_metrics.simple_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain and format results produced by tuning functions for ensemble objects — collect_metrics.simple_ensemble","text":"","code":"# S3 method for simple_ensemble collect_metrics(x, ...)  # S3 method for repeat_ensemble collect_metrics(x, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/collect_metrics.simple_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain and format results produced by tuning functions for ensemble objects — collect_metrics.simple_ensemble","text":"x simple_ensemble repeat_ensemble object ... currently used.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/collect_metrics.simple_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain and format results produced by tuning functions for ensemble objects — collect_metrics.simple_ensemble","text":"tibble.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/collect_metrics.simple_ensemble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Obtain and format results produced by tuning functions for ensemble objects — collect_metrics.simple_ensemble","text":"applied ensemble, metrics returned contain actual tuning parameter columns values (unlike collect functions run objects). reason ensembles contain different types models models different tuning parameters.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/collect_metrics.simple_ensemble.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain and format results produced by tuning functions for ensemble objects — collect_metrics.simple_ensemble","text":"","code":"collect_metrics(lacerta_ensemble) #> # A tibble: 12 × 5 #>    wflow_id       .metric     mean std_err     n #>    <chr>          <chr>      <dbl>   <dbl> <int> #>  1 default_glm    boyce_cont 0.532 0.255       5 #>  2 default_glm    roc_auc    0.969 0.00603     5 #>  3 default_glm    tss_max    0.876 0.0217      5 #>  4 default_rf     boyce_cont 0.497 0.250       5 #>  5 default_rf     roc_auc    0.985 0.00695     5 #>  6 default_rf     tss_max    0.907 0.0301      5 #>  7 default_gbm    boyce_cont 0.774 0.108       5 #>  8 default_gbm    roc_auc    0.980 0.00568     5 #>  9 default_gbm    tss_max    0.882 0.0318      5 #> 10 default_maxent boyce_cont 0.829 0.0500      5 #> 11 default_maxent roc_auc    0.992 0.00256     5 #> 12 default_maxent tss_max    0.931 0.00898     5 collect_metrics(lacerta_rep_ens) #> # A tibble: 18 × 6 #>    rep_id wflow_id       .metric     mean std_err     n #>    <chr>  <chr>          <chr>      <dbl>   <dbl> <int> #>  1 rep_01 default_glm    boyce_cont 0.776 0.0518      5 #>  2 rep_01 default_glm    roc_auc    0.963 0.0111      5 #>  3 rep_01 default_glm    tss_max    0.851 0.0270      5 #>  4 rep_01 default_maxent boyce_cont 0.890 0.0184      5 #>  5 rep_01 default_maxent roc_auc    0.989 0.00480     5 #>  6 rep_01 default_maxent tss_max    0.910 0.0316      5 #>  7 rep_02 default_glm    boyce_cont 0.641 0.161       5 #>  8 rep_02 default_glm    roc_auc    0.961 0.0156      5 #>  9 rep_02 default_glm    tss_max    0.861 0.0261      5 #> 10 rep_02 default_maxent boyce_cont 0.878 0.0238      5 #> 11 rep_02 default_maxent roc_auc    0.980 0.00790     5 #> 12 rep_02 default_maxent tss_max    0.880 0.0386      5 #> 13 rep_03 default_glm    boyce_cont 0.467 0.0987      5 #> 14 rep_03 default_glm    roc_auc    0.968 0.0131      5 #> 15 rep_03 default_glm    tss_max    0.869 0.0352      5 #> 16 rep_03 default_maxent boyce_cont 0.787 0.0870      5 #> 17 rep_03 default_maxent roc_auc    0.986 0.00642     5 #> 18 rep_03 default_maxent tss_max    0.921 0.0299      5"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/conf_matrix_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"Create confusion matrix multiple thresholds, using optimise tss","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/conf_matrix_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"","code":"conf_matrix_df(presences, absences)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/conf_matrix_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"presences Probabilities presences absences probabilities absences","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/conf_matrix_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a confusion matrix dataframe for multiple thresholds — conf_matrix_df","text":"data.frame thresholds columns thres, tp, fp, fn, tn","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/control_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Control wrappers — control_ensemble_grid","title":"Control wrappers — control_ensemble_grid","text":"Supply light wrappers control argument tune::tune_grid(), tune::tune_bayes(), tune::fit_resamples() call return needed elements use ensemble. functions return appropriate control grid ensure assessment set predictions information model specifications preprocessors, supplied resampling results object! integrate ensemble settings existing control settings, note functions just call appropriate tune::control_* function arguments save_pred = TRUE, save_workflow = TRUE. wrappers equivalent ones used stacks package.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/control_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Control wrappers — control_ensemble_grid","text":"","code":"control_ensemble_grid()  control_ensemble_resamples()  control_ensemble_bayes()"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/control_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Control wrappers — control_ensemble_grid","text":"tune::control_grid, tune::control_bayes, tune::control_resamples object.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/dist_pres_vs_bg.html","id":null,"dir":"Reference","previous_headings":"","what":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"environmental variable, function computes density functions presences absences returns (1-overlap), measure distance two distributions. Variables high distance good candidates SDMs, species occurrences confined subset available background.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/dist_pres_vs_bg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"","code":"dist_pres_vs_bg(.data, .col)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/dist_pres_vs_bg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":".data data.frame (derived object, tibble, sf) values bioclimate variables presences background .col column containing presences; assumes presences first level factor","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/dist_pres_vs_bg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"name vector distances","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/dist_pres_vs_bg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Distance between the distribution of climate values for presences vs background — dist_pres_vs_bg","text":"","code":"# This should be updated to use a dataset from tidysdm data(\"bradypus\", package = \"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>%   dplyr::mutate(presence = relevel(     factor(       dplyr::case_match(presence, 1 ~ \"presence\", 0 ~ \"absence\")     ),     ref = \"presence\"   )) %>%   select(-ecoreg)  bradypus_tb %>% dist_pres_vs_bg(presence) #> pre6190_l10 frs6190_ann tmn6190_ann pre6190_ann vap6190_ann  pre6190_l7  #>   0.4366602   0.4299480   0.4295013   0.4096230   0.3945855   0.3933454  #>       h_dem tmp6190_ann dtr6190_ann  pre6190_l4 tmx6190_ann cld6190_ann  #>   0.3647375   0.3316686   0.3288771   0.2544976   0.2418274   0.1812527  #>  pre6190_l1  #>   0.1297035"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/explain_tidysdm.html","id":null,"dir":"Reference","previous_headings":"","what":"Create explainer from your tidysdm ensembles. — explain_tidysdm","title":"Create explainer from your tidysdm ensembles. — explain_tidysdm","text":"DALEX designed explore explain behaviour Machine Learning methods. function creates DALEX explainer (see DALEX::explain()), can queried multiple function create explanations model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/explain_tidysdm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create explainer from your tidysdm ensembles. — explain_tidysdm","text":"","code":"explain_tidysdm(   model,   data,   y,   predict_function,   predict_function_target_column,   residual_function,   ...,   label,   verbose,   precalculate,   colorize,   model_info,   type,   by_workflow )  # S3 method for default explain_tidysdm(   model,   data = NULL,   y = NULL,   predict_function = NULL,   predict_function_target_column = NULL,   residual_function = NULL,   ...,   label = NULL,   verbose = TRUE,   precalculate = TRUE,   colorize = !isTRUE(getOption(\"knitr.in.progress\")),   model_info = NULL,   type = \"classification\",   by_workflow = FALSE )  # S3 method for simple_ensemble explain_tidysdm(   model,   data = NULL,   y = NULL,   predict_function = NULL,   predict_function_target_column = NULL,   residual_function = NULL,   ...,   label = NULL,   verbose = TRUE,   precalculate = TRUE,   colorize = !isTRUE(getOption(\"knitr.in.progress\")),   model_info = NULL,   type = \"classification\",   by_workflow = FALSE )  # S3 method for repeat_ensemble explain_tidysdm(   model,   data = NULL,   y = NULL,   predict_function = NULL,   predict_function_target_column = NULL,   residual_function = NULL,   ...,   label = NULL,   verbose = TRUE,   precalculate = TRUE,   colorize = !isTRUE(getOption(\"knitr.in.progress\")),   model_info = NULL,   type = \"classification\",   by_workflow = FALSE )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/explain_tidysdm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create explainer from your tidysdm ensembles. — explain_tidysdm","text":"model object - model explained data data.frame matrix - data used calculate explanations. provided, extracted model. Data passed without target column (shall provided y argument). NOTE: target variable present data, functionalities may work properly. y numeric vector outputs/scores. provided, shall size data predict_function function takes two arguments: model new data returns numeric vector predictions.   default yhat. predict_function_target_column Character numeric containing either column name column number model prediction object class considered positive (.e. class associated probability 1). NULL, second column output taken binary classification. multiclass classification setting, parameter cause switch binary classification mode one vs others probabilities. residual_function function takes four arguments: model, data, target vector y predict function (optionally). return numeric vector model residuals given data. provided, response residuals (\\(y-\\hat{y}\\)) calculated. default residual_function_default. ... parameters label character - name model. default extracted 'class' attribute model verbose logical. TRUE (default) diagnostic messages printed precalculate logical. TRUE (default) predicted_values residual calculated explainer created. happen also verbose TRUE. Set verbose precalculate FALSE omit calculations. colorize logical. TRUE (default) WARNINGS, ERRORS NOTES colorized. work R console. Now default FALSE knitting TRUE otherwise. model_info named list (package, version, type) containing information model. NULL, DALEX seek information . type type model, either classification regression. specified type extracted model_info. by_workflow boolean determining whether list explainer, one per model, returned instead single explainer ensemble","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/explain_tidysdm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create explainer from your tidysdm ensembles. — explain_tidysdm","text":"explainer object DALEX::explain ready work DALEX","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/explain_tidysdm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create explainer from your tidysdm ensembles. — explain_tidysdm","text":"","code":"# \\donttest{ # using the whole ensemble lacerta_explainer <- explain_tidysdm(tidysdm::lacerta_ensemble) #> Preparation of a new explainer is initiated #>   -> model label       :  data.frame  (  default  ) #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  predict_function  #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidysdm , ver. 0.9.5 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.06237169 , mean =  0.2679146 , max =  0.8991577   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.571285 , mean =  -0.01791465 , max =  0.7453912   #>   A new explainer has been created!   # by workflow explainer_list <- explain_tidysdm(tidysdm::lacerta_ensemble,   by_workflow = TRUE ) #> Preparation of a new explainer is initiated #>   -> model label       :  default_glm  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.005923341 , mean =  0.75 , max =  0.9993179   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.9513805 , mean =  3.739236e-15 , max =  0.9628654   #>   A new explainer has been created!   #> Preparation of a new explainer is initiated #>   -> model label       :  default_rf  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0 , mean =  0.7514254 , max =  1   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.5907905 , mean =  -0.001425431 , max =  0.523096   #>   A new explainer has been created!   #> Preparation of a new explainer is initiated #>   -> model label       :  default_gbm  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.255644 , mean =  0.6272817 , max =  0.7515493   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.7198042 , mean =  0.1227183 , max =  0.582286   #>   A new explainer has been created!   #> Preparation of a new explainer is initiated #>   -> model label       :  default_maxent  #>   -> data              :  444  rows  4  cols  #>   -> data              :  tibble converted into a data.frame  #>   -> target variable   :  444  values  #>   -> predict function  :  yhat.workflow  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package tidymodels , ver. 1.2.0 , task classification (  default  )  #>   -> model_info        :  type set to  classification  #>   -> predicted values  :  numerical, min =  0.09570046 , mean =  0.7996343 , max =  0.9997615   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.8792301 , mean =  -0.04963429 , max =  0.6252604   #>   A new explainer has been created!   # }"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/extrapol_mess.html","id":null,"dir":"Reference","previous_headings":"","what":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","title":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","text":"Compute multivariate environmental similarity surfaces (MESS), described Elith et al., 2010.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/extrapol_mess.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","text":"","code":"extrapol_mess(x, training, .col, ...)  # S3 method for default extrapol_mess(x, training, ...)  # S3 method for SpatRaster extrapol_mess(x, training, .col, filename = \"\", ...)  # S3 method for data.frame extrapol_mess(x, training, .col, ...)  # S3 method for SpatRasterDataset extrapol_mess(x, training, .col, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/extrapol_mess.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","text":"x terra::SpatRaster, terra::SpatRasterDataset data.frame training matrix data.frame sf object containing reference values; column correspond one layer terra::SpatRaster object, exception presences column defined .col (optional). .col column containing presences (optional). specified, excluded computing MESS scores. ... additional arguments terra::writeRaster() filename character. Output filename (optional)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/extrapol_mess.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","text":"terra::SpatRaster (data.frame) MESS values.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/extrapol_mess.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","text":"function modified version mess package predicts, method added work terra::SpatRasterDataset. Note method terra::SpatRasterDataset assumes variables stored terra::SpatRaster time information within x. Time also assumed years. conditions met, possible manually extract terra::SpatRaster time step, use extrapol_mess terra::SpatRasters","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/extrapol_mess.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","text":"Elith J., M. Kearney M., S. Phillips, 2010. art modelling range-shifting species. Methods Ecology Evolution 1:330-342.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/extrapol_mess.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multivariate environmental similarity surfaces (MESS) — extrapol_mess","text":"Jean-Pierre Rossi, Robert Hijmans, Paulo van Breugel, Andrea Manica","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_collinear.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter to retain only variables that have low collinearity — filter_collinear","title":"Filter to retain only variables that have low collinearity — filter_collinear","text":"method finds subset variables low collinearity. provides three methods: cor_caret, stepwise approach remove variables pairwise correlation given cutoff, choosing variable greatest mean correlation (based algorithm caret::findCorrelation); vif_step, stepwise approach remove variables variance inflation factor given cutoff (based algorithm usdm::vifstep), vif_cor, stepwise approach , step, find pair variables highest correlation cutoff removes one largest vif. correlation certain cutoff. methods terra::SpatRaster, data.frame matrix. terra::SpatRaster data.frame, numeric variables considered.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_collinear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter to retain only variables that have low collinearity — filter_collinear","text":"","code":"filter_collinear(   x,   cutoff = NULL,   verbose = FALSE,   names = TRUE,   to_keep = NULL,   method = \"cor_caret\",   cor_type = \"pearson\",   max_cells = Inf,   ... )  # S3 method for default filter_collinear(   x,   cutoff = NULL,   verbose = FALSE,   names = TRUE,   to_keep = NULL,   method = \"cor_caret\",   cor_type = \"pearson\",   max_cells = Inf,   ... )  # S3 method for SpatRaster filter_collinear(   x,   cutoff = NULL,   verbose = FALSE,   names = TRUE,   to_keep = NULL,   method = \"cor_caret\",   cor_type = \"pearson\",   max_cells = Inf,   exhaustive = FALSE,   ... )  # S3 method for data.frame filter_collinear(   x,   cutoff = NULL,   verbose = FALSE,   names = TRUE,   to_keep = NULL,   method = \"cor_caret\",   cor_type = \"pearson\",   max_cells = Inf,   ... )  # S3 method for matrix filter_collinear(   x,   cutoff = NULL,   verbose = FALSE,   names = TRUE,   to_keep = NULL,   method = \"cor_caret\",   cor_type = \"pearson\",   max_cells = Inf,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_collinear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter to retain only variables that have low collinearity — filter_collinear","text":"x terra::SpatRaster object, data.frame (numeric variables) cutoff numeric value used threshold remove variables. , \"cor_caret\" \"vif_cor\", pair-wise absolute correlation cutoff, defaults 0.7. \"vif_step\", variable inflation factor, defaults 10 verbose boolean whether additional information provided screen names logical; column names returned TRUE column index FALSE)? to_keep vector variable names want force set (note function return error correlation among variables higher cutoff). method character. One \"cor_caret\", \"vif_cor\" \"vif_step\". cor_type character. methods use correlation, type correlation: \"pearson\", \"kendall\", \"spearman\". Defaults \"pearson\" max_cells positive integer. maximum number cells used. smaller ncell(x), regular sample x used ... additional arguments specific given object type exhaustive boolean. Used terra::SpatRaster downsampling max_cells, require exhaustive approach terra::spatSample(). needed rasters sparse large, see help page terra::spatSample() details.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_collinear.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter to retain only variables that have low collinearity — filter_collinear","text":"vector names columns correlation threshold (names = TRUE), otherwise vector indices. Note indices numeric variables (.e. factors present, indices take account).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_collinear.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Filter to retain only variables that have low collinearity — filter_collinear","text":"Naimi, B., Hamm, N..S., Groen, T.., Skidmore, .K., Toxopeus, .G. 2014. positional uncertainty problem species distribution modelling?, Ecography 37 (2): 191-203.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_collinear.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Filter to retain only variables that have low collinearity — filter_collinear","text":"cor_caret: Original R code Dong Li, modified Max Kuhn Andrea Manica; vif_step vif_cor, original algorithm Babak Naimi, rewritten Andrea Manica tidysdm","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_high_cor.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated: Filter to retain only variables below a given correlation threshold — filter_high_cor","title":"Deprecated: Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"FUNCTION DEPRECATED. USE filter_collinear method=cor_caret instead","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_high_cor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated: Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"","code":"filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)  # S3 method for default filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)  # S3 method for SpatRaster filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)  # S3 method for data.frame filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)  # S3 method for matrix filter_high_cor(x, cutoff = 0.7, verbose = FALSE, names = TRUE, to_keep = NULL)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_high_cor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated: Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"x terra::SpatRaster object, data.frame (numeric variables), correlation matrix cutoff numeric value pair-wise absolute correlation cutoff verbose boolean printing details names logical; column names returned TRUE column index FALSE)? to_keep vector variable names want force set (note function return error correlation among variables higher cutoff).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_high_cor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deprecated: Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"vector names columns correlation threshold (names = TRUE), otherwise vector indices. Note indices numeric variables (.e. factors present, indices take account).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/filter_high_cor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Deprecated: Filter to retain only variables below a given correlation threshold — filter_high_cor","text":"method finds subset variable correlation certain cutoff. methods terra::SpatRaster, data.frame, work directly correlation matrix previously estimated. data.frame, numeric variables considered. algorithm based caret::findCorrelation, using exact option. absolute values pair-wise correlations considered. two variables high correlation, function looks mean absolute correlation variable removes variable largest mean absolute correlation. several function package subselect can also used accomplish goal tend retain predictors.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/form_resp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the response variable from a formula — form_resp","title":"Get the response variable from a formula — form_resp","text":"counterpart rsample::form_pred.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/form_resp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the response variable from a formula — form_resp","text":"","code":"form_resp(x)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/form_resp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the response variable from a formula — form_resp","text":"x formula","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/form_resp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the response variable from a formula — form_resp","text":"character name response","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/form_resp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the response variable from a formula — form_resp","text":"Note: might behave well functions log(y). neither form_pred modified https://stackoverflow.com/questions/13217322/--reliably-get-dependent-variable-name--formula-object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/gam_formula.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a formula for gam — gam_formula","title":"Create a formula for gam — gam_formula","text":"function takes formula recipe, turns numeric predictors smooths given k. formula can passed workflow workflow set fitting gam.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/gam_formula.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a formula for gam — gam_formula","text":"","code":"gam_formula(object, k = 10)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/gam_formula.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a formula for gam — gam_formula","text":"object recipes::recipe, already trained k k value smooth","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/gam_formula.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a formula for gam — gam_formula","text":"formula","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/geom_split_violin.html","id":null,"dir":"Reference","previous_headings":"","what":"Split violin geometry for ggplots — geom_split_violin","title":"Split violin geometry for ggplots — geom_split_violin","text":"geometry displays density distribution two groups side side, two halves violin. Note emptyx aesthetic provided even want plot single variable (see example ).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/geom_split_violin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split violin geometry for ggplots — geom_split_violin","text":"","code":"geom_split_violin(   mapping = NULL,   data = NULL,   stat = \"ydensity\",   position = \"identity\",   nudge = 0,   ...,   draw_quantiles = NULL,   trim = TRUE,   scale = \"area\",   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/geom_split_violin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split violin geometry for ggplots — geom_split_violin","text":"mapping Set aesthetic mappings created aes(). specified inherit.aes = TRUE (default), combined default mapping top level plot. must supply mapping plot mapping. data data displayed layer. three options: NULL, default, data inherited plot data specified call ggplot(). data.frame, object, override plot data. objects fortified produce data frame. See fortify() variables created. function called single argument, plot data. return value must data.frame, used layer data. function can created formula (e.g. ~ head(.x, 10)). stat Use override default connection geom_violin() stat_ydensity(). position position adjustment use data layer. can used various ways, including prevent overplotting improving display. position argument accepts following: result calling position function, position_jitter(). method allows passing extra arguments position. string naming position adjustment. give position string, strip function name position_ prefix. example, use position_jitter(), give position \"jitter\". information ways specify position, see layer position documentation. nudge Add space half-violin middle space allotted given factor x-axis. ... arguments passed layer()'s params argument. arguments broadly fall one 4 categories . Notably, arguments position argument, aesthetics required can passed .... Unknown arguments part 4 categories ignored. Static aesthetics mapped scale, fixed value apply layer whole. example, colour = \"red\" linewidth = 3. geom's documentation Aesthetics section lists available options. 'required' aesthetics passed params. Please note passing unmapped aesthetics vectors technically possible, order required length guaranteed parallel input data. constructing layer using stat_*() function, ... argument can used pass parameters geom part layer. example stat_density(geom = \"area\", outline.type = \"\"). geom's documentation lists parameters can accept. Inversely, constructing layer using geom_*() function, ... argument can used pass parameters stat part layer. example geom_area(stat = \"density\", adjust = 0.5). stat's documentation lists parameters can accept. key_glyph argument layer() may also passed .... can one functions described key glyphs, change display layer legend. draw_quantiles (NULL) (default), draw horizontal lines given quantiles density estimate. trim TRUE (default), trim tails violins range data. FALSE, trim tails. scale \"area\" (default), violins area (trimming tails). \"count\", areas scaled proportionally number observations. \"width\", violins maximum width. na.rm FALSE, default, missing values removed warning. TRUE, missing values silently removed. show.legend logical. layer included legends? NA, default, includes aesthetics mapped. FALSE never includes, TRUE always includes. can also named logical vector finely select aesthetics display. inherit.aes FALSE, overrides default aesthetics, rather combining . useful helper functions define data aesthetics inherit behaviour default plot specification, e.g. borders().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/geom_split_violin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split violin geometry for ggplots — geom_split_violin","text":"ggplot2::layer object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/geom_split_violin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split violin geometry for ggplots — geom_split_violin","text":"implementation based https://stackoverflow.com/questions/35717353/split-violin-plot--ggplot2. Credit goes @jan-jlx providing complete implementation StackOverflow, Trang Q. Nguyen adding nudge parameter.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/geom_split_violin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split violin geometry for ggplots — geom_split_violin","text":"","code":"data(\"bradypus\", package = \"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>% dplyr::mutate(presence = relevel(   factor(     dplyr::case_match(presence, 1 ~ \"presence\", 0 ~ \"absence\")   ),   ref = \"presence\" ))  ggplot(bradypus_tb, aes(   x = \"\",   y = cld6190_ann,   fill = presence )) +   geom_split_violin(nudge = 0.01)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_cellsize.html","id":null,"dir":"Reference","previous_headings":"","what":"Get default grid cellsize for a given dataset — grid_cellsize","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"function facilitates using spatialsample::spatial_block_cv multiple times analysis. spatialsample::spatial_block_cv creates grid based object data. However, spatial blocks generated multiple times analysis (e.g. spatial_initial_split(), subsequently cross-validation training dataset), might desirable keep grid). applying function largest dataset, usually full dataset spatial_initial_split(). resulting cellsize can used option spatialsample::spatial_block_cv.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_cellsize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"","code":"grid_cellsize(data, n = c(10, 10))"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_cellsize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"data sf::sf dataset used size grid n number cells grid, defaults c(10,10), also default sf::st_make_grid()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_cellsize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get default grid cellsize for a given dataset — grid_cellsize","text":"cell size","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_offset.html","id":null,"dir":"Reference","previous_headings":"","what":"Get default grid cellsize for a given dataset — grid_offset","title":"Get default grid cellsize for a given dataset — grid_offset","text":"function facilitates using spatialsample::spatial_block_cv multiple times analysis. spatialsample::spatial_block_cv creates grid based object data. However, spatial blocks generated multiple times analysis (e.g. spatial_initial_split(), subsequently cross-validation training dataset), might desirable keep grid). applying function largest dataset, usually full dataset spatial_initial_split(). resulting cellsize can used option spatialsample::spatial_block_cv.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_offset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get default grid cellsize for a given dataset — grid_offset","text":"","code":"grid_offset(data)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_offset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get default grid cellsize for a given dataset — grid_offset","text":"data sf::sf dataset used size grid","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/grid_offset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get default grid cellsize for a given dataset — grid_offset","text":"grid offset","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/horses.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinates of radiocarbon dates for horses — horses","title":"Coordinates of radiocarbon dates for horses — horses","text":"Coordinates presences horses 22k 8k YBP.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/horses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinates of radiocarbon dates for horses — horses","text":"","code":"horses"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/horses.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinates of radiocarbon dates for horses — horses","text":"tibble 1,297 rows 3 variables: latitude latitudes degrees longitude longitudes degrees time_bp time years present","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/kap_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum Cohen's Kappa — kap_max","title":"Maximum Cohen's Kappa — kap_max","text":"Cohen's Kappa (yardstick::kap()) measure similar yardstick::accuracy(), normalises observed accuracy value expected chance (helps unbalanced cases one class predominant).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/kap_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum Cohen's Kappa — kap_max","text":"","code":"kap_max(data, ...)  # S3 method for data.frame kap_max(   data,   truth,   ...,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL )  # S3 method for sf kap_max(data, ...)  kap_max_vec(   truth,   estimate,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/kap_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum Cohen's Kappa — kap_max","text":"data Either data.frame containing columns specified truth estimate arguments, table/matrix true class results columns table. ... set unquoted column names one dplyr selector functions choose variables contain class probabilities. truth binary, 1 column selected, correspond value event_level. Otherwise, many columns factor levels truth ordering columns factor levels truth. truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). _vec() functions, factor vector. estimator One \"binary\", \"hand_till\", \"macro\", \"macro_weighted\" specify type averaging done. \"binary\" relevant two class case. others general methods calculating multiclass metrics. default automatically choose \"binary\" truth binary, \"hand_till\" truth >2 levels case_weights specified, \"macro\" truth >2 levels case_weights specified (case \"hand_till\" well-defined). na_rm logical value indicating whether NA values stripped computation proceeds. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default uses internal helper generally defaults \"first\" case_weights optional column identifier case weights. unquoted column name evaluates numeric column data. _vec() functions, numeric vector. estimate truth binary, numeric vector class probabilities corresponding \"relevant\" class. Otherwise, matrix many columns factor levels truth. assumed order levels truth.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/kap_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum Cohen's Kappa — kap_max","text":"tibble columns .metric, .estimator, .estimate 1 row values. grouped data frames, number rows returned number groups.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/kap_max.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maximum Cohen's Kappa — kap_max","text":"function calibrates probability threshold classify presences maximises kappa. multiclass version function, operates binary predictions (e.g. presences absences SDMs).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/kap_max.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Maximum Cohen's Kappa — kap_max","text":"Cohen, J. (1960). \"coefficient agreement nominal scales\". Educational Psychological Measurement. 20 (1): 37-46. Cohen, J. (1968). \"Weighted kappa: Nominal scale agreement provision scaled disagreement partial credit\". Psychological Bulletin. 70 (4): 213-220.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/kap_max.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximum Cohen's Kappa — kap_max","text":"","code":"kap_max(two_class_example, truth, Class1) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 kap_max binary         0.725"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/km2m.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a geographic distance from km to m — km2m","title":"Convert a geographic distance from km to m — km2m","text":"function takes distance km converts meters, units generally used geographic operations R. trivial conversion, functions ensures zeroes lost along way!","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/km2m.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a geographic distance from km to m — km2m","text":"","code":"km2m(x)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/km2m.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a geographic distance from km to m — km2m","text":"x number km","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/km2m.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a geographic distance from km to m — km2m","text":"number meters","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/km2m.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a geographic distance from km to m — km2m","text":"","code":"km2m(10000) #> [1] 1e+07 km2m(1) #> [1] 1000"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinates of presences for Iberian emerald lizard — lacerta","title":"Coordinates of presences for Iberian emerald lizard — lacerta","text":"Coordinates presences Lacerta schreiberi. variables follows:","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinates of presences for Iberian emerald lizard — lacerta","text":"","code":"lacerta"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinates of presences for Iberian emerald lizard — lacerta","text":"tibble 1,297 rows 3 variables: ID ids GBIF latitude latitudes degrees longitude longitudes degrees","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"A simple ensemble for the lacerta data — lacerta_ensemble","title":"A simple ensemble for the lacerta data — lacerta_ensemble","text":"Ensemble SDM Lacerta schreiberi, generated vignette.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A simple ensemble for the lacerta data — lacerta_ensemble","text":"","code":"lacerta_ensemble"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta_ensemble.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A simple ensemble for the lacerta data — lacerta_ensemble","text":"simple_ensemble object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta_rep_ens.html","id":null,"dir":"Reference","previous_headings":"","what":"A repeat ensemble for the lacerta data — lacerta_rep_ens","title":"A repeat ensemble for the lacerta data — lacerta_rep_ens","text":"Ensemble SDM Lacerta schreiberi, generated vignette.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta_rep_ens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A repeat ensemble for the lacerta data — lacerta_rep_ens","text":"","code":"lacerta_rep_ens"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacerta_rep_ens.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A repeat ensemble for the lacerta data — lacerta_rep_ens","text":"repeat_ensemble object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacertidae_background.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinates of presences for lacertidae in the Iberian peninsula — lacertidae_background","title":"Coordinates of presences for lacertidae in the Iberian peninsula — lacertidae_background","text":"Coordinates presences lacertidae, used background lacerta dataset.. variables follows:","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacertidae_background.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinates of presences for lacertidae in the Iberian peninsula — lacertidae_background","text":"","code":"lacertidae_background"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/lacertidae_background.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinates of presences for lacertidae in the Iberian peninsula — lacertidae_background","text":"tibble 1,297 rows 3 variables: ID ids GBIF latitude latitudes degrees longitude longitudes degrees","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent.html","id":null,"dir":"Reference","previous_headings":"","what":"MaxEnt model — maxent","title":"MaxEnt model — maxent","text":"maxent defines MaxEnt model used Species Distribution Models. good guide options MaxEnt model work can found https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2013.07872.x","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MaxEnt model — maxent","text":"","code":"maxent(   mode = \"classification\",   engine = \"maxnet\",   feature_classes = NULL,   regularization_multiplier = NULL )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MaxEnt model — maxent","text":"mode single character string type model. possible value model \"classification\". engine single character string specifying computational engine use fitting. Currently \"maxnet\" available. feature_classes character, continuous feature classes desired, either \"default\" subset \"lqpht\" (example, \"lh\") regularization_multiplier numeric, constant adjust regularization","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MaxEnt model — maxent","text":"model_spec maxent model","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"MaxEnt model — maxent","text":"","code":"# \\donttest{ # format the data data(\"bradypus\", package = \"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>%   dplyr::mutate(presence = relevel(     factor(       dplyr::case_match(presence, 1 ~ \"presence\", 0 ~ \"absence\")     ),     ref = \"presence\"   )) %>%   select(-ecoreg)  # fit the model, and make some predictions maxent_spec <- maxent(feature_classes = \"lq\") maxent_fitted <- maxent_spec %>%   fit(presence ~ ., data = bradypus_tb) pred_prob <- predict(maxent_fitted, new_data = bradypus[, -1], type = \"prob\") pred_class <- predict(maxent_fitted, new_data = bradypus[, -1], type = \"class\")  # Now with tuning maxent_spec <- maxent(   regularization_multiplier = tune(),   feature_classes = tune() ) set.seed(452) cv <- vfold_cv(bradypus_tb, v = 2) maxent_tune_res <- maxent_spec %>%   tune_grid(presence ~ ., cv, grid = 3) show_best(maxent_tune_res, metric = \"roc_auc\") #> # A tibble: 3 × 8 #>   feature_classes regularization_multip…¹ .metric .estimator  mean     n std_err #>   <chr>                             <dbl> <chr>   <chr>      <dbl> <int>   <dbl> #> 1 l                                  1.02 roc_auc binary     0.857     2  0.0143 #> 2 lqph                               1.90 roc_auc binary     0.856     2  0.0121 #> 3 lqph                               2.50 roc_auc binary     0.854     2  0.0123 #> # ℹ abbreviated name: ¹​regularization_multiplier #> # ℹ 1 more variable: .config <chr> # }"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Parameters for maxent models — maxent_params","title":"Parameters for maxent models — maxent_params","text":"parameters auxiliary MaxEnt models using \"maxnet\" engine. functions used tuning functions, user rarely access directly.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameters for maxent models — maxent_params","text":"","code":"regularization_multiplier(range = c(0.5, 3), trans = NULL)  feature_classes(values = c(\"l\", \"lq\", \"lqp\", \"lqph\", \"lqpht\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameters for maxent models — maxent_params","text":"range two-element vector holding defaults smallest largest possible values, respectively. transformation specified, values transformed units. trans trans object scales package, scales::log10_trans() scales::reciprocal_trans(). provided, default used matches units used range. transformation, NULL. values feature_classes(), character string subset \"lqpht\" (example, \"lh\")","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parameters for maxent models — maxent_params","text":"param object can used tuning.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxent_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parameters for maxent models — maxent_params","text":"","code":"regularization_multiplier() #> Reg. multiplier (quantitative) #> Range: [0.5, 3] feature_classes() #> Feature classes  (qualitative) #> 5 possible values include: #> 'l', 'lq', 'lqp', 'lqph' and 'lqpht'"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to fit maxnet models with formulae — maxnet_fit","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"function wrapper around maxnet::maxnet, takes formula data well exposing parameters normalisation manner compatible parsnip. Users unlikely use function directly.  parsnip model specification MaxEnt, see maxent().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"","code":"maxnet_fit(   formula,   data,   regmult = 1,   classes = \"default\",   regfun = maxnet::maxnet.default.regularization,   addsamplestobackground = TRUE,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"formula formula defining outcome predictors data data.frame outcomes predictors regmult numeric, constant adjust regularization classes character, continuous feature classes desired, either \"default\" subset \"lqpht\" (example, \"lh\") regfun function, computes regularization constant feature addsamplestobackground logical, TRUE add background presence sample already ... currently used.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"Maxnet returns object class maxnet, list consisting glmnet model following elements added: betas nonzero coefficients fitted model alpha constant offset making exponential model sum one background data entropy entropy exponential model penalty.factor regularization constants used feature featuremins minimum feature, used clamping featuremaxs maximum feature, used clamping varmin minimum predictor, used clamping varmax maximum predictor, used clamping samplemeans mean predictor samples (majority factors) levels levels predictor factor","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"response needs factor class representing presences reference level factor (expected classification models). good guide options Maxent model work can found https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2013.07872.x","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper to fit maxnet models with formulae — maxnet_fit","text":"","code":"# \\donttest{ # we repeat the example in the `maxnet` package data(\"bradypus\", package = \"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>%   dplyr::mutate(presence = relevel(     factor(       dplyr::case_match(presence, 1 ~ \"presence\", 0 ~ \"absence\")     ),     ref = \"presence\"   )) mod <- maxnet_fit(presence ~ ., data = bradypus_tb, classes = \"lq\") plot(mod, \"tmp6190_ann\")  # }"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to predict maxnet models — maxnet_predict","title":"Wrapper to predict maxnet models — maxnet_predict","text":"function wrapper around predict method maxnet::maxnet, making function compatible parsnip. Users unlikely use function directly.  parsnip model specification MaxEnt, see maxent().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to predict maxnet models — maxnet_predict","text":"","code":"maxnet_predict(   object,   newdata,   type = c(\"class\", \"prob\"),   maxnet_type = c(\"cloglog\", \"link\", \"exponential\", \"logistic\"),   clamp = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to predict maxnet models — maxnet_predict","text":"object maxnet::maxnet object newdata dataframe new data type either \"prob\" \"class\" maxnet_type transformation used prediction clamp logical, defining whether clamping observed ranges used","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/maxnet_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to predict maxnet models — maxnet_predict","text":"tibble predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/niche_overlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute overlap metrics of the two niches — niche_overlap","title":"Compute overlap metrics of the two niches — niche_overlap","text":"function computes overlap metrics two rasters. currently implements Schoener's D inverse Hellinger's distance.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/niche_overlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute overlap metrics of the two niches — niche_overlap","text":"","code":"niche_overlap(x, y, method = c(\"Schoener\", \"Hellinger\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/niche_overlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute overlap metrics of the two niches — niche_overlap","text":"x terra::SpatRaster single layer y terra::SpatRaster single layer method string (vector strings) taking values \"Schoener\" \"Hellinger\"","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/niche_overlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute overlap metrics of the two niches — niche_overlap","text":"list overlap metrics, slots D (depending method)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/niche_overlap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute overlap metrics of the two niches — niche_overlap","text":"Note Hellinger's distance normalised dividing square root 2 (correct asymptote Hellinger's D), rather incorrect 2 used originally Warren et al (2008), based Erratum paper.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/niche_overlap.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute overlap metrics of the two niches — niche_overlap","text":"Warren, D.L., Glor, R.E. & Turelli M. (2008) Environmental niche equivalency versus conservativism: quantitative approaches niche evolution. Evolution 62: 2868-2883","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh.html","id":null,"dir":"Reference","previous_headings":"","what":"Find threshold that optimises a given metric — optim_thresh","title":"Find threshold that optimises a given metric — optim_thresh","text":"function returns threshold turn probabilities binary classes whilst optimising given metric. Currently available tss_max, kap_max sensitivity (target sensitivity required).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find threshold that optimises a given metric — optim_thresh","text":"","code":"optim_thresh(truth, estimate, metric, event_level = \"first\")"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find threshold that optimises a given metric — optim_thresh","text":"truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). _vec() functions, factor vector. estimate predicted probability event metric character metric optimised. Currently \"tss_max\", \"kap_max\", \"sensitivity\" given target (e.g. c(\"sensitivity\",0.8)) event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default uses internal helper generally defaults \"first\"","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find threshold that optimises a given metric — optim_thresh","text":"probability threshold event","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find threshold that optimises a given metric — optim_thresh","text":"","code":"optim_thresh(two_class_example$truth, two_class_example$Class1, metric = c(\"tss_max\")) #> [1] 0.7544818 optim_thresh(two_class_example$truth, two_class_example$Class1, metric = c(\"sens\", 0.9)) #> [1] 0.3710924"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_kap_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Find threshold that maximises Kappa — optim_thresh_kap_max","title":"Find threshold that maximises Kappa — optim_thresh_kap_max","text":"internal function returns threshold turn probabilities binary classes maximise kappa","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_kap_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find threshold that maximises Kappa — optim_thresh_kap_max","text":"","code":"optim_thresh_kap_max(presences, absences)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_kap_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find threshold that maximises Kappa — optim_thresh_kap_max","text":"presences Probabilities presences. absences Provabilities absences","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_kap_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find threshold that maximises Kappa — optim_thresh_kap_max","text":"probability threshold event","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_sens.html","id":null,"dir":"Reference","previous_headings":"","what":"Find threshold that gives a target sensitivity — optim_thresh_sens","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"internal function returns threshold turn probabilities binary classes given target sensitivity","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_sens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"","code":"optim_thresh_sens(presences, absences, sens_target)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_sens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"presences Probabilities presences. absences Provabilities absences sens_target target sensitivity","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_sens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find threshold that gives a target sensitivity — optim_thresh_sens","text":"probability threshold event","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_tss_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Find threshold that maximises TSS — optim_thresh_tss_max","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"internal function returns threshold turn probabilities binary classes maximise TSS","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_tss_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"","code":"optim_thresh_tss_max(presences, absences)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_tss_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"presences Probabilities presences. absences Provabilities absences","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/optim_thresh_tss_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find threshold that maximises TSS — optim_thresh_tss_max","text":"probability threshold event","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/out_of_range_warning.html","id":null,"dir":"Reference","previous_headings":"","what":"Warn if some times are outside the range of time steps from a raster — out_of_range_warning","title":"Warn if some times are outside the range of time steps from a raster — out_of_range_warning","text":"function helps making sure , assign times time_step layers raster, values badly range","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/out_of_range_warning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Warn if some times are outside the range of time steps from a raster — out_of_range_warning","text":"","code":"out_of_range_warning(times, time_steps)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/out_of_range_warning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Warn if some times are outside the range of time steps from a raster — out_of_range_warning","text":"times times locations time_steps time steps raster","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/out_of_range_warning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Warn if some times are outside the range of time steps from a raster — out_of_range_warning","text":"NULL return","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/plot_pres_vs_bg.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot presences vs background — plot_pres_vs_bg","title":"Plot presences vs background — plot_pres_vs_bg","text":"Create composite plots contrasting distribution multiple variables presences vs background.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/plot_pres_vs_bg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot presences vs background — plot_pres_vs_bg","text":"","code":"plot_pres_vs_bg(.data, .col)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/plot_pres_vs_bg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot presences vs background — plot_pres_vs_bg","text":".data data.frame (derived object, tibble::tibble, sf::st_sf) values bioclimate variables presences background .col column containing presences; assumes presences first level factor","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/plot_pres_vs_bg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot presences vs background — plot_pres_vs_bg","text":"patchwork composite plot","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/plot_pres_vs_bg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot presences vs background — plot_pres_vs_bg","text":"","code":"# \\donttest{ data(\"bradypus\", package = \"maxnet\") bradypus_tb <- tibble::as_tibble(bradypus) %>%   dplyr::mutate(presence = relevel(     factor(       dplyr::case_match(presence, 1 ~ \"presence\", 0 ~ \"absence\")     ),     ref = \"presence\"   )) %>%   select(-ecoreg)  bradypus_tb %>% plot_pres_vs_bg(presence)  # }"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.repeat_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict for a repeat ensemble set — predict.repeat_ensemble","title":"Predict for a repeat ensemble set — predict.repeat_ensemble","text":"Predict new dataset using repeat ensemble. Predictions individual models combined according fun","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.repeat_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict for a repeat ensemble set — predict.repeat_ensemble","text":"","code":"# S3 method for repeat_ensemble predict(   object,   new_data,   type = \"prob\",   fun = \"mean\",   metric_thresh = NULL,   class_thresh = NULL,   members = FALSE,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.repeat_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict for a repeat ensemble set — predict.repeat_ensemble","text":"object repeat_ensemble object new_data data frame look variables predict. type type prediction, \"prob\" \"class\". fun string defining aggregating function. can take values mean, median, weighted_mean, weighted_median none. possible combine multiple functions, except \"none\". set \"none\", individual member predictions returned (automatically sets member TRUE) metric_thresh vector length 2 giving metric threshold, used prune models ensemble used prediction. 'metrics' need computed workflow tuned. Examples c(\"accuracy\",0.8) c(\"boyce_cont\",0.7) class_thresh probability threshold used convert probabilities classes. can number (0 1), character metric (currently \"tss_max\" \"sensitivity\"). sensitivity, additional target value passed along second element vector, e.g. c(\"sensitivity\",0.8). members boolean defining whether individual predictions member added ensemble prediction. columns individual members name workflow prefix, separated \".\" usual column names predictions. ... used method.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.repeat_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict for a repeat ensemble set — predict.repeat_ensemble","text":"tibble predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.simple_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict for a simple ensemble set — predict.simple_ensemble","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"Predict new dataset using simple ensemble. Predictions individual models (.e. workflows) combined according fun","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.simple_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"","code":"# S3 method for simple_ensemble predict(   object,   new_data,   type = \"prob\",   fun = \"mean\",   metric_thresh = NULL,   class_thresh = NULL,   members = FALSE,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.simple_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"object simple_ensemble object new_data data frame look variables predict. type type prediction, \"prob\" \"class\". fun string defining aggregating function. can take values mean, median, weighted_mean, weighted_median none. possible combine multiple functions, except \"none\". set \"none\", individual member predictions returned (automatically sets member TRUE) metric_thresh vector length 2 giving metric threshold, used prune models ensemble used prediction. 'metrics' need computed workflow tuned. Examples c(\"accuracy\",0.8) c(\"boyce_cont\",0.7) class_thresh probability threshold used convert probabilities classes. can number (0 1), character metric (currently \"tss_max\" \"sensitivity\"). sensitivity, additional target value passed along second element vector, e.g. c(\"sensitivity\",0.8). members boolean defining whether individual predictions member added ensemble prediction. columns individual members name workflow prefix, separated \".\" usual column names predictions. ... used method.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict.simple_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict for a simple ensemble set — predict.simple_ensemble","text":"tibble predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict_raster.html","id":null,"dir":"Reference","previous_headings":"","what":"Make predictions for a whole raster — predict_raster","title":"Make predictions for a whole raster — predict_raster","text":"function allows use raster data make predictions variety tidymodels objects, simple_ensemble stacks::linear_stack","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict_raster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make predictions for a whole raster — predict_raster","text":"","code":"predict_raster(object, raster, ...)  # S3 method for default predict_raster(object, raster, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict_raster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make predictions for a whole raster — predict_raster","text":"object tidymodels object interest raster terra::SpatRaster input data. include levels names variables used object ... parameters passed standard predict() function appropriate object type (e.g. metrich_thresh class_thresh).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/predict_raster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make predictions for a whole raster — predict_raster","text":"terra::SpatRaster predictions","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_metrics_sf.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability metrics for sf objects — prob_metrics_sf","title":"Probability metrics for sf objects — prob_metrics_sf","text":"tidysdm provides specialised metrics SDMs, help pages(boyce_cont(), kap_max(), tss_max()). Additionally, also provides methods handle sf::sf objects following standard yardstick metrics: yardstick::average_precision() yardstick::brier_class() yardstick::classification_cost() yardstick::gain_capture() yardstick::mn_log_loss() yardstick::pr_auc() yardstick::roc_auc() yardstick::roc_aunp() yardstick::roc_aunu()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_metrics_sf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability metrics for sf objects — prob_metrics_sf","text":"","code":"# S3 method for sf average_precision(data, ...)  # S3 method for sf brier_class(data, ...)  # S3 method for sf classification_cost(data, ...)  # S3 method for sf gain_capture(data, ...)  # S3 method for sf mn_log_loss(data, ...)  # S3 method for sf pr_auc(data, ...)  # S3 method for sf roc_auc(data, ...)  # S3 method for sf roc_aunp(data, ...)  # S3 method for sf roc_aunu(data, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_metrics_sf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability metrics for sf objects — prob_metrics_sf","text":"data sf::sf object ... parameters pass data.frame version metric. See specific man page metric interest.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_metrics_sf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability metrics for sf objects — prob_metrics_sf","text":"tibble columns .metric, .estimator, .estimate 1 row values.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_metrics_sf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Probability metrics for sf objects — prob_metrics_sf","text":"Note roc_aunp roc_aunu multiclass metrics, relevant SDMs (work binary response). included completeness, class probability metrics yardstick sf method, applications SDMs.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_to_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"simple function to convert probability to binary classes — prob_to_binary","title":"simple function to convert probability to binary classes — prob_to_binary","text":"simple function convert probability binary classes","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_to_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simple function to convert probability to binary classes — prob_to_binary","text":"","code":"prob_to_binary(x, thresh, class_levels)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_to_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"simple function to convert probability to binary classes — prob_to_binary","text":"x vector probabilities thresh threshold convert binary class_levels binary levels","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/prob_to_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"simple function to convert probability to binary classes — prob_to_binary","text":"vector binary values","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/recipe.sf.html","id":null,"dir":"Reference","previous_headings":"","what":"Recipe for sf objects — recipe.sf","title":"Recipe for sf objects — recipe.sf","text":"method recipes::recipe() handles case x sf::sf object, commonly used Species Distribution Model, generates spatial_recipe.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/recipe.sf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recipe for sf objects — recipe.sf","text":"","code":"# S3 method for sf recipe(x, ...)  spatial_recipe(x, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/recipe.sf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recipe for sf objects — recipe.sf","text":"x sf::sf data frame. ... parameters passed recipes::recipe()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/recipe.sf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recipe for sf objects — recipe.sf","text":"object class spatial_recipe, derived version recipes::recipe , see manpage recipes::recipe() details.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/recipe.sf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recipe for sf objects — recipe.sf","text":"recipes natively compatible sf::sf objects. problem geometry column sf::sf objects list, incompatible translation formulae recipe. method strips geometry column data.frame replaces simple X Y columns operations, thus allowing usual processing recipe() succeed (X Y give role coords spatial recipe). prepping baking spatial_recipe, data.frame tibble without coordinates used training new_data, dummy X Y columns generated filled NAs. NOTE order matters! need use syntax recipe(x=sf_obj, formula=class~.) method successfully detect sf::sf object. Starting formula fail.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/repeat_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeat ensemble — repeat_ensemble","title":"Repeat ensemble — repeat_ensemble","text":"ensemble based multiple sets pseudoabsences/background. object collection (list) simple_ensemble objects predictions combined simple way (e.g. taking either mean median). simple_ensemble contains best version given model type following turning; simple ensembles need metric estimated cv process.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/repeat_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeat ensemble — repeat_ensemble","text":"","code":"repeat_ensemble(...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/repeat_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repeat ensemble — repeat_ensemble","text":"... used, function just creates empty repeat_ensemble object. Members added add_best_candidates()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/repeat_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Repeat ensemble — repeat_ensemble","text":"empty repeat_ensemble","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample background points for SDM analysis — sample_background","title":"Sample background points for SDM analysis — sample_background","text":"function samples background points raster given set presences. locations returned center points sampled cells, can overlap presences (contrast pseudo-absences, see sample_pseudoabs). following methods implemented: 'random': background randomly sampled region covered raster (.e. NAs). 'dist_max': background randomly sampled unioned buffers 'dist_max' presences (distances 'm' lonlat rasters, map units projected rasters). Using union buffers means areas multiple buffers oversampled. also referred \"thickening\". 'bias': background points sampled according surface representing biased sampling effort.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample background points for SDM analysis — sample_background","text":"","code":"sample_background(   data,   raster,   n,   coords = NULL,   method = \"random\",   class_label = \"background\",   return_pres = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample background points for SDM analysis — sample_background","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster cells sampled (first layer used determine cells NAs, thus can sampled). sampling \"biased\", sampling probability proportional values first layer (.e. band) raster. n number background points sample. coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\"). method sampling method. One 'random', 'dist_max', 'targeted'. dist_max, maximum distance set additional element vector, e.g c('dist_max',70000). class_label label given sampled points. Defaults background return_pres return presences together background single tibble.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample background points for SDM analysis — sample_background","text":"object class tibble::tibble. presences returned, presence level set reference (match expectations yardstick package considers first level event).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample background points for SDM analysis — sample_background","text":"Note units distance depend projection raster.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample background points for SDM analysis for points with a time point. — sample_background_time","title":"Sample background points for SDM analysis for points with a time point. — sample_background_time","text":"function samples background points raster given set presences. locations returned center points sampled cells,, can overlap presences (contrast pseudo-absences, see sample_pseudoabs_time). following methods implemented: 'random': background points randomly sampled region covered raster (.e. NAs). 'dist_max': background points randomly sampled unioned buffers 'dist_max' presences (distances 'm' lonlat rasters, map units projected rasters). Using union buffers means areas multiple buffers oversampled. also referred \"thickening\". 'bias': background points sampled according surface representing biased sampling effort. Note surface time step normalised sum 1;use n_per_time_step affect sampling effort within time step.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample background points for SDM analysis for points with a time point. — sample_background_time","text":"","code":"sample_background_time(   data,   raster,   n_per_time_step,   coords = NULL,   time_col = \"time\",   lubridate_fun = c,   method = \"random\",   class_label = \"background\",   return_pres = TRUE,   time_buffer = 0 )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample background points for SDM analysis for points with a time point. — sample_background_time","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster terra::SpatRasterDataset cells sampled. terra::SpatRasterDataset, first dataset used define cells valid, NAs. n_per_time_step number background points sample time step (.e. vector length equal number time steps raster) coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") time_col name column time; time lubridate object, use lubridate_fun provide function can used convert appropriately lubridate_fun function convert time column lubridate object method sampling method. One 'random', 'dist_min', 'dist_max', 'dist_disc'. class_label label given sampled points. Defaults background return_pres return presences together background single tibble time_buffer buffer time axis around presences defines effect sampling background method 'max_dist'. set zero, presences effect time step assigned raster; positive value, defines number days date provided time column presence considered (e.g. 20 days means presence considered time steps equivalent plus minus twenty days date).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_background_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample background points for SDM analysis for points with a time point. — sample_background_time","text":"object class tibble::tibble. presences returned, presence level set reference (match expectations yardstick package considers first level event)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample pseudo-absence points for SDM analysis — sample_pseudoabs","title":"Sample pseudo-absence points for SDM analysis — sample_pseudoabs","text":"function samples pseudo-absence points raster given set presences. locations returned center points sampled cells, can overlap presences (contrast background points, see sample_background). following methods implemented: 'random': pseudo-absences randomly sampled region covered raster (.e. NAs). 'dist_min': pseudo-absences randomly sampled region excluding buffer 'dist_min' presences (distances 'm' lonlat rasters, map units projected rasters). 'dist_max': pseudo-absences randomly sampled unioned buffers 'dist_max' presences (distances 'm' lonlat rasters, map units projected rasters). Using union buffers means areas multiple buffers oversampled. also referred \"thickening\". 'dist_disc': pseudo-absences randomly sampled unioned discs around presences two values 'dist_disc' defining minimum maximum distance presences.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample pseudo-absence points for SDM analysis — sample_pseudoabs","text":"","code":"sample_pseudoabs(   data,   raster,   n,   coords = NULL,   method = \"random\",   class_label = \"pseudoabs\",   return_pres = TRUE )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample pseudo-absence points for SDM analysis — sample_pseudoabs","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster cells sampled n number pseudoabsence points sample coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") method sampling method. One 'random', 'dist_min', 'dist_max', 'dist_disc'. Threshold distances set additional elements vector, e.g c('dist_min',70000) c('dist_disc',50000,200000). class_label label given sampled points. Defaults pseudoabs return_pres return presences together pseudoabsences single tibble","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample pseudo-absence points for SDM analysis — sample_pseudoabs","text":"object class tibble::tibble. presences returned, presence level set reference (match expectations yardstick package considers first level event)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample pseudo-absence points for SDM analysis for points with a time point. — sample_pseudoabs_time","title":"Sample pseudo-absence points for SDM analysis for points with a time point. — sample_pseudoabs_time","text":"function samples pseudo-absence points raster given set presences. locations returned center points sampled cells, can overlap presences (contrast background points, see sample_background_time). following methods implemented: 'random': pseudo-absences randomly sampled region covered raster (.e. NAs). 'dist_min': pseudo-absences randomly sampled region excluding buffer 'dist_min' presences (distances 'm' lonlat rasters, map units projected rasters). 'dist_max': pseudo-absences randomly sampled unioned buffers 'dist_max' presences (distances 'm' lonlat rasters, map units projected rasters). Using union buffers means areas multiple buffers oversampled. also referred \"thickening\". 'dist_disc': pseudo-absences randomly sampled unioned discs around presences two values 'dist_disc' defining minimum maximum distance presences.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample pseudo-absence points for SDM analysis for points with a time point. — sample_pseudoabs_time","text":"","code":"sample_pseudoabs_time(   data,   raster,   n_per_presence,   coords = NULL,   time_col = \"time\",   lubridate_fun = c,   method = \"random\",   class_label = \"pseudoabs\",   return_pres = TRUE,   time_buffer = 0 )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample pseudo-absence points for SDM analysis for points with a time point. — sample_pseudoabs_time","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster terra::SpatRasterDataset cells sampled. terra::SpatRasterDataset, first dataset used define cells valid, NAs. n_per_presence number pseudoabsence points sample presence coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") time_col name column time; time lubridate object, use lubridate_fun provide function can used convert appropriately lubridate_fun function convert time column lubridate object method sampling method. One 'random', 'dist_min', 'dist_max', 'dist_disc'. class_label label given sampled points. Defaults pseudoabs return_pres return presences together pseudoabsences single tibble time_buffer buffer time axis around presences defines effect sampling pseudoabsences. set zero, presences effect time step assigned raster; positive value, defines number days date provided time column presence considered (e.g. 20 days means presence considered time steps equivalent plus minus twenty days date).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sample_pseudoabs_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample pseudo-absence points for SDM analysis for points with a time point. — sample_pseudoabs_time","text":"object class tibble::tibble. presences returned, presence level set reference (match expectations yardstick package considers first level event)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_metric_set.html","id":null,"dir":"Reference","previous_headings":"","what":"Metric set for SDM — sdm_metric_set","title":"Metric set for SDM — sdm_metric_set","text":"function returns yardstick::metric_set includes boyce_cont(), yardstick::roc_auc() tss_max(), commonly used metrics SDM.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_metric_set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Metric set for SDM — sdm_metric_set","text":"","code":"sdm_metric_set(...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_metric_set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Metric set for SDM — sdm_metric_set","text":"... additional metrics added yardstick::metric_set. See help yardstick::metric_set() constraints type metrics can mixed.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_metric_set.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Metric set for SDM — sdm_metric_set","text":"yardstick::metric_set object.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_metric_set.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Metric set for SDM — sdm_metric_set","text":"","code":"sdm_metric_set() #> A metric set, consisting of: #> - `boyce_cont()`, a probability metric | direction: maximize #> - `roc_auc()`, a probability metric    | direction: maximize #> - `tss_max()`, a probability metric    | direction: maximize sdm_metric_set(accuracy) #> A metric set, consisting of: #> - `boyce_cont()`, a probability metric | direction: maximize #> - `roc_auc()`, a probability metric    | direction: maximize #> - `tss_max()`, a probability metric    | direction: maximize #> - `accuracy()`, a class metric         | direction: maximize"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_boost_tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"function returns parsnip::model_spec Boosted Trees model used classifier presences absences Species Distribution Model. uses library xgboost fit boosted trees; use another library, simply build parsnip::model_spec directly.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_boost_tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"","code":"sdm_spec_boost_tree(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_boost_tree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"... parameters passed parsnip::boost_tree() customise model. See help function details. tune character defining tuning strategy. Valid strategies : \"sdm\" chooses hyperparameters important tune sdm (boost_tree: 'mtry', 'trees', 'tree_depth', 'learn_rate', 'loss_reduction', 'stop_iter') \"\" tunes hyperparameters (boost_tree: 'mtry', 'trees', 'tree_depth', 'learn_rate', 'loss_reduction', 'stop_iter','min_n' 'sample_size') \"custom\" passes options '...' \"none\" tune hyperparameter","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_boost_tree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"parsnip::model_spec model.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_boost_tree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a Boosted Trees model for SDM — sdm_spec_boost_tree","text":"","code":"standard_bt_spec <- sdm_spec_boost_tree() full_bt_spec <- sdm_spec_boost_tree(tune = \"all\") custom_bt_spec <- sdm_spec_boost_tree(tune = \"custom\", mtry = tune())"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a GAM for SDM — sdm_spec_gam","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"function returns parsnip::model_spec General Additive Model used classifier presences absences Species Distribution Model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"","code":"sdm_spec_gam(..., tune = \"none\")"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_gam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"... parameters passed parsnip::gen_additive_mod() customise model. See help function details. tune character defining tuning strategy. hyperparameters tune gam, valid option \"none\". parameter present consistency sdm_spec_* functions, nothing case.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_gam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"parsnip::model_spec model.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_gam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a GAM for SDM — sdm_spec_gam","text":"","code":"my_gam_spec <- sdm_spec_gam()"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a GLM for SDM — sdm_spec_glm","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"function returns parsnip::model_spec Generalised Linear Model used classifier presences absences Species Distribution Model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"","code":"sdm_spec_glm(..., tune = \"none\")"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"... parameters passed parsnip::logistic_reg() customise model. See help function details. tune character defining tuning strategy. hyperparameters tune glm, valid option \"none\". parameter present consistency sdm_spec_* functions, nothing case.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"parsnip::model_spec model.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a GLM for SDM — sdm_spec_glm","text":"","code":"my_spec_glm <- sdm_spec_glm()"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_maxent.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"function returns parsnip::model_spec MaxEnt model used Species Distribution Models.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_maxent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"","code":"sdm_spec_maxent(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_maxent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"... parameters passed maxent() customise model. See help function details. tune character defining tuning strategy. Valid strategies : \"sdm\" chooses hyper-parameters important tune sdm (maxent, 'mtry') \"\" tunes hyperparameters (maxent, 'mtry', 'trees' 'min') \"custom\" passes options '...' \"none\" tune hyperparameter","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_maxent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"parsnip::model_spec model.","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_maxent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a MaxEnt for SDM — sdm_spec_maxent","text":"","code":"test_maxent_spec <- sdm_spec_maxent(tune = \"sdm\") test_maxent_spec #> maxent Model Specification (classification) #>  #> Main Arguments: #>   feature_classes = tune() #>   regularization_multiplier = tune() #>  #> Computational engine: maxnet  #>  # setting specific values sdm_spec_maxent(tune = \"custom\", feature_classes = \"lq\") #> maxent Model Specification (classification) #>  #> Main Arguments: #>   feature_classes = lq #>  #> Computational engine: maxnet  #>"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_rand_forest.html","id":null,"dir":"Reference","previous_headings":"","what":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"function returns parsnip::model_spec Random Forest used classifier presences absences Species Distribution Models. uses library ranger fit boosted trees; use another library, simply build parsnip::model_spec directly.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_rand_forest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"","code":"sdm_spec_rand_forest(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))  sdm_spec_rf(..., tune = c(\"sdm\", \"all\", \"custom\", \"none\"))"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_rand_forest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"... parameters passed parsnip::rand_forest() customise model. See help function details. tune character defining tuning strategy. Valid strategies : \"sdm\" chooses hyperparameters important tune sdm (rf, 'mtry') \"\" tunes hyperparameters (rf, 'mtry', 'trees' 'min') \"custom\" passes options '...' \"none\" tune hyperparameter","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_rand_forest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"parsnip::model_spec model.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_rand_forest.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"sdm_spec_rf() simply short form sm_spec_rand_forest().","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/sdm_spec_rand_forest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model specification for a Random Forest for SDM — sdm_spec_rand_forest","text":"","code":"test_rf_spec <- sdm_spec_rf(tune = \"sdm\") test_rf_spec #> Random Forest Model Specification (classification) #>  #> Main Arguments: #>   mtry = tune() #>  #> Computational engine: ranger  #>  # combining tuning with specific values for other hyperparameters sdm_spec_rf(tune = \"sdm\", trees = 100) #> Random Forest Model Specification (classification) #>  #> Main Arguments: #>   mtry = tune() #>   trees = 100 #>  #> Computational engine: ranger  #>"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/simple_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple ensemble — simple_ensemble","title":"Simple ensemble — simple_ensemble","text":"simple ensemble collection workflows predictions combined simple way (e.g. taking either mean median). Usually workflows consists best version given model algorithm following tuning. workflows fitted full training dataset making predictions.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/simple_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple ensemble — simple_ensemble","text":"","code":"simple_ensemble(...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/simple_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple ensemble — simple_ensemble","text":"... used, function just creates empty simple_ensemble object. Members added add_best_candidates()","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/simple_ensemble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple ensemble — simple_ensemble","text":"empty simple_ensemble. tibble columns: wflow_id: name workflows best model chosen workflow: trained workflow objects metrics: metrics based crossvalidation resampling used tune models","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/spatial_initial_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"spatial_initial_split creates single binary split data training set testing set. strategies package spatialsample available; random split strategy used generate initial split.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/spatial_initial_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"","code":"spatial_initial_split(data, prop, strategy, ...)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/spatial_initial_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"data dataset (data.frame tibble) prop proportion data retained modelling/analysis. strategy sampling strategy spatialsample ... parameters passed strategy","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/spatial_initial_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"rsplit object can used rsample::training rsample::testing functions extract data split.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/spatial_initial_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simple Training/Test Set Splitting for spatial data — spatial_initial_split","text":"","code":"set.seed(123) block_initial <- spatial_initial_split(boston_canopy, prop = 1 / 5, spatial_block_cv) testing(block_initial) #> Simple feature collection with 153 features and 18 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: 745098 ymin: 2915630 xmax: 805045.8 ymax: 2969840 #> Projected CRS: NAD83 / Massachusetts Mainland (ftUS) #> # A tibble: 153 × 19 #>    grid_id land_area canopy_gain canopy_loss canopy_no_change canopy_area_2014 #>    <chr>       <dbl>       <dbl>       <dbl>            <dbl>            <dbl> #>  1 M-9      2690727.      52443.      53467.          304239.          357706. #>  2 Q-21     2690727.      54712.     101816.         1359305.         1461121. #>  3 AB-23     725043.      13737.      13278.           52628.           65906. #>  4 AC-15    1175032.      24517.      24010.          111148.          135158. #>  5 U-25     2691491.      83740.     117496.          601040.          718536. #>  6 Y-13     2691490.      79215.      41676.          312299.          353975. #>  7 M-10     2578879.      27026.      41240.          161115.          202355. #>  8 T-22     2691490.      80929.     140490.          573628.          714118. #>  9 AO-16    1717547.      64863.      52390.          465563.          517953. #> 10 X-23     2690728.      85198.     109044.          458205.          567249. #> # ℹ 143 more rows #> # ℹ 13 more variables: canopy_area_2019 <dbl>, change_canopy_area <dbl>, #> #   change_canopy_percentage <dbl>, canopy_percentage_2014 <dbl>, #> #   canopy_percentage_2019 <dbl>, change_canopy_absolute <dbl>, #> #   mean_temp_morning <dbl>, mean_temp_evening <dbl>, mean_temp <dbl>, #> #   mean_heat_index_morning <dbl>, mean_heat_index_evening <dbl>, #> #   mean_heat_index <dbl>, geometry <MULTIPOLYGON [US_survey_foot]> training(block_initial) #> Simple feature collection with 529 features and 18 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: 739826.9 ymin: 2908294 xmax: 812069.7 ymax: 2970073 #> Projected CRS: NAD83 / Massachusetts Mainland (ftUS) #> # A tibble: 529 × 19 #>    grid_id land_area canopy_gain canopy_loss canopy_no_change canopy_area_2014 #>    <chr>       <dbl>       <dbl>       <dbl>            <dbl>            <dbl> #>  1 AB-4      795045.      15323.       3126.           53676.           56802. #>  2 I-33      265813.       8849.      11795.           78677.           90472. #>  3 AO-9      270153        6187.       1184.           26930.           28114. #>  4 H-10     2691490.      73098.      80362.          345823.          426185. #>  5 V-7       107890.        219.       3612.             240.            3852. #>  6 Q-22     2648089.     122211.     154236.         1026632.         1180868. #>  7 X-4       848558.       8275.       1760.            6872.            8632. #>  8 P-18     2690726.     110928.     113146.          915137.         1028283. #>  9 J-29     2574479.      38069.      15530.         2388638.         2404168. #> 10 G-28     2641525.      87024.      39246.         1202528.         1241774. #> # ℹ 519 more rows #> # ℹ 13 more variables: canopy_area_2019 <dbl>, change_canopy_area <dbl>, #> #   change_canopy_percentage <dbl>, canopy_percentage_2014 <dbl>, #> #   canopy_percentage_2019 <dbl>, change_canopy_absolute <dbl>, #> #   mean_temp_morning <dbl>, mean_temp_evening <dbl>, mean_temp <dbl>, #> #   mean_heat_index_morning <dbl>, mean_heat_index_evening <dbl>, #> #   mean_heat_index <dbl>, geometry <MULTIPOLYGON [US_survey_foot]>"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"function thins dataset one observation per cell retained.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"","code":"thin_by_cell(data, raster, coords = NULL, drop_na = TRUE, agg_fact = NULL)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster object defined grid coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") drop_na boolean whether locations NA raster dropped. agg_fact positive integer. Aggregation factor expressed number cells direction (horizontally vertically). two integers (horizontal vertical aggregation factor) three integers (also aggregating layers). Defaults NULL, implies aggregation (.e. thinning done grid raster)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin point dataset to have 1 observation per raster cell — thin_by_cell","text":"thinning can achieved aggregating cells raster thinning, achieved setting agg_fact > 1 (aggregation works manner equivalent terra::aggregate()).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"function thins dataset one observation per cell per time slice retained. use raster layers time slices define data cube thinning enforced (see details time formatted).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"","code":"thin_by_cell_time(   data,   raster,   coords = NULL,   time_col = \"time\",   lubridate_fun = c,   drop_na = TRUE,   agg_fact = NULL )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). raster terra::SpatRaster object defined grid layers corresponding time slices (times set either POSIXlt \"years\", see terra::time() details), terra::SpatRasterDataset first dataset used (, times dataset set either POSIXlt \"years\") terra::time() coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") time_col name column time; time lubridate object, use lubridate_fun provide function can used convert appropriately lubridate_fun function convert time column lubridate object drop_na boolean whether locations NA raster dropped. agg_fact positive integer. Aggregation factor expressed number cells direction (horizontally vertically). two integers (horizontal vertical aggregation factor) three integers (also aggregating layers). Defaults NULL, implies aggregation (.e. thinning done grid raster)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_cell_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin point dataset to have 1 observation per raster cell per time slice — thin_by_cell_time","text":"spatial thinning can achieved aggregating cells raster thinning, achieved setting agg_fact > 1 (aggregation works manner equivalent terra::aggregate()).","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin points dataset based on geographic distance — thin_by_dist","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"function thins dataset observations distance greater \"dist_min\" retained.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"","code":"thin_by_dist(data, dist_min, coords = NULL)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). dist_min Minimum distance points (units appropriate projection, meters lonlat data). coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\")","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin points dataset based on geographic distance — thin_by_dist","text":"Distances measured appropriate units projection used. case raw latitude longitude (e.g. provided data.frame), crs set WGS84, units set meters. function modified version algorithm spThin, adapted work sf objects.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"function thins dataset observations distance greater \"dist_min\" space \"interval_min\" time retained.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"","code":"thin_by_dist_time(   data,   dist_min,   interval_min,   coords = NULL,   time_col = \"time\",   lubridate_fun = c )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"data sf::sf data frame, data frame coordinate variables. can defined coords, unless standard names (see details ). dist_min Minimum distance points (units appropriate projection, meters lonlat data). interval_min Minimum time interval points, days. coords vector length two giving names \"x\" \"y\" coordinates, found data. left NULL, function try guess columns based standard names c(\"x\", \"y\"), c(\"X\",\"Y\"), c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\") time_col name column time; time lubridate object, use lubridate_fun provide function can used convert appropriately lubridate_fun function convert time column lubridate object","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"object class sf::sf data.frame, \"data\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist_time.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Thin points dataset based on geographic and temporal distance — thin_by_dist_time","text":"Geographic distances measured appropriate units projection used. case raw latitude longitude (e.g. provided data.frame), crs set WGS84, units set meters. Time interval estimated days. Note long time period, simple conversion x years = 365 * x days might lead slightly shorter intervals expected, ignores leap years. function y2d() provides closer approximation. function algorithm analogous spThin, exception neighbours defined terms space time.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tidysdm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tidysdm: Species Distribution Models with Tidymodels — tidysdm-package","title":"tidysdm: Species Distribution Models with Tidymodels — tidysdm-package","text":"Fit species distribution models (SDMs) using 'tidymodels' framework, provides standardised interface define models process outputs. 'tidysdm' expands 'tidymodels' providing methods spatial objects, models metrics specific SDMs, well number specialised functions process occurrences contemporary palaeo datasets. full functionalities package described Leonardi et al. (2023) doi:10.1101/2023.07.24.550358 .","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tidysdm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tidysdm: Species Distribution Models with Tidymodels — tidysdm-package","text":"Maintainer: Andrea Manica am315@cam.ac.uk Authors: Michela Leonardi Margherita Colucci Andrea Pozzi","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss.html","id":null,"dir":"Reference","previous_headings":"","what":"TSS - True Skill Statistics — tss","title":"TSS - True Skill Statistics — tss","text":"True Skills Statistic, defined ","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TSS - True Skill Statistics — tss","text":"","code":"tss(data, ...)  # S3 method for data.frame tss(   data,   truth,   estimate,   estimator = NULL,   na_rm = TRUE,   case_weights = NULL,   event_level = \"first\",   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TSS - True Skill Statistics — tss","text":"data Either data.frame containing columns specified truth estimate arguments, table/matrix true class results columns table. ... currently used. truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). _vec() functions, factor vector. estimate column identifier predicted class results (also factor). truth can specified different ways primary method use unquoted variable name. _vec() functions, factor vector. estimator One : \"binary\", \"macro\", \"macro_weighted\", \"micro\" specify type averaging done. \"binary\" relevant two class case. three general methods calculating multiclass metrics. default automatically choose \"binary\" \"macro\" based estimate. na_rm logical value indicating whether NA values stripped computation proceeds. case_weights optional column identifier case weights. unquoted column name evaluates numeric column data. _vec() functions, numeric vector. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default \"first\".","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"TSS - True Skill Statistics — tss","text":"tibble columns .metric, .estimator, .estimate 1 row values. grouped data frames, number rows returned number groups.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"TSS - True Skill Statistics — tss","text":"sensitivity+specificity +1 function wrapper around yardstick::j_index(), another name quantity. Note function takes classes predicted model without calibration (.e. making split 0.5 probability). usually metric used Species Distribution Models, threshold recalibrated maximise TSS; purpose, use tss_max().","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"TSS - True Skill Statistics — tss","text":"","code":"# Two class data(\"two_class_example\") tss(two_class_example, truth, predicted) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 tss     binary         0.673 # Multiclass library(dplyr) data(hpc_cv) # Groups are respected hpc_cv %>%   group_by(Resample) %>%   tss(obs, pred) #> # A tibble: 10 × 4 #>    Resample .metric .estimator .estimate #>    <chr>    <chr>   <chr>          <dbl> #>  1 Fold01   tss     macro          0.434 #>  2 Fold02   tss     macro          0.422 #>  3 Fold03   tss     macro          0.533 #>  4 Fold04   tss     macro          0.449 #>  5 Fold05   tss     macro          0.431 #>  6 Fold06   tss     macro          0.413 #>  7 Fold07   tss     macro          0.398 #>  8 Fold08   tss     macro          0.468 #>  9 Fold09   tss     macro          0.435 #> 10 Fold10   tss     macro          0.412"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum TSS - True Skill Statistics — tss_max","title":"Maximum TSS - True Skill Statistics — tss_max","text":"True Skills Statistic, defined ","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum TSS - True Skill Statistics — tss_max","text":"","code":"tss_max(data, ...)  # S3 method for data.frame tss_max(   data,   truth,   ...,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL )  # S3 method for sf tss_max(data, ...)  tss_max_vec(   truth,   estimate,   estimator = NULL,   na_rm = TRUE,   event_level = \"first\",   case_weights = NULL,   ... )"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum TSS - True Skill Statistics — tss_max","text":"data Either data.frame containing columns specified truth estimate arguments, table/matrix true class results columns table. ... set unquoted column names one dplyr selector functions choose variables contain class probabilities. truth binary, 1 column selected, correspond value event_level. Otherwise, many columns factor levels truth ordering columns factor levels truth. truth column identifier true class results (factor). unquoted column name although argument passed expression supports quasiquotation (can unquote column names). _vec() functions, factor vector. estimator One \"binary\", \"hand_till\", \"macro\", \"macro_weighted\" specify type averaging done. \"binary\" relevant two class case. others general methods calculating multiclass metrics. default automatically choose \"binary\" truth binary, \"hand_till\" truth >2 levels case_weights specified, \"macro\" truth >2 levels case_weights specified (case \"hand_till\" well-defined). na_rm logical value indicating whether NA values stripped computation proceeds. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". argument applicable estimator = \"binary\". default uses internal helper generally defaults \"first\" case_weights optional column identifier case weights. unquoted column name evaluates numeric column data. _vec() functions, numeric vector. estimate truth binary, numeric vector class probabilities corresponding \"relevant\" class. Otherwise, matrix many columns factor levels truth. assumed order levels truth.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum TSS - True Skill Statistics — tss_max","text":"tibble columns .metric, .estimator, .estimate 1 row values. grouped data frames, number rows returned number groups.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss_max.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maximum TSS - True Skill Statistics — tss_max","text":"sensitivity+specificity +1 function calibrates probability threshold classify presences maximise TSS. multiclass version function, operates binary predictions (e.g. presences absences SDMs).","code":""},{"path":[]},{"path":"https://evolecolgroup.github.io/tidysdm/reference/tss_max.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximum TSS - True Skill Statistics — tss_max","text":"","code":"tss_max(two_class_example, truth, Class1) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 tss_max binary         0.728"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/y2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a time interval from years to days — y2d","title":"Convert a time interval from years to days — y2d","text":"function takes takes time interval years converts days, unit commonly used time operations R. simple conversion x * 365 work large number years, due presence leap years.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/y2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a time interval from years to days — y2d","text":"","code":"y2d(x)"},{"path":"https://evolecolgroup.github.io/tidysdm/reference/y2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a time interval from years to days — y2d","text":"x number years interval","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/y2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a time interval from years to days — y2d","text":"difftime object (days)","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/reference/y2d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a time interval from years to days — y2d","text":"","code":"y2d(1) #> Time difference of 365 days y2d(1000) #> Time difference of 365243 days"},{"path":"https://evolecolgroup.github.io/tidysdm/news/index.html","id":"tidysdm-095","dir":"Changelog","previous_headings":"","what":"tidysdm 0.9.5","title":"tidysdm 0.9.5","text":"implement clamping MESS manage extrapolation clearly separate sampling background vs pseudo-absences update vignettes","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/news/index.html","id":"tidysdm-094","dir":"Changelog","previous_headings":"","what":"tidysdm 0.9.4","title":"tidysdm 0.9.4","text":"CRAN release: 2024-03-05 fix ’predict*’ functions prevented fixed threshold used assign classes ensure compatibility upcoming changes tune","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/news/index.html","id":"tidysdm-093","dir":"Changelog","previous_headings":"","what":"tidysdm 0.9.3","title":"tidysdm 0.9.3","text":"CRAN release: 2024-01-17 fix bug filter_high_cor due changes terra 1.6.75 implement collect_metrics ensembles.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/news/index.html","id":"tidysdm-092","dir":"Changelog","previous_headings":"","what":"tidysdm 0.9.2","title":"tidysdm 0.9.2","text":"CRAN release: 2023-11-13 Release CRAN","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/news/index.html","id":"tidysdm-091","dir":"Changelog","previous_headings":"","what":"tidysdm 0.9.1","title":"tidysdm 0.9.1","text":"Add spatial_recipe class. BREAKING change makes previously saved objects unusable, old code work expected. Additional articles showing use additional tidymodels features, debug errors. Integration DALEX explain models. functions select variables.","code":""},{"path":"https://evolecolgroup.github.io/tidysdm/news/index.html","id":"tidysdm-090","dir":"Changelog","previous_headings":"","what":"tidysdm 0.9.0","title":"tidysdm 0.9.0","text":"Initial release GitHub.","code":""}]
